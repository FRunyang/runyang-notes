<!DOCTYPE html>
<html lang="en" dir="ltr"><head><title>学习鲁棒的时间动态和空间适应用于 VHPE</title><meta charset="utf-8"/><link rel="preconnect" href="https://fonts.googleapis.com"/><link rel="preconnect" href="https://fonts.gstatic.com"/><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Schibsted Grotesk:wght@400;700&amp;family=Source Sans Pro:ital,wght@0,400;0,600;1,400;1,600&amp;family=IBM Plex Mono:wght@400;600&amp;display=swap"/><link rel="preconnect" href="https://cdnjs.cloudflare.com" crossorigin="anonymous"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta name="og:site_name" content="Quartz 4"/><meta property="og:title" content="学习鲁棒的时间动态和空间适应用于 VHPE"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="学习鲁棒的时间动态和空间适应用于 VHPE"/><meta name="twitter:description" content="理解 关键帧（当前帧） 与**辅助帧（临近帧）**的时序关联性对于视频人体姿态估计来说至关重要。 现有方法通常显式（光流，时间差分）或者隐式（可形变）建模运动信息来捕获关键帧与辅助帧的时间相关性。然而，他们都是基于当前帧的视觉特征进行的。若当前帧发生遮挡等，它们的结果比较差。 我们提出一个新的方法来显式推理关键帧（当前帧） 与**辅助帧（临近帧）**的时序关系，且独立于当前帧的视觉特征。 B, L, C."/><meta property="og:description" content="理解 关键帧（当前帧） 与**辅助帧（临近帧）**的时序关联性对于视频人体姿态估计来说至关重要。 现有方法通常显式（光流，时间差分）或者隐式（可形变）建模运动信息来捕获关键帧与辅助帧的时间相关性。然而，他们都是基于当前帧的视觉特征进行的。若当前帧发生遮挡等，它们的结果比较差。 我们提出一个新的方法来显式推理关键帧（当前帧） 与**辅助帧（临近帧）**的时序关系，且独立于当前帧的视觉特征。 B, L, C."/><meta property="og:image:alt" content="理解 关键帧（当前帧） 与**辅助帧（临近帧）**的时序关联性对于视频人体姿态估计来说至关重要。 现有方法通常显式（光流，时间差分）或者隐式（可形变）建模运动信息来捕获关键帧与辅助帧的时间相关性。然而，他们都是基于当前帧的视觉特征进行的。若当前帧发生遮挡等，它们的结果比较差。 我们提出一个新的方法来显式推理关键帧（当前帧） 与**辅助帧（临近帧）**的时序关系，且独立于当前帧的视觉特征。 B, L, C."/><meta property="og:image" content="https://runyangnotes.top/static/og-image.png"/><meta property="og:image:url" content="https://runyangnotes.top/static/og-image.png"/><meta name="twitter:image" content="https://runyangnotes.top/static/og-image.png"/><meta property="og:image:type" content="image/.png"/><meta property="twitter:domain" content="runyangnotes.top"/><meta property="og:url" content="https://runyangnotes.top/B_Experiments_Tracking/学习鲁棒的时间动态和空间适应用于-VHPE"/><meta property="twitter:url" content="https://runyangnotes.top/B_Experiments_Tracking/学习鲁棒的时间动态和空间适应用于-VHPE"/><link rel="icon" href="../static/icon.png"/><meta name="description" content="理解 关键帧（当前帧） 与**辅助帧（临近帧）**的时序关联性对于视频人体姿态估计来说至关重要。 现有方法通常显式（光流，时间差分）或者隐式（可形变）建模运动信息来捕获关键帧与辅助帧的时间相关性。然而，他们都是基于当前帧的视觉特征进行的。若当前帧发生遮挡等，它们的结果比较差。 我们提出一个新的方法来显式推理关键帧（当前帧） 与**辅助帧（临近帧）**的时序关系，且独立于当前帧的视觉特征。 B, L, C."/><meta name="generator" content="Quartz"/><link href="../index.css" rel="stylesheet" type="text/css" data-persist="true"/><style>.expand-button {
  position: absolute;
  display: flex;
  float: right;
  padding: 0.4rem;
  margin: 0.3rem;
  right: 0;
  color: var(--gray);
  border-color: var(--dark);
  background-color: var(--light);
  border: 1px solid;
  border-radius: 5px;
  opacity: 0;
  transition: 0.2s;
}
.expand-button > svg {
  fill: var(--light);
  filter: contrast(0.3);
}
.expand-button:hover {
  cursor: pointer;
  border-color: var(--secondary);
}
.expand-button:focus {
  outline: 0;
}

pre:hover > .expand-button {
  opacity: 1;
  transition: 0.2s;
}

#mermaid-container {
  position: fixed;
  contain: layout;
  z-index: 999;
  left: 0;
  top: 0;
  width: 100vw;
  height: 100vh;
  overflow: hidden;
  display: none;
  backdrop-filter: blur(4px);
  background: rgba(0, 0, 0, 0.5);
}
#mermaid-container.active {
  display: inline-block;
}
#mermaid-container > #mermaid-space {
  border: 1px solid var(--lightgray);
  background-color: var(--light);
  border-radius: 5px;
  position: fixed;
  top: 50%;
  left: 50%;
  transform: translate(-50%, -50%);
  height: 80vh;
  width: 80vw;
  overflow: hidden;
}
#mermaid-container > #mermaid-space > .mermaid-content {
  position: relative;
  transform-origin: 0 0;
  transition: transform 0.1s ease;
  overflow: visible;
  min-height: 200px;
  min-width: 200px;
}
#mermaid-container > #mermaid-space > .mermaid-content pre {
  margin: 0;
  border: none;
}
#mermaid-container > #mermaid-space > .mermaid-content svg {
  max-width: none;
  height: auto;
}
#mermaid-container > #mermaid-space > .mermaid-controls {
  position: absolute;
  bottom: 20px;
  right: 20px;
  display: flex;
  gap: 8px;
  padding: 8px;
  background: var(--light);
  border: 1px solid var(--lightgray);
  border-radius: 6px;
  box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
  z-index: 2;
}
#mermaid-container > #mermaid-space > .mermaid-controls .mermaid-control-button {
  display: flex;
  align-items: center;
  justify-content: center;
  width: 32px;
  height: 32px;
  padding: 0;
  border: 1px solid var(--lightgray);
  background: var(--light);
  color: var(--dark);
  border-radius: 4px;
  cursor: pointer;
  font-size: 16px;
  font-family: var(--bodyFont);
  transition: all 0.2s ease;
}
#mermaid-container > #mermaid-space > .mermaid-controls .mermaid-control-button:hover {
  background: var(--lightgray);
}
#mermaid-container > #mermaid-space > .mermaid-controls .mermaid-control-button:active {
  transform: translateY(1px);
}
#mermaid-container > #mermaid-space > .mermaid-controls .mermaid-control-button:nth-child(2) {
  width: auto;
  padding: 0 12px;
  font-size: 14px;
}
/*# sourceMappingURL=data:application/json;charset=utf-8;base64,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 */</style><link href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css" rel="stylesheet" type="text/css" data-persist="true"/><script src="../prescript.js" type="application/javascript" data-persist="true"></script><script type="application/javascript" data-persist="true">const fetchData = fetch("../static/contentIndex.json").then(data => data.json())</script><link rel="alternate" type="application/rss+xml" title="RSS Feed" href="https://runyangnotes.top/index.xml"/></head><body data-slug="B_Experiments_Tracking/学习鲁棒的时间动态和空间适应用于-VHPE"><div id="quartz-root" class="page"><div id="quartz-body"><div class="left sidebar"><h2 class="page-title"><a href="..">Quartz 4</a></h2><div class="spacer mobile-only"></div><div class="flex-component" style="flex-direction: row; flex-wrap: nowrap; gap: 1rem;"><div style="flex-grow: 1; flex-shrink: 1; flex-basis: auto; order: 0; align-self: center; justify-self: center;"><div class="search"><button class="search-button"><svg role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title>Search</title><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"></path><circle cx="8" cy="8" r="7"></circle></g></svg><p>Search</p></button><div class="search-container"><div class="search-space"><input autocomplete="off" class="search-bar" name="search" type="text" aria-label="Search for something" placeholder="Search for something"/><div class="search-layout" data-preview="true"></div></div></div></div></div><div style="flex-grow: 0; flex-shrink: 1; flex-basis: auto; order: 0; align-self: center; justify-self: center;"><button class="darkmode"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" class="dayIcon" x="0px" y="0px" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35" xml:space="preserve" aria-label="Dark mode"><title>Dark mode</title><path d="M6,17.5C6,16.672,5.328,16,4.5,16h-3C0.672,16,0,16.672,0,17.5    S0.672,19,1.5,19h3C5.328,19,6,18.328,6,17.5z M7.5,26c-0.414,0-0.789,0.168-1.061,0.439l-2,2C4.168,28.711,4,29.086,4,29.5    C4,30.328,4.671,31,5.5,31c0.414,0,0.789-0.168,1.06-0.44l2-2C8.832,28.289,9,27.914,9,27.5C9,26.672,8.329,26,7.5,26z M17.5,6    C18.329,6,19,5.328,19,4.5v-3C19,0.672,18.329,0,17.5,0S16,0.672,16,1.5v3C16,5.328,16.671,6,17.5,6z M27.5,9    c0.414,0,0.789-0.168,1.06-0.439l2-2C30.832,6.289,31,5.914,31,5.5C31,4.672,30.329,4,29.5,4c-0.414,0-0.789,0.168-1.061,0.44    l-2,2C26.168,6.711,26,7.086,26,7.5C26,8.328,26.671,9,27.5,9z M6.439,8.561C6.711,8.832,7.086,9,7.5,9C8.328,9,9,8.328,9,7.5    c0-0.414-0.168-0.789-0.439-1.061l-2-2C6.289,4.168,5.914,4,5.5,4C4.672,4,4,4.672,4,5.5c0,0.414,0.168,0.789,0.439,1.06    L6.439,8.561z M33.5,16h-3c-0.828,0-1.5,0.672-1.5,1.5s0.672,1.5,1.5,1.5h3c0.828,0,1.5-0.672,1.5-1.5S34.328,16,33.5,16z     M28.561,26.439C28.289,26.168,27.914,26,27.5,26c-0.828,0-1.5,0.672-1.5,1.5c0,0.414,0.168,0.789,0.439,1.06l2,2    C28.711,30.832,29.086,31,29.5,31c0.828,0,1.5-0.672,1.5-1.5c0-0.414-0.168-0.789-0.439-1.061L28.561,26.439z M17.5,29    c-0.829,0-1.5,0.672-1.5,1.5v3c0,0.828,0.671,1.5,1.5,1.5s1.5-0.672,1.5-1.5v-3C19,29.672,18.329,29,17.5,29z M17.5,7    C11.71,7,7,11.71,7,17.5S11.71,28,17.5,28S28,23.29,28,17.5S23.29,7,17.5,7z M17.5,25c-4.136,0-7.5-3.364-7.5-7.5    c0-4.136,3.364-7.5,7.5-7.5c4.136,0,7.5,3.364,7.5,7.5C25,21.636,21.636,25,17.5,25z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" class="nightIcon" x="0px" y="0px" viewBox="0 0 100 100" style="enable-background:new 0 0 100 100" xml:space="preserve" aria-label="Light mode"><title>Light mode</title><path d="M96.76,66.458c-0.853-0.852-2.15-1.064-3.23-0.534c-6.063,2.991-12.858,4.571-19.655,4.571  C62.022,70.495,50.88,65.88,42.5,57.5C29.043,44.043,25.658,23.536,34.076,6.47c0.532-1.08,0.318-2.379-0.534-3.23  c-0.851-0.852-2.15-1.064-3.23-0.534c-4.918,2.427-9.375,5.619-13.246,9.491c-9.447,9.447-14.65,22.008-14.65,35.369  c0,13.36,5.203,25.921,14.65,35.368s22.008,14.65,35.368,14.65c13.361,0,25.921-5.203,35.369-14.65  c3.872-3.871,7.064-8.328,9.491-13.246C97.826,68.608,97.611,67.309,96.76,66.458z"></path></svg></button></div><div style="flex-grow: 0; flex-shrink: 1; flex-basis: auto; order: 0; align-self: center; justify-self: center;"><button class="readermode"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" class="readerIcon" fill="currentColor" stroke="currentColor" stroke-width="0.2" stroke-linecap="round" stroke-linejoin="round" width="64px" height="64px" viewBox="0 0 24 24" aria-label="Reader mode"><title>Reader mode</title><g transform="translate(-1.8, -1.8) scale(1.15, 1.2)"><path d="M8.9891247,2.5 C10.1384702,2.5 11.2209868,2.96705384 12.0049645,3.76669482 C12.7883914,2.96705384 13.8709081,2.5 15.0202536,2.5 L18.7549359,2.5 C19.1691495,2.5 19.5049359,2.83578644 19.5049359,3.25 L19.5046891,4.004 L21.2546891,4.00457396 C21.6343849,4.00457396 21.9481801,4.28672784 21.9978425,4.6528034 L22.0046891,4.75457396 L22.0046891,20.25 C22.0046891,20.6296958 21.7225353,20.943491 21.3564597,20.9931534 L21.2546891,21 L2.75468914,21 C2.37499337,21 2.06119817,20.7178461 2.01153575,20.3517706 L2.00468914,20.25 L2.00468914,4.75457396 C2.00468914,4.37487819 2.28684302,4.061083 2.65291858,4.01142057 L2.75468914,4.00457396 L4.50368914,4.004 L4.50444233,3.25 C4.50444233,2.87030423 4.78659621,2.55650904 5.15267177,2.50684662 L5.25444233,2.5 L8.9891247,2.5 Z M4.50368914,5.504 L3.50468914,5.504 L3.50468914,19.5 L10.9478955,19.4998273 C10.4513189,18.9207296 9.73864328,18.5588115 8.96709342,18.5065584 L8.77307039,18.5 L5.25444233,18.5 C4.87474657,18.5 4.56095137,18.2178461 4.51128895,17.8517706 L4.50444233,17.75 L4.50368914,5.504 Z M19.5049359,17.75 C19.5049359,18.1642136 19.1691495,18.5 18.7549359,18.5 L15.2363079,18.5 C14.3910149,18.5 13.5994408,18.8724714 13.0614828,19.4998273 L20.5046891,19.5 L20.5046891,5.504 L19.5046891,5.504 L19.5049359,17.75 Z M18.0059359,3.999 L15.0202536,4 L14.8259077,4.00692283 C13.9889509,4.06666544 13.2254227,4.50975805 12.7549359,5.212 L12.7549359,17.777 L12.7782651,17.7601316 C13.4923805,17.2719483 14.3447024,17 15.2363079,17 L18.0059359,16.999 L18.0056891,4.798 L18.0033792,4.75457396 L18.0056891,4.71 L18.0059359,3.999 Z M8.9891247,4 L6.00368914,3.999 L6.00599909,4.75457396 L6.00599909,4.75457396 L6.00368914,4.783 L6.00368914,16.999 L8.77307039,17 C9.57551536,17 10.3461406,17.2202781 11.0128313,17.6202194 L11.2536891,17.776 L11.2536891,5.211 C10.8200889,4.56369974 10.1361548,4.13636104 9.37521067,4.02745763 L9.18347055,4.00692283 L8.9891247,4 Z"></path></g></svg></button></div></div><div class="explorer" data-behavior="link" data-collapsed="collapsed" data-savestate="true" data-data-fns="{&quot;order&quot;:[&quot;filter&quot;,&quot;map&quot;,&quot;sort&quot;],&quot;sortFn&quot;:&quot;(a,b)=>!a.isFolder&amp;&amp;!b.isFolder||a.isFolder&amp;&amp;b.isFolder?a.displayName.localeCompare(b.displayName,void 0,{numeric:!0,sensitivity:\&quot;base\&quot;}):!a.isFolder&amp;&amp;b.isFolder?1:-1&quot;,&quot;filterFn&quot;:&quot;node=>node.slugSegment!==\&quot;tags\&quot;&quot;,&quot;mapFn&quot;:&quot;node=>node&quot;}"><button type="button" class="explorer-toggle mobile-explorer hide-until-loaded" data-mobile="true" aria-controls="explorer-52"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide-menu"><line x1="4" x2="20" y1="12" y2="12"></line><line x1="4" x2="20" y1="6" y2="6"></line><line x1="4" x2="20" y1="18" y2="18"></line></svg></button><button type="button" class="title-button explorer-toggle desktop-explorer" data-mobile="false" aria-expanded="true"><h2>Explorer</h2><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="5 8 14 8" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="fold"><polyline points="6 9 12 15 18 9"></polyline></svg></button><div id="explorer-52" class="explorer-content" aria-expanded="false" role="group"><ul class="explorer-ul overflow" id="list-0"><li class="overflow-end"></li></ul></div><template id="template-file"><li><a href="#"></a></li></template><template id="template-folder"><li><div class="folder-container"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="5 8 14 8" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="folder-icon"><polyline points="6 9 12 15 18 9"></polyline></svg><div><button class="folder-button"><span class="folder-title"></span></button></div></div><div class="folder-outer"><ul class="content"></ul></div></li></template></div></div><div class="center"><div class="page-header"><div class="popover-hint"><nav class="breadcrumb-container" aria-label="breadcrumbs"><div class="breadcrumb-element"><a href="../">Home</a><p> ❯ </p></div><div class="breadcrumb-element"><a href="../B_Experiments_Tracking/">B_Experiments_Tracking</a><p> ❯ </p></div><div class="breadcrumb-element"><a href>学习鲁棒的时间动态和空间适应用于 VHPE</a></div></nav><h1 class="article-title">学习鲁棒的时间动态和空间适应用于 VHPE</h1><p show-comma="true" class="content-meta"><time datetime="2024-03-22T20:55:00.000Z">Mar 22, 2024</time><span>12 min read</span></p><ul class="tags"><li><a href="../tags/Pose-Estimation" class="internal tag-link">Pose-Estimation</a></li></ul></div></div><article class="popover-hint"><p>理解 <strong>关键帧（当前帧）</strong> 与**辅助帧（临近帧）**的时序关联性对于视频人体姿态估计来说至关重要。</p>
<p>现有方法通常显式（光流，时间差分）或者隐式（可形变）建模运动信息来捕获关键帧与辅助帧的时间相关性。然而，他们都是基于当前帧的视觉特征进行的。若当前帧发生遮挡等，它们的结果比较差。</p>
<p>我们提出一个新的方法来显式推理<strong>关键帧（当前帧）</strong> 与**辅助帧（临近帧）**的时序关系，且独立于当前帧的视觉特征。</p>
<p>B, L, C. (1,17,C)→Gaussian B,L+17,C</p>
<hr/>
<p>目前做法：</p>
<ul>
<li>先把辅助帧过Transformer Encoder建模时空信息；</li>
<li>然后把时空信息token与关键点token堆叠，预测关键点位置（建模关键点与所有帧的相关性）</li>
<li>得到时空预测后将时空token作为query，关键帧视觉特征作为key和value进行优化</li>
<li>最后用query产生增强的预测结果</li>
</ul>
<hr/>
<p>可能存在的问题：</p>
<ul>
<li>直接建模 keypoint token 与每帧的关系难度太大，因为token序列长度较长，可以考虑直接融合多帧特征 （在时间维度上进行 torch.mean），然后建模keypoint token与时空上下文的关系</li>
<li>keypoint token的位置编码需要加上，用可学习的token</li>
<li>整个输入序列的时序编码要加上，这个应该蛮影响性能的</li>
<li>修改初始化方法，学习MAE代码的初始化，初始化对模型也有影响</li>
</ul>
<hr/>
<p>后续修改版本：</p>
<ul>
<li>时空特征直接建模融合，建模 keypoint token与其关系，实现无视觉基础的关键点推理；视觉优化先用目前的版本；</li>
<li>保持多帧token，建模keyopoint token与其关系，不过视觉refine时直接将纯预测与视觉的整合（取平均），来验证是不是现在的refine策略问题；</li>
<li>多帧token加上下文重建，更换重建策略，即用类似的192个token表示关键帧图像信息，（192，embed_dim) → (192, 16_16_3) → (keyframe resize成 192， 16_16_3 进行监督)</li>
</ul>
<hr/>
<h3 id="目前v3_2版本得到了不错的结果可以作为一个不错的baseline模型架构如下">目前v3_2版本得到了不错的结果，可以作为一个不错的baseline，模型架构如下：<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#目前v3_2版本得到了不错的结果可以作为一个不错的baseline模型架构如下" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<ol>
<li>Encoder部分 <strong>联合时空建模 + 分离的时间-空间建模</strong></li>
<li>时序预测分支<strong>初始化了关节token；用每帧的特征与平均的关节token相加预测当前帧heatmap；加入了contextual token来重建当前帧作为监督（系数0.0001）；</strong></li>
<li>视觉特征分支以当前帧特征为query，融合的辅助帧特征（对时间维度取平均）为Key Value 进行特征融合；</li>
<li>监督上进行了密集的监督：
<ol>
<li>keypoint输出heatmap</li>
<li>每帧预测的当前帧heatmap进行监督</li>
<li>最终融合特征进行监督</li>
</ol>
</li>
</ol>
<h3 id="后续需要测试的baseline">后续需要测试的Baseline：<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#后续需要测试的baseline" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p>目前密集预测版本效果较好，试试去掉token直接密集预测</p>
<ol>
<li>直接用现有的 <strong>v3_2</strong> baseline，去掉重建loss看看结果（<strong>测试重建部分的效果</strong>）</li>
<li>不用每个辅助帧去预测当前帧的heatmap，直接用<strong>Keypoint Token</strong> 出预测结果，进行约束；其他部分保持不变 （保持重建loss为0.0001的比例）<strong>（测试密集预测的效果 ）</strong></li>
<li>直接用<strong>Keypoint Token</strong> 出预测结果进行约束，并把<code>visual_prediction</code>进行约束；最终融合的结果不约束 <strong>（测试中间监督与最终监督，目测结果影响不大）</strong></li>
<li>在视觉特征分支，不融合多帧特征，直接把多帧作为Key和Value,看看是否有影响（基于v3_2）</li>
<li>其他小的实验，Encoder中测试联合时空建模和分离时空建模有效性（可以试试删掉联合时空建模步骤）</li>
</ol>
<hr/>
<p>与《Global-to-Local Modeling for Video-based 3D Human Pose and Shape Estimation》不同的出发点：</p>
<ul>
<li>任务不同</li>
<li>动机完全不同。建模全局-局部运动；无关键帧推理-有关键帧识别</li>
</ul>
<hr/>
<h2 id="contribution"><strong>Contribution</strong><a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#contribution" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<ul>
<li>提出一个新颖的DSN框架，并行地学习双重时空表征（关键帧的视觉增强和无关键帧的时序推理），解耦合现有方法（单一视觉增强时空表征分支）对关键帧视觉特征的过度依赖。这允许很早期产生了合理可靠的人体姿态-即使在关键帧视觉退化下。[可以突出现有方法单分支]</li>
<li>设计两个模块自适应发觉有用信息实现视觉增强，掌握全局时许演化信息实现姿态推理。</li>
<li>SOTA.</li>
</ul>
<hr/>
<h2 id="投了-cvpr24-没中但是感觉审稿意见质量挺低的准备继续投-eccv">投了 CVPR24 没中，但是感觉审稿意见质量挺低的，准备继续投 ECCV<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#投了-cvpr24-没中但是感觉审稿意见质量挺低的准备继续投-eccv" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<h3 id="一些反思即后续修稿方向">一些反思即后续修稿方向<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#一些反思即后续修稿方向" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p>从方法上说我感觉这个方法确实解决了现存的问题，只是包装上还不太够，所以第一眼看上去觉得 contribution 不太大，先入为主就会觉得比较普通，不亮眼</p>
<p>审稿人不懂不是理由，重点是怎么完美的去突出工作的贡献；要突出贡献就得指出难点在哪里</p>
<p>之前只是说现有方法依赖于关键帧，所以我们要预测 然后直接就预测了。</p>
<p>在模型方面，我觉得得去说一下，为什么做预测是难的，与瞎按有方法的区别得点出来。</p>
<hr/>
<p><strong>与之前论文的区别：</strong></p>
<p><strong>比如 TDMI，</strong></p>
<ul>
<li>说现有方法没用过时序差分，所以怎么用是比较重要的；</li>
<li>先有方法不能挖掘有用信息，怎么挖掘也是重要的，然后挖掘的方法也新</li>
</ul>
<p><strong>进行类比 DiffPose，</strong></p>
<ul>
<li>怎么在扩散模型中引入时序信息；</li>
<li>怎么从噪声中恢复热图</li>
</ul>
<p><strong>这次，</strong></p>
<ul>
<li>现有方法不能预测，预测很重要；先突显预测的重要性，再说现在方法不行，我们的可以</li>
<li>但是姿态预测听起来比较简单，相比特征挖掘，扩散恢复等来说，姿态预测确实太简单了～</li>
<li>所以就不能直接写我们预测了，而是应该写预测的难点在哪里。</li>
</ul>
<h1 id="第二次投稿更新-eccv第一轮意见也很差-没必要去-rebuttal-了准备重新整一下这个方法继续去投这篇我不打算去降低档次">第二次投稿更新 ECCV第一轮意见也很差 没必要去 Rebuttal 了，准备重新整一下这个方法，继续去投。这篇我不打算去降低档次。<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#第二次投稿更新-eccv第一轮意见也很差-没必要去-rebuttal-了准备重新整一下这个方法继续去投这篇我不打算去降低档次" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h1>
<h3 id="投稿反思">投稿反思<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#投稿反思" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p><strong>一些关键的意见：</strong></p>
<ul>
<li>两个模块过于独立，没啥关系（说明双流网络之间做交互啥的挺重要，或者这条不是很重要）</li>
<li>Motivation 不太合理，就是图 2 但是我自己来看，这个 motivation 是合理的，就是严重依赖关键帧。但是要论证这点是比较难的。而且大家不太 get 到这一点。图 2 确实不太符合实际情况，这个我自己也考虑到了。所以说图 2 不太好（或者 eecv 投稿的时候去掉图 2 可能会好点？）</li>
</ul>
<blockquote>
<p><em>主要问题就是以上两点</em> 也就是需要更好的 motivation 去呈现，至少相比 cvpr 是有进步的这次。</p>
</blockquote>
<p>其他：</p>
<ul>
<li>参数量；离线方式；位置编码；token 方法依然不能理解</li>
</ul>
<p><strong>综上，问题就是 motivation+模型设计，我觉得需要更加 convince 的动机，以及更抓眼的模型设计。</strong></p>
<hr/>
<p>一些初步的动机想法：</p>
<ul>
<li>理解运动信息很重要。现有方法通常用光流、差分等，这个范式更加注重像素级别细粒度的像素级视觉内容对应，而忽略了比较 high-level 的帧间的关系，例如整体活肢体的人体运动。因此，这些方法对于遮挡等视觉退化是很敏感的，容易性能下降。我们提出充分去挖掘帧间的时空关系，学习图像级别的运动信息，即使在某些帧退化下依然有鲁邦的时序推理能力。 --- *本质上来说，光流/差分与像素之间相似度是密不可分的，过度依赖像素相似性，忽略全局的语义关系，会造成很容易失败</li>
<li>模型方面，可以搞一个由动态到静态的更新框架，先是一些 global keypoint token 去学习全局动态；然后进一步用关键帧的空间信息去更新，进一步增强之类的。当然主motivation 就是以上的内容。第二个点可以再想想。</li>
</ul>
<hr/>
<h2 id="投稿之后本来纠结像素相似性语义关系说法的准确性经过纠结以及看光流论文这个说法完全没问题光流差分很靠像素相似性过于直觉且容易失败对于退化而且本来也无法去理解全局的语义关系比如理解到这个人在跑步那他应该是怎么样的姿势他只能本能地去看这个像素像谁没有语义的概念所以实际上也容易出错所以我们要学习-high-level-的高层的语义动态对于局部的像素退化是不太敏感的而不是根据相似度学习像素关系反观gme-本来也就是去挖掘帧间语义关系的">投稿之后本来纠结像素相似性，语义关系说法的准确性
经过纠结以及看光流论文，这个说法完全没问题。光流/差分很靠像素相似性，过于直觉，且容易失败对于退化，而且本来也无法去理解全局的语义关系，比如理解到这个人在跑步，那他应该是怎么样的姿势。他只能本能地去看这个像素像谁，没有语义的概念，所以实际上也容易出错。所以我们要学习 high-level 的高层的语义动态，对于局部的像素退化是不太敏感的，而不是根据相似度学习像素关系。反观GME ，本来也就是去挖掘帧间语义关系的。<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#投稿之后本来纠结像素相似性语义关系说法的准确性经过纠结以及看光流论文这个说法完全没问题光流差分很靠像素相似性过于直觉且容易失败对于退化而且本来也无法去理解全局的语义关系比如理解到这个人在跑步那他应该是怎么样的姿势他只能本能地去看这个像素像谁没有语义的概念所以实际上也容易出错所以我们要学习-high-level-的高层的语义动态对于局部的像素退化是不太敏感的而不是根据相似度学习像素关系反观gme-本来也就是去挖掘帧间语义关系的" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>实验需要修改，我换一下 Backbone 网络试试，不用 pretrain 的 ViT，直接用原始的 ViT-L 和 ViT-B 去训练，看看结果，也好对比参数量。</p>
<hr/>
<h2 id="2025125-accept">2025.1.25 Accept<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#2025125-accept" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<h2 id="neurocomputing-中科院二区">Neurocomputing 中科院二区<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#neurocomputing-中科院二区" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<h2 id="这个工作也算是有个好的归宿了">这个工作也算是有个好的归宿了<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#这个工作也算是有个好的归宿了" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2></article><hr/><div class="page-footer"><div class="giscus" data-repo="FRunyang/runyang-notes" data-repo-id="R_kgDOQyjTww" data-category="Announcements" data-category-id="DIC_kwDOQyjTw84C0ftA" data-mapping="pathname" data-strict="0" data-reactions-enabled="1" data-input-position="bottom" data-light-theme="light" data-dark-theme="dark" data-theme-url="https://runyangnotes.top/static/giscus" data-lang="zh-CN"></div></div></div><div class="right sidebar"><div class="graph"><h3>Graph View</h3><div class="graph-outer"><div class="graph-container" data-cfg="{&quot;drag&quot;:true,&quot;zoom&quot;:true,&quot;depth&quot;:1,&quot;scale&quot;:1.1,&quot;repelForce&quot;:0.5,&quot;centerForce&quot;:0.3,&quot;linkDistance&quot;:30,&quot;fontSize&quot;:0.6,&quot;opacityScale&quot;:1,&quot;showTags&quot;:true,&quot;removeTags&quot;:[],&quot;focusOnHover&quot;:false,&quot;enableRadial&quot;:false}"></div><button class="global-graph-icon" aria-label="Global Graph"><svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 55 55" fill="currentColor" xml:space="preserve"><path d="M49,0c-3.309,0-6,2.691-6,6c0,1.035,0.263,2.009,0.726,2.86l-9.829,9.829C32.542,17.634,30.846,17,29,17
                s-3.542,0.634-4.898,1.688l-7.669-7.669C16.785,10.424,17,9.74,17,9c0-2.206-1.794-4-4-4S9,6.794,9,9s1.794,4,4,4
                c0.74,0,1.424-0.215,2.019-0.567l7.669,7.669C21.634,21.458,21,23.154,21,25s0.634,3.542,1.688,4.897L10.024,42.562
                C8.958,41.595,7.549,41,6,41c-3.309,0-6,2.691-6,6s2.691,6,6,6s6-2.691,6-6c0-1.035-0.263-2.009-0.726-2.86l12.829-12.829
                c1.106,0.86,2.44,1.436,3.898,1.619v10.16c-2.833,0.478-5,2.942-5,5.91c0,3.309,2.691,6,6,6s6-2.691,6-6c0-2.967-2.167-5.431-5-5.91
                v-10.16c1.458-0.183,2.792-0.759,3.898-1.619l7.669,7.669C41.215,39.576,41,40.26,41,41c0,2.206,1.794,4,4,4s4-1.794,4-4
                s-1.794-4-4-4c-0.74,0-1.424,0.215-2.019,0.567l-7.669-7.669C36.366,28.542,37,26.846,37,25s-0.634-3.542-1.688-4.897l9.665-9.665
                C46.042,11.405,47.451,12,49,12c3.309,0,6-2.691,6-6S52.309,0,49,0z M11,9c0-1.103,0.897-2,2-2s2,0.897,2,2s-0.897,2-2,2
                S11,10.103,11,9z M6,51c-2.206,0-4-1.794-4-4s1.794-4,4-4s4,1.794,4,4S8.206,51,6,51z M33,49c0,2.206-1.794,4-4,4s-4-1.794-4-4
                s1.794-4,4-4S33,46.794,33,49z M29,31c-3.309,0-6-2.691-6-6s2.691-6,6-6s6,2.691,6,6S32.309,31,29,31z M47,41c0,1.103-0.897,2-2,2
                s-2-0.897-2-2s0.897-2,2-2S47,39.897,47,41z M49,10c-2.206,0-4-1.794-4-4s1.794-4,4-4s4,1.794,4,4S51.206,10,49,10z"></path></svg></button></div><div class="global-graph-outer"><div class="global-graph-container" data-cfg="{&quot;drag&quot;:true,&quot;zoom&quot;:true,&quot;depth&quot;:-1,&quot;scale&quot;:0.9,&quot;repelForce&quot;:0.5,&quot;centerForce&quot;:0.2,&quot;linkDistance&quot;:30,&quot;fontSize&quot;:0.6,&quot;opacityScale&quot;:1,&quot;showTags&quot;:true,&quot;removeTags&quot;:[],&quot;focusOnHover&quot;:true,&quot;enableRadial&quot;:true}"></div></div></div><div class="toc desktop-only"><button type="button" class="toc-header" aria-controls="toc-13" aria-expanded="true"><h3>Table of Contents</h3><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="fold"><polyline points="6 9 12 15 18 9"></polyline></svg></button><ul id="list-1" class="toc-content overflow"><li class="depth-2"><a href="#目前v3_2版本得到了不错的结果可以作为一个不错的baseline模型架构如下" data-for="目前v3_2版本得到了不错的结果可以作为一个不错的baseline模型架构如下">目前v3_2版本得到了不错的结果，可以作为一个不错的baseline，模型架构如下：</a></li><li class="depth-2"><a href="#后续需要测试的baseline" data-for="后续需要测试的baseline">后续需要测试的Baseline：</a></li><li class="depth-1"><a href="#contribution" data-for="contribution">Contribution</a></li><li class="depth-1"><a href="#投了-cvpr24-没中但是感觉审稿意见质量挺低的准备继续投-eccv" data-for="投了-cvpr24-没中但是感觉审稿意见质量挺低的准备继续投-eccv">投了 CVPR24 没中，但是感觉审稿意见质量挺低的，准备继续投 ECCV</a></li><li class="depth-2"><a href="#一些反思即后续修稿方向" data-for="一些反思即后续修稿方向">一些反思即后续修稿方向</a></li><li class="depth-0"><a href="#第二次投稿更新-eccv第一轮意见也很差-没必要去-rebuttal-了准备重新整一下这个方法继续去投这篇我不打算去降低档次" data-for="第二次投稿更新-eccv第一轮意见也很差-没必要去-rebuttal-了准备重新整一下这个方法继续去投这篇我不打算去降低档次">第二次投稿更新 ECCV第一轮意见也很差 没必要去 Rebuttal 了，准备重新整一下这个方法，继续去投。这篇我不打算去降低档次。</a></li><li class="depth-2"><a href="#投稿反思" data-for="投稿反思">投稿反思</a></li><li class="depth-1"><a href="#投稿之后本来纠结像素相似性语义关系说法的准确性经过纠结以及看光流论文这个说法完全没问题光流差分很靠像素相似性过于直觉且容易失败对于退化而且本来也无法去理解全局的语义关系比如理解到这个人在跑步那他应该是怎么样的姿势他只能本能地去看这个像素像谁没有语义的概念所以实际上也容易出错所以我们要学习-high-level-的高层的语义动态对于局部的像素退化是不太敏感的而不是根据相似度学习像素关系反观gme-本来也就是去挖掘帧间语义关系的" data-for="投稿之后本来纠结像素相似性语义关系说法的准确性经过纠结以及看光流论文这个说法完全没问题光流差分很靠像素相似性过于直觉且容易失败对于退化而且本来也无法去理解全局的语义关系比如理解到这个人在跑步那他应该是怎么样的姿势他只能本能地去看这个像素像谁没有语义的概念所以实际上也容易出错所以我们要学习-high-level-的高层的语义动态对于局部的像素退化是不太敏感的而不是根据相似度学习像素关系反观gme-本来也就是去挖掘帧间语义关系的">投稿之后本来纠结像素相似性，语义关系说法的准确性
经过纠结以及看光流论文，这个说法完全没问题。光流/差分很靠像素相似性，过于直觉，且容易失败对于退化，而且本来也无法去理解全局的语义关系，比如理解到这个人在跑步，那他应该是怎么样的姿势。他只能本能地去看这个像素像谁，没有语义的概念，所以实际上也容易出错。所以我们要学习 high-level 的高层的语义动态，对于局部的像素退化是不太敏感的，而不是根据相似度学习像素关系。反观GME ，本来也就是去挖掘帧间语义关系的。</a></li><li class="depth-1"><a href="#2025125-accept" data-for="2025125-accept">2025.1.25 Accept</a></li><li class="depth-1"><a href="#neurocomputing-中科院二区" data-for="neurocomputing-中科院二区">Neurocomputing 中科院二区</a></li><li class="depth-1"><a href="#这个工作也算是有个好的归宿了" data-for="这个工作也算是有个好的归宿了">这个工作也算是有个好的归宿了</a></li><li class="overflow-end"></li></ul></div><div class="backlinks"><h3>Backlinks</h3><ul id="list-2" class="overflow"><li><a href="../A_Navigation/Experimental-Tracking" class="internal">Experimental Tracking</a></li><li><a href="../B_Experiments_Tracking/" class="internal">index</a></li><li class="overflow-end"></li></ul></div></div><footer class><p>Created with <a href="https://quartz.jzhao.xyz/">Quartz v4.5.2</a> © 2026</p><ul><li><a href="https://github.com/jackyzha0/quartz">GitHub</a></li><li><a href="https://discord.gg/cRFFHYye7t">Discord Community</a></li></ul></footer></div></div></body><script type="application/javascript" data-persist="true">function n(){let t=this.parentElement;t.classList.toggle("is-collapsed");let e=t.getElementsByClassName("callout-content")[0];if(!e)return;let l=t.classList.contains("is-collapsed");e.style.gridTemplateRows=l?"0fr":"1fr"}function c(){let t=document.getElementsByClassName("callout is-collapsible");for(let e of t){let l=e.getElementsByClassName("callout-title")[0],s=e.getElementsByClassName("callout-content")[0];if(!l||!s)continue;l.addEventListener("click",n),window.addCleanup(()=>l.removeEventListener("click",n));let o=e.classList.contains("is-collapsed");s.style.gridTemplateRows=o?"0fr":"1fr"}}document.addEventListener("nav",c);
</script><script type="module" data-persist="true">function E(a,e){if(!a)return;function t(o){o.target===this&&(o.preventDefault(),o.stopPropagation(),e())}function n(o){o.key.startsWith("Esc")&&(o.preventDefault(),e())}a?.addEventListener("click",t),window.addCleanup(()=>a?.removeEventListener("click",t)),document.addEventListener("keydown",n),window.addCleanup(()=>document.removeEventListener("keydown",n))}function f(a){for(;a.firstChild;)a.removeChild(a.firstChild)}var m=class{constructor(e,t){this.container=e;this.content=t;this.setupEventListeners(),this.setupNavigationControls(),this.resetTransform()}isDragging=!1;startPan={x:0,y:0};currentPan={x:0,y:0};scale=1;MIN_SCALE=.5;MAX_SCALE=3;cleanups=[];setupEventListeners(){let e=this.onMouseDown.bind(this),t=this.onMouseMove.bind(this),n=this.onMouseUp.bind(this),o=this.onTouchStart.bind(this),r=this.onTouchMove.bind(this),i=this.onTouchEnd.bind(this),s=this.resetTransform.bind(this);this.container.addEventListener("mousedown",e),document.addEventListener("mousemove",t),document.addEventListener("mouseup",n),this.container.addEventListener("touchstart",o,{passive:!1}),document.addEventListener("touchmove",r,{passive:!1}),document.addEventListener("touchend",i),window.addEventListener("resize",s),this.cleanups.push(()=>this.container.removeEventListener("mousedown",e),()=>document.removeEventListener("mousemove",t),()=>document.removeEventListener("mouseup",n),()=>this.container.removeEventListener("touchstart",o),()=>document.removeEventListener("touchmove",r),()=>document.removeEventListener("touchend",i),()=>window.removeEventListener("resize",s))}cleanup(){for(let e of this.cleanups)e()}setupNavigationControls(){let e=document.createElement("div");e.className="mermaid-controls";let t=this.createButton("+",()=>this.zoom(.1)),n=this.createButton("-",()=>this.zoom(-.1)),o=this.createButton("Reset",()=>this.resetTransform());e.appendChild(n),e.appendChild(o),e.appendChild(t),this.container.appendChild(e)}createButton(e,t){let n=document.createElement("button");return n.textContent=e,n.className="mermaid-control-button",n.addEventListener("click",t),window.addCleanup(()=>n.removeEventListener("click",t)),n}onMouseDown(e){e.button===0&&(this.isDragging=!0,this.startPan={x:e.clientX-this.currentPan.x,y:e.clientY-this.currentPan.y},this.container.style.cursor="grabbing")}onMouseMove(e){this.isDragging&&(e.preventDefault(),this.currentPan={x:e.clientX-this.startPan.x,y:e.clientY-this.startPan.y},this.updateTransform())}onMouseUp(){this.isDragging=!1,this.container.style.cursor="grab"}onTouchStart(e){if(e.touches.length!==1)return;this.isDragging=!0;let t=e.touches[0];this.startPan={x:t.clientX-this.currentPan.x,y:t.clientY-this.currentPan.y}}onTouchMove(e){if(!this.isDragging||e.touches.length!==1)return;e.preventDefault();let t=e.touches[0];this.currentPan={x:t.clientX-this.startPan.x,y:t.clientY-this.startPan.y},this.updateTransform()}onTouchEnd(){this.isDragging=!1}zoom(e){let t=Math.min(Math.max(this.scale+e,this.MIN_SCALE),this.MAX_SCALE),n=this.content.getBoundingClientRect(),o=n.width/2,r=n.height/2,i=t-this.scale;this.currentPan.x-=o*i,this.currentPan.y-=r*i,this.scale=t,this.updateTransform()}updateTransform(){this.content.style.transform=`translate(${this.currentPan.x}px, ${this.currentPan.y}px) scale(${this.scale})`}resetTransform(){let t=this.content.querySelector("svg").getBoundingClientRect(),n=t.width/this.scale,o=t.height/this.scale;this.scale=1,this.currentPan={x:(this.container.clientWidth-n)/2,y:(this.container.clientHeight-o)/2},this.updateTransform()}},T=["--secondary","--tertiary","--gray","--light","--lightgray","--highlight","--dark","--darkgray","--codeFont"],y;document.addEventListener("nav",async()=>{let e=document.querySelector(".center").querySelectorAll("code.mermaid");if(e.length===0)return;y||=await import("https://cdnjs.cloudflare.com/ajax/libs/mermaid/11.4.0/mermaid.esm.min.mjs");let t=y.default,n=new WeakMap;for(let r of e)n.set(r,r.innerText);async function o(){for(let s of e){s.removeAttribute("data-processed");let c=n.get(s);c&&(s.innerHTML=c)}let r=T.reduce((s,c)=>(s[c]=window.getComputedStyle(document.documentElement).getPropertyValue(c),s),{}),i=document.documentElement.getAttribute("saved-theme")==="dark";t.initialize({startOnLoad:!1,securityLevel:"loose",theme:i?"dark":"base",themeVariables:{fontFamily:r["--codeFont"],primaryColor:r["--light"],primaryTextColor:r["--darkgray"],primaryBorderColor:r["--tertiary"],lineColor:r["--darkgray"],secondaryColor:r["--secondary"],tertiaryColor:r["--tertiary"],clusterBkg:r["--light"],edgeLabelBackground:r["--highlight"]}}),await t.run({nodes:e})}await o(),document.addEventListener("themechange",o),window.addCleanup(()=>document.removeEventListener("themechange",o));for(let r=0;r<e.length;r++){let v=function(){let g=l.querySelector("#mermaid-space"),h=l.querySelector(".mermaid-content");if(!h)return;f(h);let w=i.querySelector("svg").cloneNode(!0);h.appendChild(w),l.classList.add("active"),g.style.cursor="grab",u=new m(g,h)},M=function(){l.classList.remove("active"),u?.cleanup(),u=null},i=e[r],s=i.parentElement,c=s.querySelector(".clipboard-button"),d=s.querySelector(".expand-button"),p=window.getComputedStyle(c),L=c.offsetWidth+parseFloat(p.marginLeft||"0")+parseFloat(p.marginRight||"0");d.style.right=`calc(${L}px + 0.3rem)`,s.prepend(d);let l=s.querySelector("#mermaid-container");if(!l)return;let u=null;d.addEventListener("click",v),E(l,M),window.addCleanup(()=>{u?.cleanup(),d.removeEventListener("click",v)})}});
</script><script src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/copy-tex.min.js" type="application/javascript" data-persist="true"></script><script src="../postscript.js" type="module" data-persist="true"></script></html>