{"A_Navigation/Experimental-Tracking":{"slug":"A_Navigation/Experimental-Tracking","filePath":"A_Navigation/Experimental Tracking.md","title":"Experimental Tracking","links":["B_Experiments_Tracking/傅里叶变换（频域）在视觉中的应用","B_Experiments_Tracking/TDMI扩展","B_Experiments_Tracking/时序不变性+时序变化性","B_Experiments_Tracking/关节结构化约束","B_Experiments_Tracking/基于互信息的时序差分学习","B_Experiments_Tracking/扩散模型应用","B_Experiments_Tracking/Global-思想","B_Experiments_Tracking/Global-Local-Attention","B_Experiments_Tracking/学习鲁棒的时间动态和空间适应用于-VHPE","B_Experiments_Tracking/MambaPose","B_Experiments_Tracking/多假设实验","B_Experiments_Tracking/半监督视频姿态估计","B_Experiments_Tracking/从局部关节出发发散姿态","B_Experiments_Tracking/极度低光条件下的姿态估计","B_Experiments_Tracking/Test-time-模型改进","B_Experiments_Tracking/Implicit-Representation","B_Experiments_Tracking/分层信息姿态估计","B_Experiments_Tracking/姿态模板","B_Experiments_Tracking/特征上采样在姿态估计的应用","B_Experiments_Tracking/3D-信息在姿态估计的应用","B_Experiments_Tracking/基于Optimal-Transport的结构化约束","B_Experiments_Tracking/解纠缠特征对齐实验"],"tags":[],"content":"做过/思考的实验完整记录\nIn Progress\n\n投稿的论文+正在做的实验\n\n\n傅里叶变换（频域）在视觉中的应用\nTDMI扩展\n时序不变性+时序变化性\n关节结构化约束\n\n\nDone\n\n基于互信息的时序差分学习\n扩散模型应用\nGlobal 思想\nGlobal-Local Attention\n\n\n学习鲁棒的时间动态和空间适应用于 VHPE\n\n\n\n\nMambaPose\n\n\n\n\n多假设实验\n\n\n\n\nNot Started\n\n未开始做或者别人做过的 idea\n\n\n半监督视频姿态估计\n从局部关节出发发散姿态\n极度低光条件下的姿态估计\nTest-time 模型改进\nImplicit Representation\n分层信息姿态估计\n姿态模板\n特征上采样在姿态估计的应用\n3D 信息在姿态估计的应用\n\n\nFailure\n\n可能无效的 idea\n\n\n基于Optimal Transport的结构化约束\n解纠缠特征对齐实验\n"},"A_Navigation/Others":{"slug":"A_Navigation/Others","filePath":"A_Navigation/Others.md","title":"Others","links":["F_Others/软件购买信息记录","F_Others/过去，现在，将来？-致自己","F_Others/将夜人物群像","F_Others/《盗墓笔记》里的终极到底是什么？-—-Ailyf-的回答---知乎","F_Others/为何对于《将夜》，许多人都宁可选择莫山山，而不选择桑桑？---知乎","F_Others/我的楼兰","F_Others/我记得"],"tags":[],"content":"\n一些反思、思考，记录某些时候突然想写的\n\n信息保存\n软件购买信息记录\n随性\n过去，现在，将来？-致自己\n将夜人物群像\n《盗墓笔记》里的终极到底是什么？ — Ailyf 的回答 - 知乎\n为何对于《将夜》，许多人都宁可选择莫山山，而不选择桑桑？ - 知乎\n我的楼兰\n我记得"},"A_Navigation/Paper-Related":{"slug":"A_Navigation/Paper-Related","filePath":"A_Navigation/Paper Related.md","title":"Paper Related","links":["D_Paper_Related/写作基础-and-总结","D_Paper_Related/SCI-写作常用高级词法","D_Paper_Related/论文创新点观察","D_Paper_Related/英文论文写作查重","D_Paper_Related/工具合集整理","D_Paper_Related/DBLP-检索式","A_Navigation/Experimental-Tracking","D_Paper_Related/Self-Constrained-Inference-Optimization-on-Structural-Groups-for-Human-Pose-Estimation","D_Paper_Related/Look-Back-and-Forth--Video-Super-Resolution-with-Explicit-Temporal-Difference-Modeling","D_Paper_Related/Local-Texture-Estimator-for-Implicit-Representation-Function","D_Paper_Related/Learning-Continuous-Image-Representation-with-Local-Implicit-Image-Function","D_Paper_Related/TDN--Temporal-Difference-Networks-for-Efﬁcient-Action-Recognition","D_Paper_Related/STM--SpatioTemporal-and-Motion-Encoding-for-Action-Recognition","D_Paper_Related/TEA--Temporal-Excitation-and-Aggregation-for-Action-Recognition","D_Paper_Related/TEINet--Towards-an-Efﬁcient-Architecture-for-Video-Recognition","D_Paper_Related/Recognize-Actions-by-Disentangling-Components-of-Dynamics","D_Paper_Related/Searching-Central-Difference-Convolutional-Networks-for-Face-Anti-Spooﬁng","D_Paper_Related/InternImage--Exploring-Large-Scale-Vision-Foundation-Models-with-Deformable-Convolutions","D_Paper_Related/Dynamic-Context-Sensitive-Filtering-Network-for-Video-Salient-Object-Detection","D_Paper_Related/Motion-Guided-Attention-for-Video-Salient-Object-Detection","D_Paper_Related/TF-Blender--Temporal-Feature-Blender-for-Video-Object-Detection","D_Paper_Related/Not-All-Tokens-Are-Equal--Human-centric-Visual-Analysis-via-Token-Clustering-Transformer","D_Paper_Related/Spatio-Temporal-Representation-Factorization-for-Video-based-Person-Re-Identification","D_Paper_Related/Disentangled-Representation-for-Age-Invariant-Face-Recognition--A-Mutual-Information-Minimization-Perspective","D_Paper_Related/Vision-Mamba---Efficient-Visual-Representation-Learning-with-Bidirectional-State-Space-Model","D_Paper_Related/VMamba---Visual-State-Space-Model","D_Paper_Related/U-Mamba---Enhancing-Long-range-Dependency-for-Biomedical-Image-Segmentation","D_Paper_Related/Mamba-UNet---UNet-Like-Pure-Visual-Mamba-for-Medical-Image-Segmentation","D_Paper_Related/U-shaped-Vision-Mamba-for-Single-Image-Dehazing","D_Paper_Related/VM-UNet---Vision-Mamba-UNet-for-Medical-Image-Segmentation","D_Paper_Related/VM-UNET-V2-Rethinking-Vision-Mamba-UNet-for-Medical-Image-Segmentation"],"tags":[],"content":"Writing\n\n写作基础&amp;总结\nSCI 写作常用高级词法\n论文创新点观察\n英文论文写作查重\n工具合集整理\nDBLP 检索式\n\n\nReading\n\n这里不会所有的都放，大概率会放到Experimental Tracking\n\nHuman Pose Estimation\n\nSelf-Constrained Inference Optimization on Structural Groups for Human Pose Estimation\n\nSuper-Resolution\n\nLook Back and Forth- Video Super-Resolution with Explicit Temporal Difference Modeling\nLocal Texture Estimator for Implicit Representation Function\nLearning Continuous Image Representation with Local Implicit Image Function\n\nAction Recognition\n\nTDN- Temporal Difference Networks for Efﬁcient Action Recognition\nSTM- SpatioTemporal and Motion Encoding for Action Recognition\nTEA- Temporal Excitation and Aggregation for Action Recognition\nTEINet- Towards an Efﬁcient Architecture for Video Recognition\nRecognize Actions by Disentangling Components of Dynamics\n\nConvolution\n\nSearching Central Difference Convolutional Networks for Face Anti-Spooﬁng\nInternImage- Exploring Large-Scale Vision Foundation Models with Deformable Convolutions\n\nObject Detection\n\nDynamic Context-Sensitive Filtering Network for Video Salient Object Detection\nMotion Guided Attention for Video Salient Object Detection\nTF-Blender- Temporal Feature Blender for Video Object Detection\n\nHuman-centric Visual Analysis\n\nNot All Tokens Are Equal- Human-centric Visual Analysis via Token Clustering Transformer\n\nPerson Re-Identification\n\nSpatio-Temporal Representation Factorization for Video-based Person Re-Identification\n\nFace Recognition\n\nDisentangled Representation for Age-Invariant Face Recognition- A Mutual Information Minimization Perspective\n\nMamba\n\nVision Mamba - Efficient Visual Representation Learning with Bidirectional State Space Model\nVMamba - Visual State Space Model\nU-Mamba - Enhancing Long-range Dependency for Biomedical Image Segmentation\nMamba-UNet - UNet-Like Pure Visual Mamba for Medical Image Segmentation\nU-shaped Vision Mamba for Single Image Dehazing\nVM-UNet - Vision Mamba UNet for Medical Image Segmentation\nVM-UNET-V2 Rethinking Vision Mamba UNet for Medical Image Segmentation\n"},"A_Navigation/Research-Knowledge":{"slug":"A_Navigation/Research-Knowledge","filePath":"A_Navigation/Research Knowledge.md","title":"Research Knowledge","links":["扩散模型","C_Research_Knowledge/Mamba-模型","C_Research_Knowledge/Conformal-Prediction","C_Research_Knowledge/深度学习入门系列一-梯度下降法","C_Research_Knowledge/深度学习入门系列一-梯度下降法-②","C_Research_Knowledge/梯度下降法3","C_Research_Knowledge/再探梯度下降法之梯度","C_Research_Knowledge/深度学习入门系列4-反向传播BP算法","C_Research_Knowledge/深度学习入门系列-逻辑回归","C_Research_Knowledge/深度学习入门系列6-Convolution-Neural-Network-CNN-卷积神经网络","C_Research_Knowledge/深度学习入门系列7-Tips-For-DeepLearning-全程高能","C_Research_Knowledge/深度学习入门系列8-Tips-For-DeepLearning-2-全程高能","C_Research_Knowledge/从分类到逻辑回归","C_Research_Knowledge/高斯混合模型","C_Research_Knowledge/机器学习中判别式模型和生成式模型","C_Research_Knowledge/卷积、反卷积图像尺寸计算","C_Research_Knowledge/卷积之空洞卷积与形变卷积","C_Research_Knowledge/马尔可夫链-Markov-Chain","C_Research_Knowledge/牛顿法","C_Research_Knowledge/朴素贝叶斯分类器","C_Research_Knowledge/深度学习实践","C_Research_Knowledge/再探形变卷积","C_Research_Knowledge/最小二乘法和梯度下降法","C_Research_Knowledge/Attention方法","C_Research_Knowledge/Auto-Encoder-in-DeepLearning","C_Research_Knowledge/Batch-Normalization","C_Research_Knowledge/Classification","C_Research_Knowledge/CNN-Receptive-Field-感受野","C_Research_Knowledge/Contrastive-Loss-对比损失-分析","C_Research_Knowledge/Cross-Entropy-的前世今生","C_Research_Knowledge/GAN对抗网络初识","C_Research_Knowledge/GAN-Basic-Theory","C_Research_Knowledge/手动实现GAN网络生成动漫头像","C_Research_Knowledge/GAN-4-f-GAN推导","C_Research_Knowledge/HMM隐马尔可夫模型-到-CRF条件随机域","C_Research_Knowledge/Recurrent-Neural-Network-RNN","C_Research_Knowledge/Softmax函数详解","C_Research_Knowledge/SVM支持向量机","C_Research_Knowledge/Why-Deep","C_Research_Knowledge/ResNet-网络分析","C_Research_Knowledge/深度学习入门系列-深度学习的误差来自哪里","C_Research_Knowledge/概率论的贝叶斯公式","C_Research_Knowledge/均值与期望","C_Research_Knowledge/联合概率","C_Research_Knowledge/似然估计-Likelyhood","C_Research_Knowledge/数学中的李群初探","C_Research_Knowledge/图像仿射变换后的坐标求解","C_Research_Knowledge/图像直方图及其均衡化算法"],"tags":[],"content":"New Techniques\n\n扩散模型\n**Mamba 模型\nConformal Prediction\n\n\nDeep Learning (History)\n\n深度学习入门系列一-梯度下降法\n深度学习入门系列一-梯度下降法-②\n梯度下降法3\n再探梯度下降法之梯度\n深度学习入门系列4-反向传播BP算法\n深度学习入门系列-逻辑回归\n深度学习入门系列6-Convolution-Neural-Network-CNN-卷积神经网络\n深度学习入门系列7-Tips-For-DeepLearning-全程高能\n深度学习入门系列8-Tips-For-DeepLearning-2-全程高能\n从分类到逻辑回归\n高斯混合模型\n机器学习中判别式模型和生成式模型\n卷积、反卷积图像尺寸计算\n卷积之空洞卷积与形变卷积\n马尔可夫链-Markov-Chain\n牛顿法\n朴素贝叶斯分类器\n深度学习实践\n再探形变卷积\n最小二乘法和梯度下降法\nAttention方法\nAuto-Encoder-in-DeepLearning\nBatch-Normalization\nClassification\nCNN-Receptive-Field-感受野\nContrastive-Loss-对比损失-分析\nCross-Entropy-的前世今生\nGAN对抗网络初识\nGAN-Basic-Theory\n手动实现GAN网络生成动漫头像\nGAN-4-f-GAN推导\nHMM隐马尔可夫模型-到-CRF条件随机域\nRecurrent-Neural-Network-RNN\nSoftmax函数详解\nSVM支持向量机\nWhy-Deep\nResNet-网络分析\n深度学习入门系列-深度学习的误差来自哪里\n\n\nMath\n\n概率论的贝叶斯公式\n均值与期望\n联合概率\n牛顿法\n似然估计-Likelyhood\n数学中的李群初探\n\n\nDigital Image\n\n图像仿射变换后的坐标求解\n图像直方图及其均衡化算法\n"},"A_Navigation/Summary":{"slug":"A_Navigation/Summary","filePath":"A_Navigation/Summary.md","title":"Summary","links":["G_Personal_Summary/3.17-->-3.23"],"tags":[],"content":"\n个人需要多做总结，每周目标，每月目标，季度目标，等等。\n\nWeekly\n3.17 → 3.23\nMonthly\nYearly"},"A_Navigation/Technique-Explores":{"slug":"A_Navigation/Technique-Explores","filePath":"A_Navigation/Technique Explores.md","title":"Technique Explores","links":["E_Technique_Explores/服务器初始环境安装与配置","E_Technique_Explores/记录MMDetection-ConvNext配置过程","E_Technique_Explores/内网穿透工具","E_Technique_Explores/一套用于人体姿态估计的关键点标注流程","E_Technique_Explores/Linux操作整理","E_Technique_Explores/Mac-pycharm-任意版本永久激活","E_Technique_Explores/pycharm常见问题合集","E_Technique_Explores/pycocotools-和-numpy-的兼容性问题","E_Technique_Explores/win10环境配置maskrcnn-benchmark","E_Technique_Explores/win10环境下detectron2配置","E_Technique_Explores/PyTorch冻结部分参数训练","E_Technique_Explores/Pytorch-Dataloader-用法","E_Technique_Explores/pytorch-numpy中的shape和size","E_Technique_Explores/Pytorch中的GPU调用","E_Technique_Explores/从0开始搭建私人云盘","E_Technique_Explores/爬虫入门","E_Technique_Explores/拥有自己优雅的图床","E_Technique_Explores/自己的域名与github绑定","E_Technique_Explores/MyBlog-Hexo快速搭建","E_Technique_Explores/hexo-blog搭建2-主题相关","E_Technique_Explores/hugo_blog搭建部署笔记","E_Technique_Explores/DeepFaceLab-操作","E_Technique_Explores/git错误与代理相关","E_Technique_Explores/git推送文件","E_Technique_Explores/python-库-you-get下载视频","E_Technique_Explores/Ubuntu16-18可道云云盘搭建","E_Technique_Explores/windwos下几款实用又美观的软件推荐","E_Technique_Explores/斐波那契额数列几种实现方式","E_Technique_Explores/个人信息管理系统-Web项目总结分析","E_Technique_Explores/计算机网络-IP地址-端口","E_Technique_Explores/牛客网刷题笔记-剑指offer系","E_Technique_Explores/JavaWeb简要总结-Mysql-JDBC","E_Technique_Explores/AECC2020中英文切换","E_Technique_Explores/X-Particles-4-0-粒子"],"tags":[],"content":"实验配置（踩坑）\n\n服务器初始环境安装与配置\n记录MMDetection-ConvNext配置过程\n内网穿透工具\n一套用于人体姿态估计的关键点标注流程\nLinux操作整理\nMac pycharm 任意版本永久激活\npycharm常见问题合集\npycocotools 和 numpy 的兼容性问题\nwin10环境配置maskrcnn-benchmark\nwin10环境下detectron2配置\n\n\nPyTorch相关\n\nPyTorch冻结部分参数训练\nPytorch-Dataloader-用法\npytorch-numpy中的shape和size\nPytorch中的GPU调用\n\n\n其他技术 (CS方向)\n\n从0开始搭建私人云盘\n爬虫入门\n拥有自己优雅的图床\n自己的域名与github绑定\nMyBlog-Hexo快速搭建\nhexo-blog搭建2-主题相关\nhugo_blog搭建部署笔记\nDeepFaceLab 操作\ngit错误与代理相关\ngit推送文件\npython-库-you-get下载视频\nUbuntu16-18可道云云盘搭建\nwindwos下几款实用又美观的软件推荐\n\n\n编程刷题\n\n斐波那契额数列几种实现方式\n个人信息管理系统-Web项目总结分析\n计算机网络-IP地址-端口\n牛客网刷题笔记-剑指offer系\nJavaWeb简要总结-Mysql-JDBC\n\n\n视觉设计\n\nAECC2020中英文切换\nX-Particles-4-0-粒子\n"},"A_Navigation/index":{"slug":"A_Navigation/index","filePath":"A_Navigation/index.md","title":"index","links":["A_Navigation/Experimental-Tracking","A_Navigation/Paper-Related","A_Navigation/Research-Knowledge","A_Navigation/Technique-Explores","A_Navigation/Others","A_Navigation/Summary"],"tags":[],"content":"\n导航页面，表示整体的笔记布局\n\n\n一、Experimental Tracking\n二、Paper Related\n三、Research Knowledge\n四、Technique Explores\n五、Others\n六、Summary"},"B_Experiments_Tracking/3D-信息在姿态估计的应用":{"slug":"B_Experiments_Tracking/3D-信息在姿态估计的应用","filePath":"B_Experiments_Tracking/3D 信息在姿态估计的应用.md","title":"3D 信息在姿态估计的应用","links":[],"tags":["Pose-Estimation"],"content":"利用 3D 人体 mesh 信息，促进 2D 人体姿态估计。该方法理论上可促进图像/视频人体姿态估计。\n首先，SAM3D 可以提取图像中的 mesh\n\n引入 Mesh 信息，做特征融合；\n可以提取每个实例/物体的 mesh，然后和二维图像做 cross attention，这样可以学习 3D 的跨实例交互关系，促进姿态推理\n"},"B_Experiments_Tracking/Global-思想":{"slug":"B_Experiments_Tracking/Global-思想","filePath":"B_Experiments_Tracking/Global 思想.md","title":"Global 思想","links":[],"tags":["Pose-Estimation"],"content":"短期内遮挡严重，用全局信息去做：\n\n取一个长序列，使用一个Transformer中的Encoder对每帧进行编码（显式输入帧号作为位置编码），得到一个Vector\n用当前帧的编码与其他帧计算相似度，作为每帧的概率\n取Top N相似的帧，对其特征进行融合\n动态Conv作为解码器，以当前帧作为模板，生成不同尺寸的卷积核，到聚合的特征中进行卷积操作，最终聚合得到heatmap预测\n\n\n以上实现存在问题，分析：\n\nstage1的特征过于浅层，直接进行融合后，直接用动态卷积计算heatmap网络层过浅\n\n进行改进：\n\nstage1 融合之后，输入后续HRNet进行训练（正在训练）\n\n\n改进二：\n\n为了快速出效果，在stage3特征进行融合；\n初步融合之后，使用边界attention操作为当前帧补充信息；\n随后使用卷积+动态卷积进行解码\n\n\nTOMM论文 《GLPose: Global-Local Representation Learning for Human Pose Estimation》已录用。"},"B_Experiments_Tracking/Global-Local-Attention":{"slug":"B_Experiments_Tracking/Global-Local-Attention","filePath":"B_Experiments_Tracking/Global-Local Attention.md","title":"Global-Local Attention","links":[],"tags":["Pose-Estimation"],"content":"Intra-frame Attention\nCross-frame Attention\n\n\n（Personalized Feature Extraction module）个性化的图像特征抽取 (Insatnce-aware)\n在 CNN Backbone抽取出特征之后，使用Deformable Conv进行个性化特征提取\n5帧 cat-conv\nbasicblock 序列特征\n\n\nself-feature refinement （VIT / VItPose）\n\n输入是当前帧\nMulti-head self-attention\n48 patch transformer\n8 block\n\n\n\ncross-frame interaction / temporal learning （MHFormer）\n\nq当前 hrent+可形变+vit\nk v序列 序列特征 basicblock\n先算self-attention，再算有交互的\nMulti-Frame Cross-Attention\n\n\n\n\ndisentangled keypoint decoder （Bottom-up keypoint regression ）\n\n\n[batch,48,96,72] 3*3 [batch,1,96,72]\n\n\n用17个CNN，每个输出1个通道\n\n\n用17个head 分别去解码每个关节的heatmap\n分关节考虑计算，17个head，关节解耦合\n\nNovelty：\n\n全局局部模型架构，整个流程说一下\n以当前为查询 去序列特征搜索，建模更相关的特征\n解耦合（decoupling/disentanglement）的关键点解码\n效果好\n\n\nExperiments：\nPoseTrack17:\n88.9037 | 89.6823 | 85.5904 | 79.5469 | 84.2159 | 83.1099 | 75.8414 | 84.179\nPoseTrack18\n84.3253 | 87.4536 | 83.463 | 78.5233 | 80.8873 | 80.2367 | 74.4472 | 81.5332\nSub-JHMDB\nSplit1: 0.994 | 0.9905 | 0.9609 | 0.9334 | 0.9913 | 0.9212 | 0.9369 | 0.963 Split2: 0.9929 | 0.9826 | 0.9217 | 0.8782 | 0.9915 | 0.9169 | 0.9576 | 0.9516 Split3: 0.9923 | 0.9846 | 0.9469 | 0.9311 | 0.994 | 0.9189 | 0.9651 | 0.9646\nMean：\n\n\n\n\n\nTCSVT paper 已录用"},"B_Experiments_Tracking/Implicit-Representation":{"slug":"B_Experiments_Tracking/Implicit-Representation","filePath":"B_Experiments_Tracking/Implicit Representation.md","title":"Implicit Representation","links":[],"tags":["Pose-Estimation"],"content":"图像的隐式表示可以表征任意分辨率的图像/特征：\n对于Feature-Level的表示\n\n主要是训练一个解码器 f , 对于原始feature map的xq位置，z_表示它的latent code（即特征的值），x_表示2d坐标（每个element都分配一个2d坐标）\n\n后续在学习学习这个方向 感觉很容易用到姿态估计"},"B_Experiments_Tracking/MambaPose":{"slug":"B_Experiments_Tracking/MambaPose","filePath":"B_Experiments_Tracking/MambaPose.md","title":"MambaPose","links":[],"tags":["Pose-Estimation"],"content":"docs.qq.com/sheet/DZUlqR0xGdGRqa2R5\n解耦合的时空 mamba 网络用于人体姿态估计\n把 Mamba 用于视频姿态估计感觉可以做：\n\n\n表征学习：shared mamba block → 增强每帧的空间表达 + sequence mamba block → 增强时序表示\n\n\n初始的 heatmap / 坐标， 可以想下怎么用\n\n\n\n空间序列+时间序列\n每一帧先做高分辨率增强；\n然后可以进行 pooling，拿出有用的信息，做时序建模 （这里感觉可以多尺度，pixel + patch ）\n\n，目前可以做一个 baseline  HRNet+SpaceMamba + 时序 Mamba\n空间 Mamba 可以用多尺度Concat建模\n\n*编译 Mamba 出错了\nMamba 编译踩坑：\n\n主要是版本得符合，版本对应直接就可以\n用 torch2.0 + cuda11.7 这两个版本对应可以直接安装 Mamba\n\nV1\n空间先用 Mamba 增强；\n\n时间上分为两块，一块是把多帧拼成一张大图，直接让模型去学习 （交叉着可行变卷积）；一块是通道拼接，做交互；\n训练不稳定\n\n\nV2 版本去掉 DCNV2\n初步效果 V1 V2 效果很类似，不相上下，已经很不错，相比 CNN，提高了 1 个点。\n但是效果依然还是远远不够的，需要思考进一步怎么去提点。\n\n感觉下一步可以把特征对齐给加上— 但是特征对齐大概率应该是没啥用的，因为用 mamba 扫描的过程中已经考虑了全局信息—也不一定，我先试试用DCN 去做对齐然后在聚合，相当于改了一下 head\n\n这个是 v2 版本的特征图，能看出来 mamba 模型不能实现很好的对应，所以可以加上可形变对齐的那个试试。也就是说存在对齐问题\n\n继续跑下 V3 V4 —\nV3：V2+可形变对齐聚合\nV4：三向时序融合建模 + 可形变对齐聚合\nv3 v4 均无效\n\n在 v3 基础上加互信息试试— deformable align 其实没用 所以可以在 v2 基础上加互信息\n之前的工作已经证明高分辨率表征学习很重要对于准确的姿态热图。\n然而，学习高分辨率时序建模是非常具有挑战性的。transformer\n为了将成功的经验迁移到视频中，大量的工作已经做出了努力。\n在时序姿态估计中，学习高分辨率时序建模是有挑战的。\n\n全维度（向）高分辨率全局时序建模策略。显式直接建模全序列的动态在横向、纵向以及深度方向。学习 semantic 以及 detail 的动态信息。— 全局高分辨率时序建模\n局部序列节别时空可形变管道注意力，自适应学习局部信息 — 局部时空调优聚合\n故事可以按照上面的来讲。\n\n\n\n*发现互信息不 work，训练一直 NAN\n加上 SE Attention 分层融合之类的，都没啥用 所以重点就还是 mamba 故事怎么讲\n*所以只能走 B 计划，换 Backbone\n\n\n无压力建模高分辨率视频序列\n\n加一些设计，比如说*多尺度序列建模\n\n直接用 Transformer Backbone\n然后把分辨率上采样两倍，然后用 Mamba 建模时序看看效果。\n读一下综述和论文找找灵感 现在还缺一个引子\n\n实验上看到一篇 VIT-B 直接可以到86.2 的精度，所以感觉应该先试试去 train 一下 ViT-B，但是不知道他 bounding box 用的是啥 === VIT-B 训练了 50 epoch 效果很差，换了 box 效果依然很差，得 debug 看看是什么问题。不知道是不是 sigma 的问题 感觉是 batch 太大，更新少了好几倍的关系 — 试试 train 150epoch — 再试试小batch 训练方案 —\nViT-B 72.3 ｜ 50epoch 80.2｜大 batch 我试试 150 / 100 epoch\n正在测 TDMI 看看有没有问题（没问题）。待会儿也去测一下Vit-H（没问题），以前训练好的模型拿来测测看看效果咋样，确保代码啥的没问题（没问题）。然后测试一下 ViT-B 原本模型的效果（72.6），最后再去调整 ViTB 的训练。调整思路感觉就是改一下 SIGMA，学习率，weight decay 之类的\n\n\nMambaPoseV7 版本\n\n时空 mamba（空间调制（逐帧过滤重要信息）+全向时序建模（全局动态捕获+语义整合））写论文重点就是高分辨率全向序列建模\n局部时空优化-自适应采样局部信息进行优化细节\n\nV7精度 83.5，退化了；首先是 SIGMA=3 效果更好，所以后续实验都用 SIGMA=3\n\n分析精度退化的问题，我觉得可以改进一下，试试两个版本的实验：\n\n低分辨率（16 * 12）直接Mamba做序列建模，然后直接用当前帧信息 84.8 加上时间编码 84.6\n低分辨率序列建模+可形变融合 84.4\n很奇怪的一点是多帧融合之后效果会退化,我感觉就是 backbone 的特征表达能力被破坏了\n待会儿也试试加上时间编码看看\n还有一种可能就是 Backbone 的特征不应该上采样去用，而应该跟 HRNet一样，吧 final layer 之前的特征拿来，可能会更好。这一点等第一个实验结果出来之后看看。所以可以先尝试在低分辨率上加入时间编码\n低分辨率（16 * 12）来自 Backbone + 高分辨率（64 * 48） 来自 keypoint-head 的双分辨率序列建模 84.8\n三个版本挂上了，同时把 weight_decay调整到了 0.01\n\n\n\n统一的精度提高，说明横向 cat 的mamba 结果不应该和通道 cat 的直接加；或者就是 weightdecay 的影响\n给输入特征加上时间编码，低分辨率做一组实验，单纯高分辨率也做一组实验\n\n\n\n低分辨率 （current frame）+ 时间编码 84.5\n高分辨率+时间编码 + 用当前帧 85.4（有希望能调上去 说明后面还是得高分辨率特征,确实很有希望）\n高分辨率+时间编码+可形变局部优化融合 85.2\n\n\n感觉下一步可以试试， 低分辨率+高分辨率特征融合，不过都用当前帧特征，可以把两个特征融合完用一个 SEAttention；然后这两个版本实验都把 encodeing 去掉\n\n高分辨率当前帧+SE 85.3 说明 SEAttention 没啥用，不如时间编码\n低分辨率+高分辨 85.0 —⇒ 新版本低分辨率+高分辨率 85.3\n高分辨率动态+语义  先进行语义建模，再进行动态建模，以及局部融合85.1\n高分辨率 全局动态和语义并行建模并融合，+ 局部可行变融合 85.4\n\n\n下一步需要开发出一个不退化的融合方案：感觉可以多帧融合，加到当前帧特征上去\n对，把局部对齐加进去，多帧融合；\n\n只用高分辨率多帧融合\n两种高-低分辨率融合方案：\n把低分辨率上采样，和高分辨率融合\n构造一个特征金字塔，低分辨率上采样 高分辨率下采样 但是这种感觉有点问题 就是 语义和最终的融合问题\n\n\n目前实验总结：\n\n目前一直没超过 backbone 比较伤感。应该是特征的损耗。对于 Transformer 提取的特征来说，全局特征感觉是够的，就是局部特征有点奇怪。\n\n\n只用高分辨率，全局融合，有 85.4 的精度 （目前最高）\n高分辨率下，全局 mamba + 局部的可行变融合，会掉点。既然是为了凑创新点，那这个模块得想想其他方法，不能简单的一个可形变\n目前高分辨率+低分辨率，带局部融合， 85.3\n在比较高的精度中，都是当前帧特征发挥的比较好。所以这么看，不如调整一下局部融合的方法，根据 Mamba 建模的全局动态,感觉每一帧是不是应该单独处理，然后用 mamba 吸收全局动态 ----- 五帧像素 每个都关注全序列信息，需要进一步做局部增强 等出了结果在改代码把\nGMLP 高了 0.1， 85.5 --- 我觉得回头可以试试用 ViT-L\n加时间编码掉了 0.2 85.3\n\n\n\n这个工作我想想，我觉得需要做的比较完善才可以：\n\nHRNet 和 ViT作为 baseline，我觉得都需要，可以更好的展示我们方法的效果。也就是 baseline，baseline+全局 mamba，baseline+全局 mamba+局部优化\n关于高分辨率，可以是先用低分辨率建模，然后对比引入高分辨率的效果，这块就用 ViT 作为 baseline 去比较\n低分辨率：baseline+CNN 聚合 ｜ baseline+transformer 操作 ｜ baseline + mamba\n高分辨率：baseline +CNN 聚合\n\n\n我发现在这里记录不太好，看不太清楚，我在网页进行进度记录。\n\nMamba 在 HRNet 作为 Backbone 时，效果不好\n在 No-Pretrain 的 HRNet 作为 backbone 时训练有问题的版本是 82 也是比较低\n\n已经取得 SOTA 结果。\n\n看了最新的 VideoMamba论文，不影响我们发挥，他们都是 1/16 的特征分辨率，我们可以实现 1/4，所以优势很大；\n扫描方向不一样\n整体设计贡献不一样\n也就是说不影响现在的工作。最重要的是领域不一样\n\n\nICCV 2025中了"},"B_Experiments_Tracking/TDMI扩展":{"slug":"B_Experiments_Tracking/TDMI扩展","filePath":"B_Experiments_Tracking/TDMI扩展.md","title":"TDMI扩展","links":[],"tags":["Pose-Estimation"],"content":"准备把 CVPR2023 的论文进行扩展，目前思路是在特征提取之后首先加入傅里叶频域分析，增强全局特征；同时融合全局与局部特征得到更好的表示；特征解纠缠改成 pixel level 的\n\n改了很多版，精度最好的也低了0.2，说明没啥用\n又得重新做了，关乎毕业===\n\n整理一下写作思路：\n现有方法的三个问题：\n\n无法进行长依赖性（全局建模）→ 时空频域增强模块\n无法利用差分表示 → 频域调制的差分特征学习\n无法学习有用的运动特征 → 像素级特征解耦合\n\nPipeline: Conv 提取空间特征；用 deformable conv/optical flow 建模全场运动信息\n在这个流程中，存在以上三点问题。对此我们提出上述解决方案\n\n基于互信息的时间差分学习\n探索增强的时间差分建模使用频谱分析分层特征解耦合\n感觉还是两个点比较好 不然频域显得太强行了\nExploring Robust Temporal Difference Modeling Using   for Human Pose\n\n直接用差分捕获点到点的运动信息不够鲁棒,我们从频域分析的角度出发，提出频域增强的时间差分建模，可以学习多到多的映射，建模全局运动信息。\n基于互信息的分层特征解耦合方法，进行多级特征分解，对比-一致性互信息损失函数，更精确地找到有意义的运动特征。\n基于 Heatmap 的表示只用 L2 对于挑战性的关节不够鲁棒，我们提出频域损失，学习更鲁棒的热图。\n\nPoseTrack17-18-21 + HiEve+JHMDB\n\n感觉这个工作反正也只限于写作，不如我先做做手头的，这个只需要后续写就行\n如果说除了全局特征，还要看频域的作用，后面直接做一些可视化把。比如说频域有利于捕捉全局分量，对于遮挡有用等等。\n\n2026-12-17 PAMI投稿"},"B_Experiments_Tracking/Test-time-模型改进":{"slug":"B_Experiments_Tracking/Test-time-模型改进","filePath":"B_Experiments_Tracking/Test-time 模型改进.md","title":"Test-time 模型改进","links":[],"tags":["Pose-Estimation"],"content":"论文 《Test-Time Personalization with a Transformer for Human Pose Estimation》\n使用自监督方法，在测试期间可以train一下以适应测试集。\n启发与改进：\n\n自监督学习与姿态估计结合，本文用的是姿态到图像的重建任务；可以结合 human parsing 任务到图像重建，观察其特征对姿态估计的作用\n使用多个自监督任务与姿态估计结合，看看是否有提升\n自监督任务可以由多个 KeyPoints 获 parsing 的body特征转换为最终关键点\n本篇论文使用Tranformer来进行自监督的结果到最终结果的转换，可以替换成可形变卷积，应该会提点\n"},"B_Experiments_Tracking/index":{"slug":"B_Experiments_Tracking/index","filePath":"B_Experiments_Tracking/index.md","title":"index","links":["B_Experiments_Tracking/傅里叶变换（频域）在视觉中的应用","B_Experiments_Tracking/TDMI扩展","B_Experiments_Tracking/时序不变性+时序变化性","B_Experiments_Tracking/关节结构化约束","B_Experiments_Tracking/基于互信息的时序差分学习","B_Experiments_Tracking/扩散模型应用","B_Experiments_Tracking/Global-思想","B_Experiments_Tracking/Global-Local-Attention","B_Experiments_Tracking/学习鲁棒的时间动态和空间适应用于-VHPE","B_Experiments_Tracking/MambaPose","B_Experiments_Tracking/多假设实验","B_Experiments_Tracking/半监督视频姿态估计","B_Experiments_Tracking/从局部关节出发发散姿态","B_Experiments_Tracking/极度低光条件下的姿态估计","B_Experiments_Tracking/Test-time-模型改进","B_Experiments_Tracking/Implicit-Representation","B_Experiments_Tracking/分层信息姿态估计","B_Experiments_Tracking/姿态模板","B_Experiments_Tracking/特征上采样在姿态估计的应用","B_Experiments_Tracking/3D-信息在姿态估计的应用","B_Experiments_Tracking/基于Optimal-Transport的结构化约束","B_Experiments_Tracking/解纠缠特征对齐实验"],"tags":[],"content":"做过/思考的实验完整记录\n!\nIn Progress\n\n投稿的论文+正在做的实验\n\n\n傅里叶变换（频域）在视觉中的应用\nTDMI扩展\n时序不变性+时序变化性\n关节结构化约束\n\n\nDone\n\n基于互信息的时序差分学习\n扩散模型应用\nGlobal 思想\nGlobal-Local Attention\n\n\n学习鲁棒的时间动态和空间适应用于 VHPE\n\n\n\n\nMambaPose\n\n\n\n\n多假设实验\n\n\n\n\nNot Started\n\n未开始做或者别人做过的 idea\n\n\n半监督视频姿态估计\n从局部关节出发发散姿态\n极度低光条件下的姿态估计\nTest-time 模型改进\nImplicit Representation\n分层信息姿态估计\n姿态模板\n特征上采样在姿态估计的应用\n3D 信息在姿态估计的应用\n\n\nFailure\n\n可能无效的 idea\n\n\n基于Optimal Transport的结构化约束\n解纠缠特征对齐实验\n"},"B_Experiments_Tracking/从局部关节出发发散姿态":{"slug":"B_Experiments_Tracking/从局部关节出发发散姿态","filePath":"B_Experiments_Tracking/从局部关节出发发散姿态.md","title":"从局部关节出发发散姿态","links":[],"tags":["Pose-Estimation"],"content":"在遮挡、模糊等情况下，视觉信息缺失，各种时序方法很难很好的去缓解这个问题。如果能从 一个关节 / 一个关节组 / 半身 等出发，去合理猜测可能存在的各种姿态，就很有可能能解决这个问题\n\n可以先通过大量的 heatmap 预训练，比如扣掉大多数的关节，让模型去恢复｜但是这个做法有个弊端就是，人体的自由度过高｜但是自然图像扣掉 90% 也很难，所以值得一试\n\n\nheatmap 预训练\n加到现有 backbone 末端\n想象的多 pose 融合，可以从耽搁关节开始生成；也可以从关节组开始；以及半身；全身\n\n\n感觉不是很靠谱 以后再看\n或者详细看了 egoego 之后再说"},"B_Experiments_Tracking/傅里叶变换（频域）在视觉中的应用":{"slug":"B_Experiments_Tracking/傅里叶变换（频域）在视觉中的应用","filePath":"B_Experiments_Tracking/傅里叶变换（频域）在视觉中的应用.md","title":"傅里叶变换（频域）在视觉中的应用","links":[],"tags":["FFT"],"content":"基本概述\n最近发现傅里叶变换经常会用于视觉领域，3D 姿态估计比较多。以下是相关论文的大致总结：\n《Learning in the Frequency Domain》\n!\n\n\n做输入数据维度压缩。先有方法一般是先下采样图片，会导致信息丢失。所以 follow JPEG 编码格式，先把图转到频域，然后挑选重要的通道（类似 SENet）。\n《Global Filter Networks for Image Classiﬁcation》\n\n\n\n现有的 Transformer/MLP 风格的模型都是依靠计算图像 token 之间的交互。本文将 token 转到频域，然后用一个可学习的滤波器调整频域特征，然后转回空域。\n\n\n更加计算高效\n\n\n这篇论文很多的理论信息可以学习一下\n\n\n《Exploring Temporal Frequency Spectrum in Deep Video Deblurring》\n一般来说 video 需要时-空建模，这篇论文引入时-频建模。\n3个基本论点：\n\nThe blurred degradation can be better modeled in the frequency domain. 频域可以更好的建模运动模糊\nThe frequency spectrum can enlarge the unpredictable change of temporal motion blur. 空间域模糊的图形变化不大，但是他们的频域时间变化是更大的\nThe blur degradation may decay the energy spectrum of the sharp videos. 更清晰的视频/frame 包含的能量更大\n\n整体框架：\n把频域建模整合到 特征提取-对齐-聚合-优化（loss）Pipeline 中\n\n\n融合频域和空间信息提取，频域来建模模糊；\n用频域全局运动信息引导特征对齐 &lt;这里我感觉可以把输入序列换成差分，就可以是显式的全局运动信息了&gt; （输入序列学习到的 offset 用频率信息重新赋权重，改进的话可以用学到的进一步调整 modulated scalar）\n（现有方法特征融合时是直接加法，遮挡/模糊/清晰的特征系数都一样的）本文用频域引导特征融合，直接用 softmax 把多帧频域特征变成权重，清晰的频域特征能量更大，自然而然的就会给更清晰的图像帧特征分配更多的权重 ｜｜ 这个我觉得很重要，相当于是一种新的特征聚合方法。\n\nIDEA：*********************************\n感觉基于这篇论文，可以做一篇期刊：\n从频域中学习增强的时序对齐和聚合\n\n对齐方面，先有方法由于有限的卷积核难以把握全局运动信息，且在图像模糊下会难以察觉到运动变化；我们用傅里叶变换，首先在频域建模能够提取全局的运动信息；其次傅里叶本身能量信号是可以提高对模糊、（遮挡等，这种情况需要验证）运动信息的建模能力的，因此可以提高 offset 以及 mask 的学习；这里可以直接用 HRNet 第四个阶段特征，做multi-scale对齐。在这里用傅里叶的时候可以加一个可学习的 filter 来提高适应性建模\n聚合方面，直接聚合多帧信息不可避免会用到模糊等信息，傅里叶则可以自适应的配分更多权重给清晰的（有用的、准确的）特征。 ｜ 由粗到精的聚合，先用傅里叶做 framelevel 的reweight，然后用 channel attention 做每个帧内部的调整，然后再聚合\n监督方面，可以考虑 heatmap 的傅里叶优化，直接 l2 在遮挡等情况会不规则导致量化误差，傅里叶强迫学习更多的细节 heatmap\n\n\n\n《Multi-Frequency Representation Enhancement with Privilege Information for Video Super-Resolution》\n\n提出使用频域的一个好的理由：CNN 感受也小；VIT 计算量太大；频域则可以填补这个 gap；传统的频域方法用固定的系数直接转换；我们用一个课学习的提高适应性\n\n\n（c）空间频域表示增强 — — — 傅里叶变换具有全局性；大核卷积也具有全局性；两者共同保证全局依赖；可学习的 filter+IN 保证泛化性\n(d) 能量频域表示增强 — — — — 增强通道间的交互\n\n感觉可以学之前论文，做一篇傅里叶的全流程探索论文\n有几个重点：\n\n傅里叶怎么更好地在时序中应用，不能有轻模块嫌疑，也就是说得找一种好的时序建模方法\n解决了第一点就简单了\n\n把频域信息整合到空间特征提取-时序交互-优化流程中。\n探索时空建模在频域中\n探索时空频谱\n\nIdea-1\nMotivation：\n现有方法采用双流网络，一个分支使用 CNN融合多帧信息来建模时空特征，另一个分支用光流/时间梯度来建模运动。这样的流程有两个缺点：\n\n时空和运动建模均在像素空间，受CNN 很有限的感受野的限制。本文提了一个频域增强的时空模块，从频域角度探索增强的时空表征；以及一个频域运动模块，计算频域的时间梯度来提取全局运动信息。\n双分支网络在推理期间比较 cost，例如光流很慢。为了解决这个问题，本文提出一个新的基于胡信息的蒸馏策略，显式蒸馏细粒度的运动知识并且施加不同模态的一致性，从而在推理期间无需运动建模分支。\n\n相比 SOTA，本文获取了多少 mAP vs 多少性能。\n感觉可以学习一下特征蒸馏的思想\n\nIdea-2\n傅里叶+四维运动矩阵 做姿态估计"},"B_Experiments_Tracking/关节结构化约束":{"slug":"B_Experiments_Tracking/关节结构化约束","filePath":"B_Experiments_Tracking/关节结构化约束.md","title":"关节结构化约束","links":[],"tags":[],"content":"姿态估计希望模型具有结构化推理能力，能根据人体结构进行推理，目前看到的一些做法：\n\n用GAN来做，判别器判别当前姿态是否合理，隐式学习人体关节之间的（生理）结构信息 《Adversarial PoseNet: A Structure-aware Convolutional Network for Human Pose Estimation 》\nTokenPose：用符号表示关节，Transformer学习，《TokenPose: Learning Keypoint Tokens for Human Pose Estimation》直接用Self-Attention建模所有关节之间的关系\n对比学习 ： 两张图中同一个关节表征应该是相似的；成对的特征也该是相似的\n\n粗略分析：\n\n遮挡情况下，CNN依然会去考虑局部依赖性，根据视觉信息来推断当前的点，所以有时候推测出来的pose不是一个合理的pose，即看上去不像个正常的人。\n反观我们人类，则在任何情况下都可以推测出那些看似合理的姿态\n主要原因可以归结为模型无法学习到结构化的（生理）几何约束\n2D图像的 几何约束 == 3D信息？\n\n为了能让模型捕获到结构化信息，可以让网络从部分点向后推测：\n\n每一层的特征都是\n\n还没想好，一些关键思路：\n\n\nKey Idea：\n\n\n整体流程：首先从姿态估计结果中找出一些置信度高的；然后根据其去推测置信度低的关节\n\n\n为了实现结构化预测，可以直接挖去一些关节，让模型从已有的关节中直接推测这些被挖掉的关节，以此来训练模型的结构化（参考MAE，《Masked Autoencoders Are Scalable Vision Learners》），根据部分关节预测其他关节可以采用迭代式架构，逐渐计算出其姿态；\n\n\n从某些高置信度的关节去推理其他关节，直接的idea就是训练一个判别器去识别那些是高置信度的，但是这个难以与视频结合；为了与video结合，可以计算多帧之间关节的相似度（遮挡的关节一般heatmap接近于0，与非遮挡的关节不相似），例如5帧之间计算每个关节的相似度，并根据相似度计算得分，若一个关节在几帧都很相似，则说明其置信度比较高；反之则比较低；\n\n此处计算相似度包含两个方面，即 位置敏感 与 特征（heatmap亮暗）敏感；\n同时包括层次化的相似度，即单个关节joint-level与pair-/group-level的相似度\n几帧中若有一帧遮挡，则期望其会降低整体的可信度\n\n\n\n上述方案创新就在于一个新的思路，一个模型架构；有个问题在于 只是利用了邻近帧来计算关节置信度，缺乏其他信息的利用；如果结果比较好则可以额外与DCPose、FAMI-Pose等对比模型参数量、FLOPs等。\n\n\n\n\n显式学习结构化特征，即人体的三维几何特征（难实现 《Unsupervised Geometry-Aware Representation for 3D Human Pose Estimation》）\n\n\n\n经过讨论感觉2d估计下不只是结构化问题，所以打算先利用补全的思想\n\n基于聚类 + 属性自适应调整\n基于解耦合特征学习 + 补全\n24.8.17— 重新整理该思路，思考这个问题。\n\n以前的想法比较幼稚\n\n整理现有的做关节结构化约束的论文，大致分三种方法：\n\n用 GAN 去判断生成的 Pose 是否合理 (这个我是不打算用的)\n手动去划分关节组，用基本关节推理末端关节；或者是引入关节组结构化约束。｜*感觉这类过于手动指定了，扩组关节关系就无法考虑到\n初始化一组关键点 Token，直接用 Token 和图像特征堆在一起，就能学到关节之间的关系了。\n根据这些思路，有以下启发：\n首先是具体的关节关系，也就是两类：临近关节的约束 （组内约束）；或者是对称关节的约束（不同组之间的关系） ---实际上也可能有隐含的关节联系 这两种算是通用的关节约束，这也只是明面上的，实际上人体跨连接的关节之间可能也有联系，比如头和脖子是一条直线—  这一类也可以成为是人体固有的生理约束\n在具体的视频姿态估计中，与单张图片不同。图片姿态估计只能统计出普遍的统计量。而视频涉及各种各样的动作，不同动作下，关节依赖都具有特殊性。比如跑步情况下，胳膊和腿是有一定联系的；而在投篮/棒球等，两条胳膊是有联系的。在不同的运动场景下，关节联系可以归结为运动特定的协作性约束，此时当然也遵守基本的人体结构，只是有额外的联系在。\n基于上述，可以做一个分层关系学习网络，基本的结构学习+运动特定的约束\n*我们首先定义一组超原型，每一个可以视作高层抽象关键点，补获了跨实例以及跨时间的语义相关的关键点统计量。超原型是稳定的-跨任务跨时间稳定的，编码了基础的人体结构依赖（临近关节的约束 （组内约束）；*或者是对称关节的约束）。（2）我们进一步设计了运动特定的原型适应，通过与具体时空特征充分学习交互，把超原型的知识迁移到具体的多样的运动序列中来适应不同运动模式的关节协作。我们的方法获得了 SOTA 的性能。除此之外，我们提出一致性正则化来促进原型有效学习。\n所以基本思路定下来，感觉可以做基本的 token+分组的超级 token，感觉没必要，直接能学到基础的关系；运动特定的原型学习，首先离不开时空建模；其次是时空特征中怎么去抽象关节模式，比如特征差分+Transformer==== 还有一点要思考的是是否要引入显示的关节关联矩阵，回归出这种矩阵。 明天继续思考模型结构。\n\n已投稿 CVPR 2025， 没中\n已投稿 ICCV 2025， 没中\n\n基于审稿人意见，分析这篇论文主要问题还是在于写作方面：\n\n论文的贡献总结不好；得突出与现有方法的区别\n模型过于复杂\n"},"B_Experiments_Tracking/分层信息姿态估计":{"slug":"B_Experiments_Tracking/分层信息姿态估计","filePath":"B_Experiments_Tracking/分层信息姿态估计.md","title":"分层信息姿态估计","links":[],"tags":["Pose-Estimation"],"content":"Top-Down视频姿态估计 拿出当前人的单人图像序列作为输入\n可以试试全局context信息\n全局图像信息\nbox信息\nROI信息\n人体动作先验\n\n感觉 好几篇论文做过了 比如 Contextual 那篇 以及 EDPose\n《Contextual Instance Decoupling for Robust Multi-Person Pose Estimationtmap》\n《EXPLICIT BOX DETECTION UNIFIES END-TO-END MULTI-PERSON POSE ESTIMATION》"},"B_Experiments_Tracking/半监督视频姿态估计":{"slug":"B_Experiments_Tracking/半监督视频姿态估计","filePath":"B_Experiments_Tracking/半监督视频姿态估计.md","title":"半监督视频姿态估计","links":[],"tags":["Pose-Estimation"],"content":"姿态估计领域半监督文章很少，只有2篇，而视频姿态估计半监督则更少，1篇都没有，可以尝试下这个新领域\n记录一些领域经典论文\n姿态估计中的半监督：\nCVPR’23 《ScarceNet: Animal Pose Estimation with Scarce Annotations》\nCVPR’23 《Semi-Supervised 2D Human Pose Estimation Driven by Position Inconsistency Pseudo Label Correction Module》\nICCV’21 《An Empirical Study of the Collapsing Problem in Semi-Supervised 2D Human Pose Estimation》\n\n半监督领域经典论文：\nFixMatch: Simplifying Semi-Supervised Learning with Consistency and Conﬁdence\n\n视频领域半监督论文：&lt;重点&gt;\nLearning from Temporal Gradient for Semi-supervised Action Recognition\n从现有的视频半监督方法中找一些能用的，与pose estimation结合在一起即可，成为第一篇 semi- supervised video pose estimation 方法。\n\nIDEA"},"B_Experiments_Tracking/基于Optimal-Transport的结构化约束":{"slug":"B_Experiments_Tracking/基于Optimal-Transport的结构化约束","filePath":"B_Experiments_Tracking/基于Optimal Transport的结构化约束.md","title":"基于Optimal Transport的结构化约束","links":["B_Experiments_Tracking/关节结构化约束"],"tags":["Pose-Estimation"],"content":"2023-3-17夜与凡哥讨论\n关节结构化约束\n书接上文，尽管现有方法采用各种设计来捕获人体结构化信息，但是他们只有逐关节一一对应的loss约束，无法直接显式对关节之间的空间关系分布进行约束。我们采用OT，显式建模有刚体连接的关节之间的空间关系分布，强制其与GT保持一致性。这样就可以显式学习关节关系。\n\n自然而然，多帧关节相似就可以输出更连续的姿态（这一点感觉不好实现）\n图像-关节双一致性的最优传输用于视频人体姿态估计"},"B_Experiments_Tracking/基于互信息的时序差分学习":{"slug":"B_Experiments_Tracking/基于互信息的时序差分学习","filePath":"B_Experiments_Tracking/基于互信息的时序差分学习.md","title":"基于互信息的时序差分学习","links":[],"tags":["Pose-Estimation"],"content":"目前的差分架构（Baseline）：\n\n用FAMI-Pose中的局部对齐和聚合获得Appearance特征；\n用多阶段特征差分融合作为运动Motion特征；\n两种特征融合，然后用一个Heatmap Head得到最终的热图。\n\n根据最近做的差分实验，有一些初步实验结果：\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMethodAccuracyAlignment &amp; Aggregation，W/O Motion(Base)82.8（83.0）Alignment &amp; Aggregation, Motion83.1Alignment &amp; Aggregation, Motion(Multi-Stage)83.2\n基础的差分特征能起到一定作用，但是作用不大，所以引出本工作的主体：\nTemporalDifference常用于运动建模，且兼具计算量低&amp;性能可观的优点，已经被广泛应用视频超分、动作识别等任务，但是经过分析，Temporal Difference得到的运动特征是比较Noisy的，即这种Noisy Motion Feature包含两部分：\n\nTemporally dynamic features (e.g., motion patterns) that change over time(由相机和人物运动引起的) ，能真正反映人物运动的特征\nTemporally static features (e.g., 光照),例如光照、遮挡等引起的人物appearance变化并不是运动信息，对于运动估计会产生噪声\n\n因此，需要对Temporal Difference得到的Feature进行分解，解纠缠地去学习这两部分特征，然后用useful且鲁棒的Temporally dynamic features进行姿态估计。\n\n基于以上分析，下一步工作首先需要进行特征分解 ，对于这块目前有两种思路：\n\n首先进行特征对齐，然后做减法，按理说得到的就是运动信息之外的特征（但是感觉对齐缺乏硬约束，有点问题）\n用 Attention Mask 的形式，也就是从Noisy的运动特征中进行挖掘，解纠缠学习，得到有效的时间动态特征（实现简单，看起来也合理，相当于做运动蒸馏）\n\n\n特征分解和约束应该要同步进行，所以需要推导一下运动特征的监督\n\n已经推导出三项主要互信息损失函数，同时加入，效果不错\n经过一段时间调参，在做两组实验：\n\n互信息加一项最大化的\n只修改原始的解纠缠学习方式\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMethodAccuracyAlignment &amp; Aggregation，W/O Motion(Base)82.8（83.0）Alignment &amp; Aggregation, Motion， MI（3项）83.7 (85.7)\n\n目前最高精度 83.94 - 85.8，比FAMi-Pose高了1个点\n主打：Temporal Difference Representation Learning：\n\n多阶段渐进式融合，学习出鲁棒的特分特征\n用解纠缠进一步学习出有用的任务相关的运动特征\n在与外观融合，得到最终的\n\n后续 测试\n\n多模态互信息\n正则化互信息\n训练更多轮\n不同辅助帧数\n\n最近需要重构一下model，在抽象出两个函数：\n\n差分特征这里，改成函数\nattention也改成函数式\n\n关于模型的结构调整，暂时还没有什么思路，能看进去的话就再看看论文，尝试一下新的\n然后就准备写论文\n\n关于MI的消融发现\n\n差分互信息项很有效\n多模态融合的只是0.1，影响不大\n正则化项正在消融\n\n\n发现一个问题，就是反转测试的使用\nHRNet： 83.3 → 84.0\nOurs： 85.3 → 85.75\n我看了一下，输出的heatmap太暗了，反转测试提升不明显 应该是这个问题\n测试了一些方案暂时还没解决\n\n差分设计的消融实验证明所提的拆分+互信息方案特别有效\n差分特征和光流特征的对比完成，差分更有效\n\n重新开始CVPR实验，整理一下最近要做的事\n硬性实验：\n\n 把ConvNext在PoseTrack2017测试集，PoseTrack2018 验证/测试集的Box结果跑出来\n 可以思考一下互信息的正负问题，提一个完整的互信息损失函数，类似Pytorch的余弦相似度函数，尽量解决一下实验结果问题\n 多看论文\n 继续看论文，准备写\n\n本文的重心目前在于运动建模，利用时间差分的方式，如何获取任务导向的运动特征\n\n将代码迁移到新服务器后，对于代码的复现出现了一些问题，复盘一下：\n\n 首先是对训练好的模型，在验证集进行验证，在2080TI上训练好的模型在RTX显卡上可以复现，这说明大体上环境是不影响的。\n 之前是V15版本精度最高，我尝试在RTX和2080TI都训练这个模型，在RTX复现时，Batch_Size设置的60， 第 5 个epoch精度崩塌了(80.3→75.9)，最终训练精度在80.1附近，验证精度83.2； 在2080ti的复现之前给中断了，今天（9-11）继续跑一下看，看看能不能复现，从现在的结果来看是暂时正常\n 由于RTX模型崩塌问题，我挑选了类似的模型V16进行训练，两个服务器配置完全相同，2080TI上训练基本精度和实验记录类似（83.89，以前最高83.92，差了0.04%），该问题可以忽略不计，说明确实是复现出v16的结果了，然后在RTX上也复现到了类似的结果（83.82），这说明两个服务器影响确实不大，但是RTX服务器同样配置训练的精度就是会低一些，误差/计算精度\n V16可以认为是复现成功，v16和v15唯一的差别在于互信息项的使用，我改了互信息项之后v16_15,在2080ti训练精度和没改类似；RTX则直接崩了（batch设置的48），精度掉到了HRNet一样，不知道是不是代码问题\n 精度变化有点诡异，只是在16模型文件的基础上改成了15，瞬间训练就崩了？？？？？ 训练会崩溃可能确实是这个模型存在的一些问题 我再次浮现一下v16，以及看看从v15_2 → v15的情况，看看能否复现出来 若可以复现则去直接训练posetrack18 以及 JHMDB\n\nRTX服务器稳定性确实有些许问题\n\n对于后续代码上的改进与思考\n这个实验主要贡献在于差分操作下的运动建模，思考如果给运动特征再添加一个自监督约束是不是会更好，表示学习到的确实是运动特征，\n感觉现在主要矛盾其实是在于写论文，复现之类的 有差不多的一个版本就够了，就可以开始在PoseTrack2018和JHMDB跑了，因为精度已经够了；然后PoseTrack21是个机会 可以说我们超过之前baseline方法十几个点！\n可以准备开始写论文了，今天多看一些相关的论文，感觉写的重点可以不包括特征融合，重点就在于基于差分算子的运动特征的建模上。\n\n提出渐进式差分特征密集融合，选择性的融合不同的阶段（内部融合+阶段间融合）得到基于差分的运动表征 （informative motion）coarse\n解纠缠的运动特征蒸馏，用解纠缠的学习方式，从差分表征中进一步显式分离出useful feature 和 noisy feature （effective motion）fine\n\n直接用运动特征，但是它有很多杂乱无章或者无关信息，提供的性能特别有限\n直接用attention进行蒸馏，但是这种缺乏约束，依然\n我们的解纠缠机制通过显示分离出两类特征，并用互信息约束最小化其信息量可以学到有判别性的且对任务直接相关的有效的运动特征\n\n\n\n\n关于HiEve数据集的实验：\n\n最后一次更新\nCVPR 中了\n\n2023-03-22\n笔记迁移到 Obsidian"},"B_Experiments_Tracking/多假设实验":{"slug":"B_Experiments_Tracking/多假设实验","filePath":"B_Experiments_Tracking/多假设实验.md","title":"多假设实验","links":[],"tags":["Pose-Estimation"],"content":"参考论文《MHFormer: Multi-Hypothesis Transformer for 3D Human Pose Estimation》\nMotivation 可参考上述论文，主要思想为多假设，即遮挡后是存在多个看似合理的pose的\n短期内用Transformer网络不太现实，所以打算从CNN入手，考虑CNN + 对比学习\nMPHN：Multi Pose Hypothesis Network for Video-Based Human Pose Estimation\n概率性多假设表征生成模块\n基于公共空间学习的表征差异化约束\n\n首先，CNN搞一个序列的时空特征\n然后时空特征按通道划分，作为三种假设的表征\n用对比学习，让三种表征变得不一样 / 实在不行再用互信息\n每个表征出一个姿态，然后合并\n\n\n概率性多假设表征生成模块：\nHRNet 特征 → Attention （从原生特征中找出特定分支关注的区域）→ Deformable Convolution（结合原始特征和attention的特征，对该分支进行个性化学习，进一步优化）\n三组平行的attention，以及deformable conv\n\n论文《MPHN: Multi-Pose Hypothesis Network for Video-Based Human Pose Estimation》初讨论存在的一些问题（9-2）：\n方法层面存在的：\n\n\n主打的是多假设，但是多个分支模块的生成结构是一模一样的，只靠一个loss去约束它们学到不一样的东西是不是有点弱？\n我自己感觉这里多假设通过两个方面实现：\n\n一个是独立的多分枝结构，但是多个分支的GT一样，所以可能确实会存在上述问题\n\n改进：改成 spatio attention ，加上embedding\n\n\n这个新的loss是专门约束学习不同特征的，确实可以学到不一样的\n\n\n\n论文层面\n\n文章标记的\n图的问题，可以第一页画一张展示生成不同的pose的图\n\n\n之前被 ICASSP 拒了，我觉得是审稿人的问题\n\n二次修改\n把 MPHN 并入多表征子空间学习以及互信息目标，可以投到 tcsvt\n\n整体思路\n\n首先根据一个原始的特征序列，进行对齐聚合，得到时空特征；\n随后把时空特征进行划分，得到多个表征子空间；\n最后把每个空间都做姿态估计并且聚合他们的结果，得到最终的 pose\n加入互信息约束不同表征子空间，使得他们不相似\n\n\nTCSVT 被拒了。主要就是说：\n\nr1: 特征分解太常见了。 但是在 vhpe 并没有人这么做过，而且我们的特征分解是有理论分析的，我觉得这个点不用管，就是强调一下我们提出的是基于互信息的特征分解。多假设也有人做了，但是在 2d vhpe 也没人做 。所以针对这些问题应该突出一下目前的写法，就是基于互信息的子表征提取；可以说 3d 用多假设来解决歧义性，2d 没有考虑。我们首次考虑多假设问题。\n多假设：按照上面的，说3d歧义性用多假设解决，调研多假设是否能促进 2d 也是会很有效果。实验表明能促进。多假设也可以往关节优化上引。\n也就是说 novelty 我们通过写作解决—\n实验中把中间分支的 loss 去掉会比较合理，让模型端到端训练。这个建议可以\n写作上主要是实验表格加粗的有问题，仔细改改\n\n\n我觉得不要过度慌，投稿嘛，怕啥，冲就完了\n可以花一周时间来修这篇 paper\n虽然把故事编合理了，但是实际上多假设 idea 感觉还是没那么好的。所以 KBS 试试，不行就往下投。\n5 月份之前修完。\n2025.1.13\nPR 投稿，拒搞重投\n之后继续修一下这个论文，然后再投\n中了PR"},"B_Experiments_Tracking/姿态模板":{"slug":"B_Experiments_Tracking/姿态模板","filePath":"B_Experiments_Tracking/姿态模板.md","title":"姿态模板","links":[],"tags":["Pose-Estimation"],"content":"Deep Hierarchical Semantic Segmentation\n通过聚类给出姿态模版作为先验知识；\n在姿态估计的时候加入先验知识进行姿态估计"},"B_Experiments_Tracking/学习鲁棒的时间动态和空间适应用于-VHPE":{"slug":"B_Experiments_Tracking/学习鲁棒的时间动态和空间适应用于-VHPE","filePath":"B_Experiments_Tracking/学习鲁棒的时间动态和空间适应用于 VHPE.md","title":"学习鲁棒的时间动态和空间适应用于 VHPE","links":[],"tags":["Pose-Estimation"],"content":"理解 关键帧（当前帧） 与**辅助帧（临近帧）**的时序关联性对于视频人体姿态估计来说至关重要。\n现有方法通常显式（光流，时间差分）或者隐式（可形变）建模运动信息来捕获关键帧与辅助帧的时间相关性。然而，他们都是基于当前帧的视觉特征进行的。若当前帧发生遮挡等，它们的结果比较差。\n我们提出一个新的方法来显式推理关键帧（当前帧） 与**辅助帧（临近帧）**的时序关系，且独立于当前帧的视觉特征。\nB, L, C. (1,17,C)→Gaussian B,L+17,C\n\n目前做法：\n\n先把辅助帧过Transformer Encoder建模时空信息；\n然后把时空信息token与关键点token堆叠，预测关键点位置（建模关键点与所有帧的相关性）\n得到时空预测后将时空token作为query，关键帧视觉特征作为key和value进行优化\n最后用query产生增强的预测结果\n\n\n可能存在的问题：\n\n直接建模 keypoint token 与每帧的关系难度太大，因为token序列长度较长，可以考虑直接融合多帧特征 （在时间维度上进行 torch.mean），然后建模keypoint token与时空上下文的关系\nkeypoint token的位置编码需要加上，用可学习的token\n整个输入序列的时序编码要加上，这个应该蛮影响性能的\n修改初始化方法，学习MAE代码的初始化，初始化对模型也有影响\n\n\n后续修改版本：\n\n时空特征直接建模融合，建模 keypoint token与其关系，实现无视觉基础的关键点推理；视觉优化先用目前的版本；\n保持多帧token，建模keyopoint token与其关系，不过视觉refine时直接将纯预测与视觉的整合（取平均），来验证是不是现在的refine策略问题；\n多帧token加上下文重建，更换重建策略，即用类似的192个token表示关键帧图像信息，（192，embed_dim) → (192, 16_16_3) → (keyframe resize成 192， 16_16_3 进行监督)\n\n\n目前v3_2版本得到了不错的结果，可以作为一个不错的baseline，模型架构如下：\n\nEncoder部分 联合时空建模 + 分离的时间-空间建模\n时序预测分支初始化了关节token；用每帧的特征与平均的关节token相加预测当前帧heatmap；加入了contextual token来重建当前帧作为监督（系数0.0001）；\n视觉特征分支以当前帧特征为query，融合的辅助帧特征（对时间维度取平均）为Key Value 进行特征融合；\n监督上进行了密集的监督：\n\nkeypoint输出heatmap\n每帧预测的当前帧heatmap进行监督\n最终融合特征进行监督\n\n\n\n后续需要测试的Baseline：\n目前密集预测版本效果较好，试试去掉token直接密集预测\n\n直接用现有的 v3_2 baseline，去掉重建loss看看结果（测试重建部分的效果）\n不用每个辅助帧去预测当前帧的heatmap，直接用Keypoint Token 出预测结果，进行约束；其他部分保持不变 （保持重建loss为0.0001的比例）（测试密集预测的效果 ）\n直接用Keypoint Token 出预测结果进行约束，并把visual_prediction进行约束；最终融合的结果不约束 （测试中间监督与最终监督，目测结果影响不大）\n在视觉特征分支，不融合多帧特征，直接把多帧作为Key和Value,看看是否有影响（基于v3_2）\n其他小的实验，Encoder中测试联合时空建模和分离时空建模有效性（可以试试删掉联合时空建模步骤）\n\n\n与《Global-to-Local Modeling for Video-based 3D Human Pose and Shape Estimation》不同的出发点：\n\n任务不同\n动机完全不同。建模全局-局部运动；无关键帧推理-有关键帧识别\n\n\nContribution\n\n提出一个新颖的DSN框架，并行地学习双重时空表征（关键帧的视觉增强和无关键帧的时序推理），解耦合现有方法（单一视觉增强时空表征分支）对关键帧视觉特征的过度依赖。这允许很早期产生了合理可靠的人体姿态-即使在关键帧视觉退化下。[可以突出现有方法单分支]\n设计两个模块自适应发觉有用信息实现视觉增强，掌握全局时许演化信息实现姿态推理。\nSOTA.\n\n\n投了 CVPR24 没中，但是感觉审稿意见质量挺低的，准备继续投 ECCV\n一些反思即后续修稿方向\n从方法上说我感觉这个方法确实解决了现存的问题，只是包装上还不太够，所以第一眼看上去觉得 contribution 不太大，先入为主就会觉得比较普通，不亮眼\n审稿人不懂不是理由，重点是怎么完美的去突出工作的贡献；要突出贡献就得指出难点在哪里\n之前只是说现有方法依赖于关键帧，所以我们要预测 然后直接就预测了。\n在模型方面，我觉得得去说一下，为什么做预测是难的，与瞎按有方法的区别得点出来。\n\n与之前论文的区别：\n比如 TDMI，\n\n说现有方法没用过时序差分，所以怎么用是比较重要的；\n先有方法不能挖掘有用信息，怎么挖掘也是重要的，然后挖掘的方法也新\n\n进行类比 DiffPose，\n\n怎么在扩散模型中引入时序信息；\n怎么从噪声中恢复热图\n\n这次，\n\n现有方法不能预测，预测很重要；先突显预测的重要性，再说现在方法不行，我们的可以\n但是姿态预测听起来比较简单，相比特征挖掘，扩散恢复等来说，姿态预测确实太简单了～\n所以就不能直接写我们预测了，而是应该写预测的难点在哪里。\n\n第二次投稿更新 ECCV第一轮意见也很差 没必要去 Rebuttal 了，准备重新整一下这个方法，继续去投。这篇我不打算去降低档次。\n投稿反思\n一些关键的意见：\n\n两个模块过于独立，没啥关系（说明双流网络之间做交互啥的挺重要，或者这条不是很重要）\nMotivation 不太合理，就是图 2 但是我自己来看，这个 motivation 是合理的，就是严重依赖关键帧。但是要论证这点是比较难的。而且大家不太 get 到这一点。图 2 确实不太符合实际情况，这个我自己也考虑到了。所以说图 2 不太好（或者 eecv 投稿的时候去掉图 2 可能会好点？）\n\n\n主要问题就是以上两点 也就是需要更好的 motivation 去呈现，至少相比 cvpr 是有进步的这次。\n\n其他：\n\n参数量；离线方式；位置编码；token 方法依然不能理解\n\n综上，问题就是 motivation+模型设计，我觉得需要更加 convince 的动机，以及更抓眼的模型设计。\n\n一些初步的动机想法：\n\n理解运动信息很重要。现有方法通常用光流、差分等，这个范式更加注重像素级别细粒度的像素级视觉内容对应，而忽略了比较 high-level 的帧间的关系，例如整体活肢体的人体运动。因此，这些方法对于遮挡等视觉退化是很敏感的，容易性能下降。我们提出充分去挖掘帧间的时空关系，学习图像级别的运动信息，即使在某些帧退化下依然有鲁邦的时序推理能力。 --- *本质上来说，光流/差分与像素之间相似度是密不可分的，过度依赖像素相似性，忽略全局的语义关系，会造成很容易失败\n模型方面，可以搞一个由动态到静态的更新框架，先是一些 global keypoint token 去学习全局动态；然后进一步用关键帧的空间信息去更新，进一步增强之类的。当然主motivation 就是以上的内容。第二个点可以再想想。\n\n\n投稿之后本来纠结像素相似性，语义关系说法的准确性\n经过纠结以及看光流论文，这个说法完全没问题。光流/差分很靠像素相似性，过于直觉，且容易失败对于退化，而且本来也无法去理解全局的语义关系，比如理解到这个人在跑步，那他应该是怎么样的姿势。他只能本能地去看这个像素像谁，没有语义的概念，所以实际上也容易出错。所以我们要学习 high-level 的高层的语义动态，对于局部的像素退化是不太敏感的，而不是根据相似度学习像素关系。反观GME ，本来也就是去挖掘帧间语义关系的。\n实验需要修改，我换一下 Backbone 网络试试，不用 pretrain 的 ViT，直接用原始的 ViT-L 和 ViT-B 去训练，看看结果，也好对比参数量。\n\n2025.1.25 Accept\nNeurocomputing 中科院二区\n这个工作也算是有个好的归宿了"},"B_Experiments_Tracking/扩散模型应用":{"slug":"B_Experiments_Tracking/扩散模型应用","filePath":"B_Experiments_Tracking/扩散模型应用.md","title":"扩散模型应用","links":["扩散模型"],"tags":["Pose-Estimation"],"content":"扩散模型目前看到的论文里，应用到了 目标检测、语义分割、全景分割里面，看看能不能继承到姿态估计领域，扩散模型也分好几种，根据目前学习的情况，准备应用去噪声的扩散模型。\n\n第一次更新\nDIffusion关键变量、公式、计算\n\nDDPM有两个重要过程，前向和反向扩散；\n应用扩散模型时我感觉有几个要点：\n训练过程的基础与约束：\n\n要么用原始的方法，从GT形式开始拟合噪声\n\n这里GT形式一般是map,比如语义分割的map；也有论文比如DiffusionDet用的GT BBOX的坐标，加上信噪比\n然后直接用最原始的扩散模型，就拟合噪声\n\n\n另一类范式从GT开始groudtruth\n\n比如目标检测、分割 都会用一个head层来解码特征，和gt对比进行训练，我感觉这个比较适合搬过来\n\n\n\n反向推理过程：\n\n要么是用原始的方案，一步一步推理，但是具有随机性，可以多运行几次去个平均得到最终结果，这种方案显然不太好\n另外一种是快速的非马尔可夫迭代过程的逆向过程，可以基于前一步的生成结果来生成下一步而且效果不错\n\n现有感知任务应用算法对比\n\nLABEL-EFFICIENT SEMANTIC SEGMENTATION WITH DIFFUSION MODELS\nTraining:\n按照最原始的扩散模型，用生成图片的方式进行训练，没有改变（我理解是这样的），即生成高斯噪声作为GT然后训练逆向估计这些噪声\nInference：\n\n用扩散模型估计噪声的UNet提取的特征作为学习到的图像表征，即执行前向过程，固定不同step的噪声，然后不同step加噪声会生成相应的x_t,训练一个MLP分类起得到最终的分割结果；\n\n测试了不同 t 以及相应的不同深浅的Unet的block得到的图像特征\n\n\nSegDiff: Image Segmentation with Diffusion Probabilistic Models\n\nTraining:\n以Ground Truth的mask作为生成基础，开始加噪声扩散；逆向的时候修改了噪声预测模型，为图中的G F D E；其他保持不变\nInference：\n初始化一个噪声xt，根据输入图像和噪声开始逆向扩散，一步一步得到最终的结果；相当于就是改了原始扩散模型unet的输入、计算方式；\n\n逆向扩散的时候有个噪声z是随机的，所以多次测试然后平均得到最终结果\n\n\nDiffusionDet: Diffusion Model for Object Detection\n&lt;回头在看看代码&gt;\n\nTraining:\n原始图像一次性用Encoder提取特征；采样Ground Truth的box信息；前向添加噪声进行扩散，然后对扩散结果输入到Deocder里面，Decoder中用扩散的box结果裁剪图像特征，然后一个detection head来输出最终的检测结果（分类和回归），和GT的分类回归做loss\nInference：\n从高斯采样的随机box开始恢复；用DDIM进行简化渐进采样；自己的更新策略\n\nA Generalist Framework for Panoptic Segmentation of Images and Videos\n\n上一篇论文DiffusionDet和这篇类似，应该是继承这篇的。\n训练的时候用Decoder解码真实结果；测试的时候也用了DDIM。\n\nDiffPose: Multi-hypothesis Human Pose Estimation using Diffusion Models\n用扩散模型来建模姿态估计的不确定性\n\n训练是按照正常的训练过程，预测噪声，有几个要点：\n\n从heatmap中进行采样，生成一堆可能的，相当于建模不确定性 然后用transformer block得到特征\n逆向去噪声过程从hrnet估计的2D heatmap开始，而不是从完全随机的噪声开始\n\n\nDiffPose: Toward More Reliable 3D Pose Estimation\n\n在姿态估计的应用\n\n\n\n训练基础，按照检测这种范式，计算出最终的pose然后监督\n\n回归：坐标，17个关键点\n用多通道 Heatmap —— 做baseline来说可以从heatmap入手，基于heatmap进行迭代生成学习 （可以测试）\n\n\n\n时空扩散模型\n\n\n之前都是输入一帧，在时空姿态估计一般是输入多帧输出一帧的姿态估计结果，有个想法是维护一个序列相关性矩阵，即关键帧与每个辅助帧都进行特征相关性计算，可以作为一个丰富的时空特征来使用；（替换原来的可形变卷积融合之类的）\n\n\n每次的17个通道的heatmap去时空特征进行索引；然后通过原始图像的Context、索引的相关性局部特征，来计算出heatmap\n\n\n然后前向就按照上述过程训练\n\n\n逆向的时候：从随机高斯开始，加上每帧图像的context；索引的相关性特征；迭代的一步一步输出heatmap\n\n\n\n第二次更新\n代码基于DiffusionDet实现；\n\n先把扩散模型迁移过来跑通\n扩散模型跑通了 heatmap生成这部分死活懒得写\n\n第三次更新\n\n写完是写完了 但是训练为0\n\n\n\n\n解决训练精度为0的bug修复：\n\n原始代码中对ground truth加了信噪比，以及对采样的数据做了截断，我试试删掉这两个部分（测试出不是这部分问题）\n检查梯度似乎没有异常值\n在forward代码中有一句赋值，这句话影响了模型学习\n\nBug修复，\n下一步看看训练最高能到多少 [68+ Ac]\n\n分析：扩散模型因为要从ground-truth的heatmap开始扩散，然后通过一系列操作算一个预测值再和从ground-truth的heatmap计算损失函数，这种训练似乎不是很科学，现在68左右的Acc，距离普通的姿态估计结果还差挺远 —- 要么是因为heatmap本身不宜扩散？直接从坐标开始？\n基于扩散模型的后续改进需要再看看，扩散模型不一定是要做的，现在还在探索尝试阶段\n\n12-8 新的思路：\n鉴于当前基于Heatmap扩散的模型精度不高（68），准备进行以下尝试：\n\n基于坐标直接进行扩散，作为一个baseline模型（这里可以考虑用Transformer架构），看看效果\n结合坐标和Heatmap：在坐标扩散的过程中，对于每个Step，加入Heatmap进行引导修正\n结合坐标和Heatmap：双流扩散模型，扩散过程中互相交互，融合坐标信息以及heatmap信息\n\n2022-12-8，杨总贡献！\n\n12-13 更新思考：\n论文《DiffPose: Toward More Reliable 3D Pose Estimation》启发，目前扩散模型可能可以改进的点：\n\n时间编码那块，变成一个attention 估计有问题，可以试试把时间编码的tensor 和 image feature cat在一起，然后再学习\n扩散基础，基于backbone的结果进行扩散，而不是从0开始扩散，给一个好的初始化作为基础，重点放在怎么结合temporal和diffusion (cycle)\n扩散模型中生成噪声的，随机采样可以换成 高斯混合分布采样\n扩散模型的训练部分可操作空间较大\n\n\n12-15 继续整理思路\n\n采样姿态是必须要做的，用多个姿态进行扩散，建模不确定性，所以这块要解决一下姿态采样问题\n扩散基础也是必须要改，基于Backbone结果进行采样多个，再进行逆向会比较好\n把时间编码的tensor直接cat起来会好点**（已实现）待会儿再试试放开HRNet一起train的一个版本 [精度低，先排除一下实现方式问题 我试试把原始精度高的版本的推理也抽象成方法]**\nTemporal + Diffusion，为了学习时间一致性，进一步进行约束：\n\n进行cross frame的扩散过程：current frame 的pose + 邻近frame的feature来扩散生成临近frame的pose；同理从临近的frame扩散到current frame，类似一个cycle的过程\n\n\n训练过程我觉得可以先不用改 就用目前的，先不需要改成学习噪声的\nBackbone替换以及冻结问题，有待考虑，先用冻结的版本看看\n\n\n还有一个猜想是 直接回归heatmap 因为这个没有什么明确的语义，所以扩散模型学习起来难度比较大，借鉴binary mask的思路学习一个类似的mask能否帮助最终的关键点定位 也是有待考虑的一件事\n\n12-22梳理和计划\n做实验发现通过多次随机初始化进行扩散，可以提高精度（80），已经到了一个可接受level的baseline，接下来就是进行改进，因为还是有很大的可操作空间，梳理一下改进思路\n\n先复现之前最好的68精度，这样应该就可以达到82mAP了 （应该是误差，这条可以忽略）\n调整信噪比scale和heatmap约束的scale，看看对精度的影响\n在模型设计中引入可形变卷积（CNN体系）\n用HRNet提取特征，设计transformer结构来处理后续扩散过程\n放开HRNet一起训练\n修改backbone，用vitpose作为backbone提取特征，用transformer结构进行学习处理\n引入高阶的时序设计\n\n\n就是比如处理多帧的时候，会把多帧的patch叠在一起，然后比如用一个block算完attention特征维度是很大的，做下游任务是直接把这么大维度的resize成二维的feature然后用Head么还是其他的\n[10, 192, 1280] → [10,1280,16,12] → [10,1280,64,48] → [10,17,64,48]\n[10, 192*3, 1280] → [10,3840,16,12] → [10,3840,64,48] → [10,17,64,48]\n\nTransposeconv()\n3840→1280→vithead\n[B, patch, Channel] | 192_3 → 192_3 |1280\n[10, 192*3, 1280] → [10,17,192]\n\ncat [patch_embedding,pose_embedding]\nInput: patch_embedding + pose_embedding\nourput: patch_embedding + pose_embedding\nMLP(pose_embedding)\n\n1-7最新进展\n\n用ViT作为Backbone提取特征，transformer结构进行多帧特征融合\n扩散从heatmap和时空特征进行学习\n精度83.8 w/o tricks\n\n\n2-2对最新模型进行总结：\n\n用Transformer聚合多帧特征的时候，直接交互数据维度，把帧数（或是对应的patch embedding数）放到最后一维，用Linear层进行聚合即可\n多尺度**&lt;heatmap_diffusion, Transformer&gt;**特征交互,Transformer分辨率太低，所以尝试上采样两次得到一共三组特征分别进行交互\n特征交互的时候直接cat 扩散的heatmap与Transformer特征会出现性能严重下降，需要以heatmap作为mask进行特征索引，然后进行交互，相当于是迭代搜索优化逐关节的特征\n\n\n但是需回答一个问题，为什么是扩散模型？\nNovelty：\n\n现有扩散模型都是基于静止图像的，我们首次把扩散模型用到了视频姿态估计\n提出一个基于时空特征的多尺度特征交互模块，以噪声为引导，建模多分辨率下噪声和时空特征的交互。\n我们demonstrate基于所提的多尺度特征交互模块，扩散模型可以迭代地搜索并优化关键点区域表征，有利于屏蔽干扰，对于姿态估计任务很有利 -已经得到验证\n在多个数据集SOTA，并且已开源\n\n—\n如何验证扩散模型可以迭代学习关键点区域表征？\n\n跑验证，把步长设置大， 看看不同步长下模型学习的表征变化 若可以迭代变化就是一个好的兆头\n不同初始化的互补性姿态验证\n\n\n最后一次更新\nICCV Paper 中了 Idea结束"},"B_Experiments_Tracking/时序不变性+时序变化性":{"slug":"B_Experiments_Tracking/时序不变性+时序变化性","filePath":"B_Experiments_Tracking/时序不变性+时序变化性.md","title":"时序不变性+时序变化性","links":[],"tags":["Pose-Estimation"],"content":"Main Idea\n\n可以吧多帧图像看成多视角的，去保持他们的时序不变特征，即 object 本身的几何信息之类的\n另一个分支是时序变化即运动信息\n\n几何+运动感觉可以\n\n《CDDFuse: Correlation-Driven Dual-Branch Feature Decomposition for Multi-Modality Image Fusion》\n这篇论文提到将特征解耦为 base+detail\n所以一个自然的思路：\n\n关键帧和辅助帧都过一个 share 的 Backbone，去提取图像的空间特征\n把图像特征解耦，分为结构化特征 和 细节特征\nHigh-Level 上，结构化特征可以理解为时序不变性，而细节特征则可以认为是时序变化的，因为每帧有自己的外观信息\n把多帧的结构化特征保持一致性，学习时序不变的深层次人体结构特征 可以考虑 互信息 / 最优传输  / 对比学习 /etc. → 这里也可以讲故事说 图像退化下的共识约束可以提高模型鲁棒性\n同时可以提出 loss让结构化特征和细节特征变得不一样\n最后就是搞一个模块，同时利用这两种信息\n\n\n今天想到一个东西，就是在约束学习结构不变的人体特征时，可以用 clip 内+clip 外的共同约束；\nClip 内-同一个 person 结构强一致性，比如关节位置、人体动作比较相似等等，这可能会更偏向于这种细节，忽略本质结构；且可能会引入背景；\nClip 外-不同 person 本质结构相似性，这样可以避免背景信息干扰，学习到 person 本身不变特征\n相当于重点就是做时序一致性特征提取\n\n新的这个方向 idea 得用vit 作为 backbone 才行 — 感觉不太好说\n\n可以先以 HRNet 为基础做个看看\n在扩展成 VIT 版本\n或者直接用VIT 作为 Backbone 试试\n\n\nHRNet版本效果一般。83.0，和对齐聚合效果差不多，这个实验之后再做吧，先针对 mamba 来做一篇"},"B_Experiments_Tracking/极度低光条件下的姿态估计":{"slug":"B_Experiments_Tracking/极度低光条件下的姿态估计","filePath":"B_Experiments_Tracking/极度低光条件下的姿态估计.md","title":"极度低光条件下的姿态估计","links":[],"tags":["Pose-Estimation"],"content":"暗光环境下的人体姿态估计\n这个方向目前检索到的只有一篇论文\nCVPR 2023 《Human Pose Estimation in Extremely Low-Light Conditions》\n感觉这个方向可以做\n\n\n大致 IDEA\n\n暗光图像和原图享有相同的物体几何结构，例如人体的位置、形状等；\n但是两者的光照条件不同\n所以可以训练一个共享的 backbone，学习从 low-light 和 well-lit 对应的图像上提取几何信息；\n另一个分支，用轻量级的 CNN 来提取光照特征\n整体网络就是一个解耦合的结构\n最后把光照特征与几何特征融合"},"B_Experiments_Tracking/特征上采样在姿态估计的应用":{"slug":"B_Experiments_Tracking/特征上采样在姿态估计的应用","filePath":"B_Experiments_Tracking/特征上采样在姿态估计的应用.md","title":"特征上采样在姿态估计的应用","links":[],"tags":["Pose-Estimation"],"content":"\n看到一篇相关论文《LoftUp: Learning a Coordinate-Based Feature Upsampler for Vision Foundation Models》，可实现更好的特征上采样质量，因此可应用于姿态估计网络生成质量更高的热图。\n\n\n主要 idea 是用了坐标表示，即高分辨率下坐标和图像一起输入提取特征；然后作为 query，与低分辨率特征做 cross-attention。\n\n如果把上采样作为一个创新点，那么就得想一下其他的创新点：\n\n精心设的 heatmap 上采样，提高姿态估计精度 （比如 ViT 低分辨率，固定 ViT Backbone， 然后后面拼接上高级的上采样，看看效果）\n\n"},"B_Experiments_Tracking/解纠缠特征对齐实验":{"slug":"B_Experiments_Tracking/解纠缠特征对齐实验","filePath":"B_Experiments_Tracking/解纠缠特征对齐实验.md","title":"解纠缠特征对齐实验","links":[],"tags":["Pose-Estimation"],"content":"Hierarchical Motion-Aware Temporal Alignment for Video-Based Human Pose Estimation\nMotivation:\n\n要证明一下现有的方法直接进行整体图像对齐，可能会趋近于拟合全局图像变化平均值，导致对齐整体力度不够 （效果不明显）\n而对于姿态估计来说，邻近帧通常不会整体变化很大(人体躯干)，但是局部关节变化通常很显著，能捕获到局部变化至关重要\n我们提出可学习的运动感知的特征对齐，分别处理不同的区域，\n\n\n分离出motion比较小和比较大的区域，进行分别对齐\n\n传统数字图像处理方法需要手动设置阈值，我们提出一套可学习的方案自动识别和局部显著的运动特征区域\n两帧做差\nmaxpool→Avg &gt; 阈值 &gt;过滤 分别生成不同程度的pixel-wise的Motion map （可以用Convolution平滑）\n分别对齐\n这个点自监督可选 （Optional）\n\n\n对齐之后用Optimal Transport Loss，因为可以维持原始的特征结构，又能让两帧更相似 ，优于L1 L2 等等loss\nSOTA performance\n\n写作创新点：\n\n分层对齐框架\n针对该对齐范式，提出对应的直接约束\n\n图像分为 内容 + 风格（位置） 对齐就是提取位置属性\n所以在图像特征对齐中，semantic level已经是严格对齐的，主要问题就是spatial level的对齐\n问题依然存在：\n\n怎么去做到spatial change aware？这里的aware是按pixel-level还是channel-level？\n\n\n进一步改进idea：\n\n\n现有方法根据视觉特征直接对整体的图像进行对齐\n\n整体对齐：同时对齐变化平滑和变化剧烈的区域可能是困难的，例如对于仿射变换会拟合平均值；对于Deformable convolution来说，由于外观特征都比较像，受限于卷机核尺寸，难以把握大的空间变化\n直接对齐：直接聚合两帧的视觉特征来学习对齐参数offsets，而相邻帧特征通常很相似，这种做法对于高度相似的两帧是不利的 —— 我们提出渐进式对齐框架，1⃣️显式建模两帧之间的动态上下文，基于运动信息进行调制，显式学习细粒度的offsets，2⃣️随后为了兼顾整体语义信息，基于整体视觉特征学习semantic-level的offsets\n\n现有方法是motion-unaware的\n运动导向的分层对齐框架\n\n根据变化大小激活相应的区域，这一步应该同时激活source和tagret的区域，用share的参数来提取 （目前方法只激活的target区域 和source全图进行对齐 不太好）\n提取好之后对齐的时候，用两步对齐法motion-appearance对齐法，根绝差分map计算运动特征，首先根据运动特征（和辅助帧的特征）生成offset进行运动对齐；随后用辅助帧+关键帧特征来生成offset（语义优化）\n\n\n\n对于对齐的监督现有方法一般无监督 或者通过图像重建来实现，并非直接的对齐目标\n\n\n\n\n\n目前模型效果还没有backbone好，我感觉可以简化一下模型\n\n先测试一下特征融合操作\n然后试试baseline分区域对齐模型\n\n不一样的点：\n\n分层对齐框架，自适应对不同的region进行对齐\n对于不同区域的特征，我们提出motion-guided的对齐流程\n对齐之后的监督\n\n\n两个方案：\n\n用transformer直接聚合\n全部CNN：SENet + CNN聚合\n\n\n测试epoch-8 是85.2，虽然说可能会进一步提升，但是这个idea本身总感觉似乎有点奇怪，不是很创新，分层对齐\n\n目前对齐本身的效果依然存疑，不一定有效果\n变化显著和平缓区域的识别\n对齐的手段太过于平淡 [ 我觉得块是目前主要诟病的地方，即网络结构问题～如果能换成基于Transformer的会好些，然后开发打磨一下对齐的结构 ]\n目前的效果总是低于backbone，我自己分析，是第一步，通道数1280直接降到512，这步信息损失很大，明天可视化特征图看看，分析一下问题出在哪里\n\n\n最近计划\n\n最近继续看一些特征对齐论文，定向读一读，看看不同领域的对齐，然后做一个新模型；\n分析一下在大通道数下如何让特征的损失最小 [类似蒸馏的思想]\n\n\n\n4-19夜反思\n\n时序处理可以划分多个level的clip分别提取时序特征，再融合；\n关键帧作为Q，KV为辅助帧进行融合\n\n\n通过可视化特征发现，加了se-attention之后，特征变成马赛克了；\n解决一下“曝光过度”特征的问题；\n多帧特征聚合导致特征会曝光过度；说明聚合可以根据相似度来\n\n\n明天试试把VIT提取的特征写到本地，当成数据读进来\n-行不通 训练有数据增强 所以不能直接把特征写到本地\n\n重新整理创新点\n感觉从运动大小的角度直接说有点俗，应该是人体适应的特征对齐 Human-Adaptive Temporal Alignment\n人体图像的特点：\n\n背景不重要\n不会发生剧烈的运动变化\nbody区域变化不大\nend-point joint区域变化大\n\nDCN可以学习channel-level的offset,但是channel通常也是global-context的信息，所以依然是学习耦合的offset，不能很好的关注到局部的关节特征的对齐\n\n人体body主体区域通常变化较小，而末端关节自由度高导致运动变化通常较大，现有方法用单个DCN进行对齐需要泛化很好地同时适应不同幅度的运动，难度很大，产生了不好的姿态估计；\n\n\n2023-5-1\n\nMotion-Aware 的分层对齐机制，自适应地识别不同幅度的运动区域分别进行对齐，可以更好的关注末端关节的特征恢复；\nInstead 隐式根据特征相似性学习对齐， 显式建模运动信息引导offset采样，使优化更容易\n\n《FDAN: Flow-guided Deformable Alignment Network for Video Super-Resolution》指出可形变卷积对于大的运动依然无法很好的处理，而这对于姿态估计来说是很致命的，因为局部端点的关节通常运动幅度大易被遮挡，只对齐变化小的body-region势必会影响性能。分析原因：\n\n人体运动通常不会发生突变，目标运动具有均匀性，在短期内人的主体区域通常变化较小；而end-point的关节区域运动自由度高导致变化较大。\n对齐均在feature-level进行，对应的每个pixel都要对齐；feature通常包含的是global-context，整体变化较小，直接使用可形变卷积进行耦合地对齐global-context，模型会偏向更整体的变化小的视觉内容；而局部关节则容易被忽略，产生较差的结果\n因此我们提出在全局特征语义对齐的基础上，引入局部分解增强的细节内容对齐\n具体地，我们将不同幅度运动的视觉内容进行分解，分层进行salient-plain的对齐，以更好地学习局部关节的对齐\n考虑到局部关节较大的运动，我们显式地用temporal-difference建模运动信息来引导kernel offset的学习\n所以以上就是第一个创新点，解纠缠的特征对齐框架\n第二个创新点：\n感知任务中的特征对齐（语义分割，姿态估计）通常只有label相关信息约束，这难以约束特征对齐的学习；直觉的图像重建约束又增加了学习难度，因为我们只需要学习关节位置，无需关注其他视觉信息例如图像纹理等，因此我们提出一个基于互信息的特征对齐约束，通过最小化局部对齐前后特征的\n\n\n测量不同部位关节的运动量\n\n\n测量一下具体的变化，给出量化值 （等效果好了）\n\n\n一直超不过Backbone，做几个baseline测试下\n\n融合backbone的多帧特征\n直接融合最后抽象的特征\n多尺度思想\n\n\n假如运动引导的transformer可以解决大幅变换问题，那么为什么不直接对全局采用运动引导呢\n得分析出区域进行运动引导的好处\n晚上用HRNet作为backbone\n\n宣告失败\n6.4"},"C_Research_Knowledge/Attention方法":{"slug":"C_Research_Knowledge/Attention方法","filePath":"C_Research_Knowledge/Attention方法.md","title":"Attention方法","links":[],"tags":["Attention","Transformer"],"content":"Attention背景\nRNN循环神经网络，具有一定的记忆性，但是有一个固有的缺点就是不能并行运算，要算后一个必须就要先算出前一个。显示提出了CNN假设，用CNN替代RNN，此时已经可以并行计算，但是CNN必要有很多层才可以看更长时间的东西。\n因此出现了Attention这种方法，成为RNN的代替方案。\n\n计算流程\n计算出 Q,K,V\n首先输入x计算出a，a分别乘上三个矩阵得到q，k，v。\n\nq_i = W_q *a_i,\\space \\space k_i = w_k*a_i,\\space \\space v_i = w_v*a_i\n\n其中q表示query，查询，也就是用来做匹配；\nk表示key，是被匹配的对象；\nv表示value，是要被抽取出来的信息\n\n用每个q对k做Attention\n\n$$\n\\alpha_{1,i} = q_1 \\cdot k_i / \\sqrt d\n$$\n其中，q和k做内积除以它们的维度。d是q和k的维度。这个注意力计算公式也可以是其他的。\n\n其中α经过softmax层得到α帽\n\nα帽和相应的v做一个加权和得到b，即：\nb_1 =\\sum_i \\hat a_{1,i} * v_i\n此时，b1就已经考虑了整个的次序。\nb2,b3,b4流程和b1一模一样，即：\nb_j =\\sum_i \\hat a_{j,i} * v_i\n并且是可以并行计算的。\n并行运算细节\n\n首先是Q，K，V三个矩阵，把a堆起来乘以w矩阵得到Q矩阵，K,V类似，这就可以同时计算。\n\n把k堆起来乘以q可以得到α，经过softmax得到α帽。\n\n此时把v堆起来，就可以计算得出b1，b2，b3，b4.堆起来就是最后的输出。\n位置信息\n此时Attention其实是没有考虑时间次序的，因为q对每一个k都会做Attention，不管远近都会做，所以时序信息是没有考虑进去的。\n\n此时只要给ai加上一个ei矩阵即可。\n\n此时可以解释为给输入xi拼上了一个onehot向量，用来代表是哪个输入，然后得到上面的式子。\n这只是大概讲了一下 Attention怎么计算，具体的NLP中怎么应用可以深究一下，但我不是做NLP的，所以大概了解就可以。\n不求甚解如我。关于封面，Attention顾名思义，注意力，所以封面更能体现Attention。"},"C_Research_Knowledge/Auto-Encoder-in-DeepLearning":{"slug":"C_Research_Knowledge/Auto-Encoder-in-DeepLearning","filePath":"C_Research_Knowledge/Auto-Encoder-in-DeepLearning.md","title":"Auto Encoder in DeepLearning","links":[],"tags":["VAE"],"content":"VAE部分公式推导省略的可以观看李宏毅老师的课：www.bilibili.com/video/av9770190\nBack Ground\n首先自编码器的意义是什么呢？\n以CV举例，在影像处理中，人脸识别一张普通的200*200像素的图，就有40000维向量要处理，显然不实际，因此如果可以有一个编码器可以输入一张图，输出一个30维的向量；再将这个30维向量输出成200*200的图，尽量与原图接近。也就是说30维的向量代表了40000维的图，并且尽量保持图片特征、不失真，这就是自编码器的应用。\nAuto Encoder\n最初的 Auto Encoder设计结构如下图：\n\n只要让输出尽可能的接近输入即可，然后改造成深度自编码器，如下：\n\nloss依然是最初的。这便是Auto Encoder\nProblem in Auto Encoder\n首先，Auto Encoder存在一个问题，它把所有训练的图片都是对应到了一个高维空间中的一个点，可以这么理解，本来图像是40000维空间的一个点，经过编码变成30维空间的一个点，如果采样正好采了这个点，则可以比较精确的用解码器还原图像；但是如果在30维空间中，采样到了一个从未训练过的点，那么解码器就大概率只会解码出一堆噪音，如下图：\n\n如果采样到 code中间未训练过的点，解码出来的图像就不像是一张真实图像，会是一堆乱码，而VAE做的事情就是每张图片不在是对应一个点，而是一个区间，在这个区间内都可以解码出这张图片，如下图：\n\n这就引入了VAE.\nVariance Auto Encoder(VAE)\n先看一下VAE的整体架构：\n\n首先是输入经过一个编码器，产生一组m，一组σ，然后一组e是从高斯分布采样的，即：\nm_i+\\exp(\\sigma_i)+e_i = c_i\n也就是原始编码加上噪音，而σ则是控制噪音的方差，即\\exp(\\sigma_i)就是噪音的方差，是自动学习的；同时损失函数在原来的基础上，加上了上图右下角一项。\n直观上，如果不加限制的让机器自己去学习，那么机器肯定会认为噪音对原图像干扰越小越好，于是会给exp(σ)赋值为0或者很接近0的数，但是这也就失去了意义。因此加上这一项，\\exp(\\sigma_i)-(1+\\sigma_i)的最小值在σ=0时得到最小值，也就是说σ=0loss最小，此时方差=1,所以机器自己学习就不会让Variance太小；m_i^2可以认为是L2正则化。这就是直观上VAE这样设计的原理，下面从数学上理解VAE的原理。\nVAE的原理\n首先，我们的工作任务是可以采样到需要的图像，也就是要估计原始图像的概率分布P(X),如果知道了原始图像的分布，那么我们只要让我们的生成的图像分布尽量接近原始图像也就是计算它们的KL散度即可。所以为题转化为求P(X).\n高斯混合模型认为，任何一个分布都可以由多种高斯分布混合(加权和)而成。则此时；\nP(X) =\\sum_m P(m)*P(x|m)\\\\\nx|m\\approx N(\\mu_m,\\sigma_m)\n这件事更像是对图像做了一个分类，我们所看到的x都来自于某一类，这是不好的，更好的方法应该是用一个向量来表示图像。\n则此时引出了z，z是一个隐向量，是服从高斯分布的；向量的每一个维度对应图像的某一些特征。注意，这里之所以z取高斯分布，是因为逻辑上说，没有特色的东西占多数，图像每种属性的分布其实大概率是服从高斯分布的，因此z取高斯分布也是比较合理的，但是z可以是任何分布。\n同时，z有无穷多个，是连续的，不再是高斯混合模型那样有固定个z；每一个z对应的均值μ、方差σ都是由神经网络学来的。\n所以此时，\nP(X) =\\int_z P(z)*P(X|z)dz\\\\\nP(z)\\approx N(0,1)\\\\\nX|z\\approx N(\\mu(z),\\sigma(z))\n真正要求的就是μ(z)、σ(z)，最大化P(X):\nL = \\sum_X\\log P(X)\n\n所以，此时z经过一个伸进和网络输出均值和方差，目的是最大化L；这时候需要引入另外一个分布q(z|x),也就是输入图像，提取它的高斯分布；所以，上图中蓝色的就是Decoder，绿色的就是Encoder。然后继续用数学推导：\n注意以下公式推导有跳步，具体可以看一下李宏毅老师的视频讲解。\n\\log P(x) = \\int_z q(z|x)\\log P(x)dz = \\int_zq(z|x)\\log\\frac{P(z,x)}{q(z|x)}dz+KL(q(z|x)||p(z|x))\n其中，KL散度一项大于等于0，故前面的一项就是\\log P(x)的下限(lower bound).这个下限记为Lb:\nL_b=\\int_zq(z|x)\\log\\frac{P(z,x)}{q(z|x)}dz=\\int_zq(z|x)\\log\\frac{P(z)*P(x|z)}{q(z|x)}dz\n则原式化作:\n\\log P(x) = L_b + KL(q(z|x)||p(z|x))\n这里就真正的体现了VAE的精妙之处：\n本来是要寻找P(x|z)来最大化L，但是现在需要同时寻找P(x|z)、q(z|x)两项，来最大化Lb，从而最大化L。\nq分布实际上与 log P(x) 是无关的，log P(x)至于P分布有关。所以q无论取什么值，log P(x) 都不变，如下图：\n\n所以当P固定，q最大化Lb时，KL会越来越小，最后消失不见，也就是q(z|x)和p(z|x)的分布完全相同，此时在上升下限，Likelyhood也会最大化。所以也就是说损失函数中实际起作用的就是Lb这一项。\n所以此时，就是寻找P(x|z)、q(z|x)最大化Lb来最大化似然估计，同时顺便会找到q(z|x)相似于p(z|x)。\nL_b=\\int_zq(z|x)\\log\\frac{P(z,x)}{q(z|x)}dz = -KL(q(z|x)||P(z))+\\int_zq(z|x)\\log P(x|z)dz\n此时最大化Lb，也就是要最小化q(z|x)和P(z)的相似度，其中q是一个神经网络，用来提取输入x所服从的高斯分布，所以这里最小化的散度就是要调节q所对应的神经网络，让它产生的高斯分布与z这个高斯分布越接近越好，最小化散度这一项可以推导为：\nKL(q(z|x)||P(z)) = \\sum_{i=1}^3(\\exp(\\sigma_i)-(1+\\sigma_i)+(m_i)^2)\n这就是文章一开始提到的VAE架构中新增的需要最小化的损失函数。\n而Lb另外一项积分也要最大化，即：\n\\max \\space \\int_zq(z|x)\\log P(x|z)dz\n直观上就是从q中采样一个z分布，使得在在z分布下采样到的x的概率越大越好。这实际上就是Auto Encoder在做的事情。\n\n通俗的描述一下，就是说，首先q会从输入图像x中采样出一个(Normal Distribution )z,然后要最大化z分布产生x的概率，就是会把z作为NN的输入，输出一组高斯分布，使得这个高斯分布产生x的概率最大 。所以现在就是如何让这组高斯产生的x概率最大。\n实际上在训练时，我们不会取考虑方差，只需要让Decoder输出的均值**(mean)μ=x(Input)**即可，因为高斯分布在均值μ处采样的可能性最大，所以只需要让x=μ即可。也就是说这一部分就是让输入的x与输出尽可能的接近。\n同时，因为Encoder和Decoder的输出3都是一组分布，即现在图片对应的是一个分布，所以也就解决了把图片对应到了一个点上的问题，也就解决了Auto Encoder的存在的问题。\n所以Lb这两项合起来，就是文章一开始提到的VAE的两个损失函数。所以说VAE就精妙在损失函数的设计上面。\n妙哉妙哉！"},"C_Research_Knowledge/Batch-Normalization":{"slug":"C_Research_Knowledge/Batch-Normalization","filePath":"C_Research_Knowledge/Batch-Normalization.md","title":"Batch Normalization","links":[],"tags":["Optimization"],"content":"Problem\nBatch Normalization 批标准化 ，在传统的深度学习没有批标准化时，存在以下一些问题：\n\n难以训练、拟合   例如下图，x1值为比较小，x2则很大， 整体公式是 a = w1*x1 + w2*x2 + b ,此时就是图中左下角的图，w1影响就非常小了，相应的w1方向梯度就很小，w2方向梯度变化就非常大，这会使得模型难以训练、难以拟合。\n\n\n\n\nInternal Covariate Shift 内部协变量偏移\n神经网络有很多的隐藏层，每层都是以前一层的输出作为自己的输入，因此要对每一层做Feature Scaling, 这样会比较有效的解决该问题。\nInternal Covariate Shift问题 大致意思是强一层便宜会对后续产生很大的影响，每层的输入如果不做特征缩放，很大很小模型就需要一直调整去适应，就需要把每层的输入都固定下来\n\n\nFeature Scaling\n这种方法比较简单，对于每一维数据，计算平均数、标准差，然后每个数减去平均数除以标准差即可标准化。\n\nBatch Normalization\nNote :\n\nbatch_size 要够大才有意义，估算batch的分布。如batch_size 为1，则没有意义做这件事。\n一般先做标准化在进入激活函数\n\n流程\n\n首先计算出 一个batch的平均值、标准差\n\n\n\n具体的每个输出 - mean / 标准差\n\n\n\n\ntrain：训练时反向传播需要考虑到 σ μ\n\n\ntest：测试\n\n\n首先计算出训练过程中所有的 **σ μ **\n如上图的右上角 取三个 μ，靠近后面的 μ 给一个比较大的权值，靠近初始化的给比较小的权值以此来估计整个网络的 μ\n\n\n\nBatch Normalization好处\n\n解决了  Internal Covariate Shift 问题，可以设置比较大的学习率，增加训练速度\n可以有效防止梯度弥散 。之前讲sigmoid函数在比较深的网络是训练不起来的，因为梯度会消失，而每次采取了标准化，数据分布会比较靠近0附近，函数在此处斜率比较大，梯度会一直比较大\n对参数初始化不是那么敏感。例如所有的w都变成 w*k，但是经过公式推导经过标准化最终并无任何影响\n一定程度上解决过拟合  标准化从其实一定程度相当于正则化。\n\nBatch Normalization 在训练效果不好时可以使用，而测试效果不好则不一定要采用该方法。"},"C_Research_Knowledge/CNN-Receptive-Field-感受野":{"slug":"C_Research_Knowledge/CNN-Receptive-Field-感受野","filePath":"C_Research_Knowledge/CNN-Receptive-Field-感受野.md","title":"CNN Receptive Field(感受野)","links":[],"tags":["CNN"],"content":"最近阅读文献碰到了感受野，但是详细的一直没有深入理解，这里记录一下。\n定义\n感受野用来表示网络内部的不同神经元对原图像的感受范围的大小。换句话说，就是Feature Map一个像素映射到原始图像上所对应的范围。\n\n神经元感受野的值越大表示其能接触到的原始图像范围就越大，也意味着它可能蕴含更为全局，语义层次更高的特征；\n神经元感受野的值越小则表示其所包含的特征越趋向局部和细节。因此感受野的值可以用来大致判断每一层的抽象层次。\n\n对应到姿态估计就可以利用感受野来控制不同分辨率的特征。\n计算\n\n感受野可以通过下图更形象的理解。\n\n\n\nRaw Image是原始图像；\n用kerneal_size = 3*3，stride = 2的卷积核扫过得到conv1，故conv1中一个像素映射到原始图像是3*3；\nconv2中一个像素对应conv12*2，映射回原始图像为5*5\n\n\n上图则更加直观的看出了感受野的大小变花，感受野与卷积核尺寸、步长都有关系。\n下面给出计算公式：\nr_n = r_{n-1} + (k_n - 1)\\prod_{i=1}^{n-1}S_i\n其中，rn表示第n层的接受野；Si表示第i层的步长"},"C_Research_Knowledge/Classification":{"slug":"C_Research_Knowledge/Classification","filePath":"C_Research_Knowledge/Classification.md","title":"分类到回归的过程","links":[],"tags":["Classification"],"content":"DeepLearning中另外一个常见的问题就是分类问题，这里以二分类为例。\n*做二分类可不可以直接当作回归问题来处理呢？例如人为规定 calss1 对应输出值为1， class2对应值为-1，然后以均方误差为loss，做损失函数，这样直接train是可以train出来的，但是这样有一个问题，回归损失函数会惩罚比较大的正确项，例如一个数得出来结果是10远大于1，但是它和1明显是一类，而用回归做的话就会尽量减小这一loss，导致出现错误的结果，如下图：本来绿色的线是正确的分类，但是因为回归的惩罚会导致偏向紫色。 这是因为回归和分类的判断标准不一样。\n\n分类流程\n\n\n这里的model直接用的贝叶斯求出概率\n假设x服从高斯分布，此时要求出  μ，σ  用最大似然估计使得这个高斯分布所采样出来的这些点的概率最大\n求解最优的μ，σ。μ 就是一组书的均值mean  ，  μ，σ都有对应公式，也可以根据微分来求，如下图：\n\n\n分类与回归的转化\n这里用图片截图出所有的公式推导，可以从第一张图直接跳到最后一张图看结论\n\n\n\n\n\n也就是说分类函数经过一系列的转换，最终变成了线性函数，即  P(C1|X) = σ(w * x + b),之前的方法我们要求五个参数才可以计算到最终结果，现在我们可以直接计算w，b就可以得到最终的概率！此时就变成了回归问题。然后就有了后面的逻辑回归，用交叉熵求出w，b\n同时以上公式也说明了为什么 σ函数可以用来分类!"},"C_Research_Knowledge/Conformal-Prediction":{"slug":"C_Research_Knowledge/Conformal-Prediction","filePath":"C_Research_Knowledge/Conformal Prediction.md","title":"Conformal Prediction","links":[],"tags":["CP"],"content":"\n共性预测 Conformal Prediction 学习\n\n基本流程\n共性预测目标：\n对于任意一个模型，构造出一个预测集合，而不是单一的输出。这个集合满足，真实标签在集合中的概率大于等于 1-\\alpha。注意，这里模型输出都是logits，即每个类别的预测概率。\n具体流程：\n\n为了构造集合，需要n个校准集。首先对校准集每一个样本计算一个 Conformal Score（也称不一致性分数，越大表示预测越不可靠）：\n# 1: get conformal scores. n = calib_Y.shape[0] \n\tcal_smx = model(calib_X).softmax(dim=1).numpy()\n\tcal_scores = 1-cal_smx[np.arange(n),cal_labels]\n# Example\n\tcal_smx = np.array([\n\t  [0.9, 0.05, 0.05],   # sample 0\n\t  [0.2, 0.7, 0.1],     # sample 1\n\t  [0.3, 0.2, 0.5]      # sample 2\n\t])\n\tcal_labels = np.array([0, 1, 2])\n\n有 socre 之后，计算一个 \\hat{q},即大约1-\\alpha分位数，（如排序后第 1-\\alpha个数）\nq_level = np.ceil((n+1)*(1-alpha))/n\nqhat = np.quantile(cal_scores, q_level, method=&#039;higher&#039;)\n# example\ns = np.array([1, 2, 3, 4, 5])\nprint(np.quantile(s, 0.8, method=&quot;higher&quot;)) # 5\n# n=5,q=0.8, 计算分位点的位置：\n#p=q×(n−1)=0.8×4=3.2\n#这意味着分位点在第 3.2 个样本（从 0 开始计数）处。  \n#按照 `&quot;higher&quot;` 的规则：\n#找到 大于等于 3.2 的最小索引 → 索引 4（对应值 5）\n#所以输出 5\n\n为每个测试样本构造集合，C(X_{test}) = {y : \\hat{f}(X_{test})_y ≥ 1 − \\hat{q}},即测试样本类别概率大于等于1 − \\hat{q}\n就放进集合里。这样就完成了构造。\n"},"C_Research_Knowledge/Contrastive-Loss-对比损失-分析":{"slug":"C_Research_Knowledge/Contrastive-Loss-对比损失-分析","filePath":"C_Research_Knowledge/Contrastive-Loss-对比损失-分析.md","title":"Contrastive Loss (对比损失)分析","links":[],"tags":["Contrastive-Learning"],"content":"定义及分析\nContrastive loss 即对比损失，其表达式为：\nL = \\frac{1}{2N}\\sum_{n=1}^Nyd^2 + (1-y)max(margin-d, 0)^2\n其中，\n\n\ny表示标签值，取值为 1，0;\n\n\nmargin是人为定义的边界距离；\n\n\nd表示两个样本的欧氏距离\n\n\n输入为 ( dataA, dataB, label )\n\n该损失函数可以有效的处理孪生神经网络中的paired data的关系，具体逻辑是：\n\n若一对数据是同一组，那么label = 1 即y = 1 ，只有前半部分起作用，后半部分值为0；此时最小化loss, 两者的欧氏距离变小，就会使两者更相似；\n若一对数据非同一组，那么label = 0 即y = 0， 只有后半部分起作用，前半部分值为0，此时最小化损失函数，即最小化margin - d, 就等于最大化d ,即两者的欧氏距离变大，更不相似\n\n梯度求解\ny = 1时，\nL = \\frac{1}{2N}\\sum_{n=1}^NyD^2 =  \\frac{1}{2N}\\sum_{n=1}^N(||x_1-x_2||_2))^2\n\\frac{\\partial L}{\\partial X} = \\frac{\\partial D_w}{\\partial X} = ||x_1-x_2||_2*\\partial D/\\partial X\n此处省略，以后补充\n代码实现\n# ------------------------------------------------------------------------------\n# # @Time    : 2020/5/17 上午 11:19\n# # @Author  : fry\n# @FileName: clac.py\n# ------------------------------------------------------------------------------\n&#039;&#039;&#039;\n计算欧式距离\n&#039;&#039;&#039;\n \nimport torch\n \ndef euclidean_dist_vector(x, y):\n    &#039;&#039;&#039;\n \n    :param x: [100]\n    :param y: [100]\n    :return:\n    &#039;&#039;&#039;\n    dist = torch.sqrt(torch.sum(torch.pow((x-y), 2)))\n    return dist\n \ndef euclidean_dist(x, y):\n    &quot;&quot;&quot;\n    Args:\n      x: pytorch Variable, with shape [m, d]\n      y: pytorch Variable, with shape [n, d]\n    Returns:\n      dist: pytorch Variable, with shape [m, n]\n    &quot;&quot;&quot;\n \n    m, n = x.size(0), y.size(0)\n    # xx经过pow()方法对每单个数据进行二次方操作后，在axis=1 方向（横向，就是第一列向最后一列的方向）加和，此时xx的shape为(m, 1)，经过expand()方法，扩展n-1次，此时xx的shape为(m, n)\n    xx = torch.pow(x, 2).sum(1, keepdim=True).expand(m, n)\n    # yy会在最后进行转置的操作\n    yy = torch.pow(y, 2).sum(1, keepdim=True).expand(n, m).t()\n    dist = xx + yy\n    # torch.addmm(beta=1, input, alpha=1, mat1, mat2, out=None)，这行表示的意思是dist - 2 * x * yT\n    dist.addmm_(1, -2, x, y.t())\n    # clamp()函数可以限定dist内元素的最大最小范围，dist最后开方，得到样本之间的距离矩阵\n    dist = dist.clamp(min=1e-12).sqrt()  # for numerical stability\n    return dist\n \n \n# ------------------------------------------------------------------------------\n# # @Time    : 2020/5/11 下午 7:52\n# # @Author  : fry\n# @FileName: MatchLoss.py\n# ------------------------------------------------------------------------------\n \nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom utils.clac import euclidean_dist,euclidean_dist_vector\n# match loss function\n \nclass ContrastiveLoss(nn.Module):\n    &quot;&quot;&quot;\n    Contrastive loss function.\n \n    Based on:\n    &quot;&quot;&quot;\n \n    def __init__(self, margin=1.2):\n        super(ContrastiveLoss, self).__init__()\n        self.margin = margin\n \n \n    def check_type_forward(self, in_types):\n        assert len(in_types) == 3\n        #[1,1,10,10] [1,1,10,10] [10]\n        x0_type, x1_type, y_type = in_types\n        assert x0_type.size() == x1_type.shape\n        # assert x1_type.size()[1] == y_type.shape[0]\n        # assert x1_type.size()[0] &gt; 0\n        assert x0_type.dim() == 4\n        assert x1_type.dim() == 4\n        # assert y_type.dim() == 1\n \n    def forward(self, x0, x1, y):\n \n        # self.check_type_forward((x0, x1, y))\n \n        data_1 = x0 # [1,10,10]\n        data_2 = x1 # [1,10,10]\n        # euclidian distance\n        distance = euclidean_dist_vector(data_1, data_2)\n        # distance = euclidean_dist(data_1, data_2)\n        md = self.margin - distance\n        dist = torch.clamp(md, min=0.0)\n        loss = y * torch.pow(distance, 2) + (1-y)*torch.pow(dist, 2)\n        loss = torch.mean(loss/2)\n        # loss = torch.sum(loss) / 2.0\n        return loss\n# if __name__ == &#039;__main__&#039;:\n#     citer = ContrastiveLoss()\n#     a = torch.randn(10,10)\n#     b = torch.randn(10,10)\n#     y = torch.ones(1)\n#     loss = citer(a,b,y)\n#     print(loss)"},"C_Research_Knowledge/Cross-Entropy-的前世今生":{"slug":"C_Research_Knowledge/Cross-Entropy-的前世今生","filePath":"C_Research_Knowledge/Cross-Entropy-的前世今生.md","title":"Cross Entropy 的前世今生","links":[],"tags":["Classification"],"content":"Deep Learning 中可以分为分类问题和回归问题，而分类问题也可以转化为回归问题，具体可以看Classification 这篇文章。回归问题一般用均方误差作为损失函数(显而易见)，而分类问题一般则用交叉熵作为损失函数，这是为什么？这一切要从信息熵说起…\n信息熵&amp;交叉熵\nwww.zhihu.com/question/41252833/answer/195901726 这篇知乎文章很通俗的解释了交叉熵的概念，总结一下：\n信息论中，使用信息熵来对概率分布进行量化。**信息熵代表的是随机变量或整个系统的不确定性，熵越大，随机变量或系统的不确定性就越大。**根据真实分布，我们能够找到一个最优策略，以最小的代价消除系统的不确定性，而这个代价大小就是信息熵，记住，信息熵衡量了系统的不确定性，而我们要消除这个不确定性，所要付出的【最小努力】（猜题次数、编码长度等）的大小就是信息熵。\n但是并不是每次都知道真实分布，在深度学习中是不知道真实的分布的，这时候就引入交叉熵：交叉熵，其用来衡量在给定的真实分布下，使用非真实分布所指定的策略消除系统的不确定性所需要付出的努力的大小。\n交叉熵公式如下：\n\\sum_{k=1}^Np_k * log_2^{\\frac 1q_k}\n其中(www.zhihu.com/equation) 表示真实分布，(www.zhihu.com/equation) 表示非真实分布。交叉熵最小值就是采用真实分布预测出来的结果，此时\n交叉熵 == 信息熵 , 因此在机器学习中要最小化交叉熵，此时就最近真最佳策略。\n深度学习中的交叉熵\n举一个最简单的例子，y = w * x + b，目标是输入为1输出为0.经过 σ 函数进行二分类,首先定义损失函数(先用均方误差)：\nC = (y-a)^2/2 \n其中y为真实值，a为我们算出来的值，因为要经过 σ 函数进行分类，所以 a = σ (w*x + b),令 z =  w*x + b,a = σ(z)，用梯度下降法求w，b就需要先对其求微分，用到链式求导法则：\n\\frac{\\partial C}{\\partial w} = \\frac{\\partial C}{\\partial a} *\\frac{\\partial a}{\\partial w} \\\\\n\\partial C / \\partial a = a-y\\\\\n\\partial a / \\partial w =  \\sigma&#039;(z)*x\n此时设x=1，相应的y=0，\n\\partial C / \\partial w = a *\\sigma&#039;(z)\\\\\n\\partial C / \\partial b = a *\\sigma&#039;(z)\n因为经过了sigmoid函数，所以当z较大时，梯度很小，此时会发生梯度弥散,更新参数会很慢，那么如何加快参数更新？交叉熵是凭空出现的吗?若果没有σ这项：\n\\partial C / \\partial w_i = (a-y)*x_i\\\\\n\\partial C / \\partial b = a-y\n考虑之前的推导：\n\\partial C / \\partial w_i = \\frac {\\partial C}{\\partial a} *\\sigma&#039;(z)x_i=\\frac {\\partial C}{\\partial a} *a(1-a)x_i\n令（6）和（5）相等可得出:\n\\partial C / \\partial a = \\frac {a-y}{a(1-a)} \n此时用积分求出原函数即：\nC = -[y\\ln a+(1-y)\\ln (1-a)] + constant\n这就是二分类中交叉熵的函数形式，伯努利二项分布。\n由此可以知道交叉熵的来源、推导。也解释了分类问题为什么用交叉熵作为损失函数，简而言之，交叉熵作为损失函数，消去了σ这一项，从而变成线性的，解决了更新参数很慢的问题，这时候再引用一张之前的图就一目了然了：\n"},"C_Research_Knowledge/GAN-4-f-GAN推导":{"slug":"C_Research_Knowledge/GAN-4-f-GAN推导","filePath":"C_Research_Knowledge/GAN-4-f-GAN推导.md","title":"GAN-4-f-GAN推导","links":["C_Research_Knowledge/GAN对抗网络初识","C_Research_Knowledge/GAN-Basic-Theory","C_Research_Knowledge/手动实现GAN网络生成动漫头像","C_Research_Knowledge/GAN-4-f-GAN推导"],"tags":["GAN"],"content":"*GAN系列知识\nGAN对抗网络初识\nGAN-Basic-Theory\n手动实现GAN网络生成动漫头像\nGAN-4-f-GAN推导\n为什么要引入f-GAN？\n之前谈到GAN基础理论，可以知道GAN的生成器其实就是在最小化 P_data 和 P_G 的 JS散度。而事实上不一定要用JS-Divergence 来测量，KL-Divergence、JS-Divergence等都是属于f-Divergence，故f-GAN其实就是想要换掉GAN中的分布之间的距离测度函数。\nf-Divergence\nD_f(P||Q) = \\int_x q(x)*f(\\frac{p(x)}{q(x)})dx\n其中，f是convex凸函数, f(1) = 0，故：\n\n其中&gt;=用到了詹森不等式.\n\n若p(x),q(x)相等，则值为0；\n若其不相等，则Df&gt;=0  因此 Df可以用来测量分布之间的距离\n\n\n此时f(x)取不同的表达式，得到不同的散度。\n共轭函数\n\n每一个凸函数f都有一个共轭函数f*\n对于函数f(x),\n\n首先t取t1，x取遍定义域内所有的值x1…xn,找出最大的f^*(t1) = x_1t_1-f(x)\n对于每一个t都用相同的方法计算，就可以知道最终的f*\n\n这样计算太麻烦，于是：\n\n如上图，画出所有的函数图像，取得图像上界即可。\n\n当f(x) = x\\log(x)时，求的其共轭函数为：f^*(t) = exp(t-1)\nConnection with GAN\n\n这块的逻辑是这样：\n\n首先 f(x) 于 f* 互为共轭函数，故可以有第一行的公式，然后把它代入D中，得到上图中得第三行的公式，目前问题变为：找一个t，使得这个积分项最大；\n直接找t不好找，所以训练一个判别器D，输入是x，输出是t，D(x) = t,便得到上图下部分得公式，所以问题又转化为寻找一个D，可以最大两个积分项做减法得值\n简单来说，就是把寻找t变为寻找D(x)\n\n\n这里就很顺理成章了，P_data 和 P_G 之间的f-divergence就可以求出来了;而D(x)、f()取不同的表达式 ，得到的就是不同的f-divergence。*\n对于生成器来说，就是最小化真实数据和生成数据之间的f-divergence\n\n其中，Generator f(u) 就是上面公式中的D(x)表达式；f*(t)就是上面公式的 f*.\n以上就是f-GAN得数学推导过程。"},"C_Research_Knowledge/GAN-Basic-Theory":{"slug":"C_Research_Knowledge/GAN-Basic-Theory","filePath":"C_Research_Knowledge/GAN-Basic-Theory.md","title":"GAN2_Basic_Theory","links":["C_Research_Knowledge/GAN对抗网络初识","C_Research_Knowledge/GAN-Basic-Theory","C_Research_Knowledge/手动实现GAN网络生成动漫头像","C_Research_Knowledge/GAN-4-f-GAN推导"],"tags":["GAN"],"content":"*GAN系列知识\nGAN对抗网络初识\nGAN-Basic-Theory\n手动实现GAN网络生成动漫头像\nGAN-4-f-GAN推导\n这篇文章主要来讨论下GAN基础的理论、公式推导。\nKL 散度\nKL Divergence 指的是相对熵，其用来衡量两个取值为正的函数或概率分布之间的差异；\n相对熵 = 某个策略的交叉熵 - 信息熵（根据系统真实分布计算而得的信息熵，为最优策略）  ,  公式如下:\nKL(P || Q) = H(P,Q) - H(P) = \\sum_{k=1}^N P_k*log_2\\frac {1} {Q_k} - \\sum_{k=1}^N P_k*log_2\\frac {1} {P_k} = \\sum_{k=1}^N P_k*log_2\\frac {P_k} {Q_k}\n复习一下，交叉熵是使用非真实分布，信息熵是使用真实分布计算得到的。\nJS 散度\nJS Divergence  是 KL散度的变体，衡量两个分布之间的相似性。公式如下：\nJS(P||Q) = \\frac 1 2 KL(P||\\frac{P+Q}{2}) + \\frac 1 2 KL(Q||\\frac{P+Q}{2})\nFormula Of GAN\n首先，生成器要做的事情就是生成一个最靠近真实data的分布，这在之前可以用最大似然估计来找这样的一个分布，最大似然估计实际上可以与KL散度扯上关系：\n\n这里面需要注意的是约等于 这一步，意思是最大值求和m笔data的概率其实就是求x服从于真实分布的情况下概率的最大值，所以就 = 下面的积分，而下面的积分后边减去的一项不影响求最大值，所以再意思上是相等的，但是真实值上并不相等。然后上面的公式主要是来说明：\n最大化最大似然估计概率就是最小化KL散度，即：\nG^* = arg\\underset G min Div(P_G,P_{data})\n所以要训练Generator，也就是要计算出真实数据和生成数据之间的散度，而Discriminator则可以做到这一件事。\n在训练Discriminator时，在固定住Generator的情况下进行，定义对象方程：\n\n这里后面一项之所以要定义为1-D(x),是因为判别器做的事情是给Pdata高分，给Pg低分，也就是使得生成的数据和真实的数据分布的散度最大。因此在x服从Pg分布时，写1-D(x) 就可以最大化这个式子，比较容易train，即：\nD^* = arg\\underset D max V(D,G)\n实际上，当生成的图像和真实图像散度比较大，相似度比较小时，判别器是很好训练的，而相似度比较大就比较难训练，所以训练好的判别器其实就是在最大化生成的图像和真实图像的散度。下面证明这一项就是在最大化JS 散度：\n\n\n\n\n这里最后一步可以参考一开始文章提到的JS散度对比一下公式就会明白。所以此时：\nG^* = arg\\underset G min (\\space \\underset D maxV(D,G))\n此时梯度下降法求解G*:\n\n这里的max取微分可以看作分段函数求微分。所以现在已经可以铜鼓哦梯度下降法来更新生成器了，然后进行算法：\n\n这里有一个问题，事实上更新G1参数真的是在减小JS散度吗？\n\n举个例子如上图，在G更新后，G的分布已经变了(由左到右)，此时的D0*依然是固定的未发生变化，因此此时的D0*已经不再是max值了，因此此时（伪）max V(G,D)  便不再是JS散度了。所以有个假设，G的变化不能太大，每次更新一点点，否则误差会过大。\n以是就是GAN的基础理论公式推导。\nPractice\n\n最后整体回顾一遍算法流程：\n\n**注意的是一般Generator只训练一次 **。原因前面说了 。\n以上就是GAN 对抗网络的 Basic Theory，还有其他的一些之后的细节可以看李宏毅老师的视频。有错误欢迎指正！"},"C_Research_Knowledge/GAN对抗网络初识":{"slug":"C_Research_Knowledge/GAN对抗网络初识","filePath":"C_Research_Knowledge/GAN对抗网络初识.md","title":"GAN对抗网络1_初识","links":["C_Research_Knowledge/GAN对抗网络初识","C_Research_Knowledge/GAN-Basic-Theory","C_Research_Knowledge/手动实现GAN网络生成动漫头像","C_Research_Knowledge/GAN-4-f-GAN推导"],"tags":["GAN"],"content":"*GAN系列知识\nGAN对抗网络初识\nGAN-Basic-Theory\n手动实现GAN网络生成动漫头像\nGAN-4-f-GAN推导\nGAN基本原理\n参考链接：www.bilibili.com/video/av24011528\n首先，GAN全称是Generative Adversarial Nets ，一般叫对抗网络。由两部分组成：Generator(生成器)、Discrimator(判别器)。\n\nGenerator\n\n生成器可以认为就是一个Neural Network。以CV为例，现在的目标是生成图片，则生成器的工作就是输入一个vector，然后输出一张图像(动漫头像)，其中某一维可能对应头发长短，另一维对应眼睛颜色，这样通过调整向量就可以控制自己需要的头像，类似于下图：\n\n\nDiscrimator\n\n判别器也是一个Neural Network,  顾名思义，是判断生成器生成的东西的好坏，输入是一张image(动漫头像), 输出一个scalar(标量，数字) , 衡量生成器生成的image的好坏程度。如下图：\n\nGAN算法流程 Algorithm\n\n初始化 Generator 、Discrimator\n在每一个迭代中：\n\n\n固定生成器，训练判别器，生成器目标是欺骗过判别器，也就是生成器生成的图片判别器给出的分数越高越好\n固定判别器，训练生成器\n重复以上步骤，判别器和生成器在同时进行学习，同时进化，相互博弈，直到生成器能瞒过判别器就算是训练好了\n\nStructured Learning\n结构化学习是很有挑战性的，例如生成图像，很重要的一个东西是像素之间的关系，像素本身是没有什么错误的，但是它们之间的关系很重要。\n\nGenerator 是先画每一个部件，最后形成一幅 image；\n\n\nDiscrimator 则是整体上判断图像好不好，并且可以挑选出来最好的图像；\n\n两者结合起来就是GAN对抗网络了，可以发挥各自的性能，如下图：\n\nQ1:为什么Generator(生成器)不自己学习呢？\n首先生成器是输入一个vector，输出是image，这和Auto Encoder (自编码器)技术中的Decoder(解码器)工作室一样的，也就是说Decoder就是一个Generator。\nAuto Encoder自编码器是有缺陷的，也就是说一张图片只是对应到一个点，从这个点解码出图像，如果取一个没有出现过的点进行解码，解码出来的就是noise了，也就是说只能解码出现过的，因此出现了VAE技术：\n\n也就是在编码图像时添加了噪音等，这样采样任何一个点都会解码出来比较真实的图像。在这两种技术中都有Decoder, 那么这种技术缺少了什么？\n对于生成器来说，部件之间的关系很重要，而在解码器中神经元相互之间是没有关系的，也就是部件之间很难产生联系，这就直接导致很容易失去大局观，整体性就比较差。\n关于部件之间的练习，可以看这个例子：\n\n人眼来判断的话当然觉得下面看两幅图比较好，但是Decoder则会认为上面两幅图比较好，因为逐像素比较确实是前两幅图误差小，这就很容易看出来没有考虑到整体性。事实上Decoder很难做到这一点。\nQ2:为什么Discrimator(判别器)不自己生成呢？\n判别器既然可以判断生成的图像好不好，为什么不自己生成呢？\n\n还是用上面的例子，比如判别器可能有这样一个卷积核，即中心有颜色其他地方没有，看到这样的就给图像低分，判别器很容易判断图像整体性。\n事实上判别器就像是批评家，比较形象的可以说是**“键盘侠”**，不管你说什么他只给说哪里不好，你要问他什么是好的他可能说不出来这样子。严密的论证可以看文章开始的视频链接讲到的，简单来说就是：\n\n训练判别器首先不能只是正样本，也需要一些负样本，而我们手上是没有负样本的，因此需要生成负样本\n要产生比较好的负样本就需要一个比较好的判别器才能找到比较好的负样本\n\n因此，就陷入了鸡生蛋、蛋生鸡的问题。因此训练单独的一个判别器首先要假定已经有比较好的负样本。训练流程类似，可以看视频讲解。\nSummary\n总结一下，生成器和判别器各自的优势劣势：\n\nAdditional\n\n上面提到GAN训练时要先训练判别器，这是为什么？后面文章会有公式理论说明这个问题。\n上面提到的 Auto Encoder VAE 这两种编码技术随后文章会详细讨论细节问题。\n"},"C_Research_Knowledge/HMM隐马尔可夫模型-到-CRF条件随机域":{"slug":"C_Research_Knowledge/HMM隐马尔可夫模型-到-CRF条件随机域","filePath":"C_Research_Knowledge/HMM隐马尔可夫模型-到-CRF条件随机域.md","title":"从 HMM隐马尔可夫模型 到 CRF条件随机域","links":[],"tags":["Markov","CRF"],"content":"李宏毅老师课程：www.bilibili.com/video/av82565851/\n知乎文章：zhuanlan.zhihu.com/p/70067113\nHMM\n整体分析\n以词性标注为例，从问题入手，输入X（word），输出Y(tag) ，这里的词性Y是Hidden隐藏的,X是可观测到的Observed。\n隐马尔可夫链模型（HMM），着手于建模P(X,Y)即两个序列的联合概率（可以看出来是生成式模型），有条件概率转为联合概率过程如下：\ny = argmax_yP(y|x) \\\\\n \\space= argmax_y{\\frac {P(x,y)}{P(x)}} \\\\\n  \\space= argmax_yP(x,y)\n目前为题转化为 穷举所有的y，使得P(x,y)最大。\n这里直接计算复杂度会很大 ，所以采用维特比算法 可以求解出这个问题，并且复杂度不是很高。\n\n层层深入\n这里的P(x,y)如何求解呢？\nP(x,y) = P(x|y)*P(y) \\\\\n\\space =\\prod_{i=1}^NP(x_i|y_i)*\\prod_{i=1}^{N}P(y_i|y_{i-1})\n其中，\n\n\n第一项条件概率称为发射概率，指从一个词性中抽到某个单词的概率，它基于观测独立性假设，即假设任意时刻的观测只依赖于该时刻的马尔可夫链的状态，与其他观测及状态无关。\n\n\n\n第二项则是由**马尔可夫假设**得到，如下图：\n\n\n\n即马尔可夫最终学习的就是发射概率矩阵(x*y)和·转移概率矩阵(y*y),对应起来就是 当前词性产生下一个词性的概率；从当前词性中选出某个词的概率。\n\n缺陷\n求解时，目标是：\nP(x,\\hat y) &gt; P(x,y)\n即真实概率大于预测概率，但是HMM并不会保证这一点。\n故HMM存在的一个缺陷是会脑补很多东西，具体可以看李宏毅老师视频。脑部在数据不充分时比较好，但是数据量大依然会凭空捏造，这样效果就不太好。\nCondition Random Field 条件随机域\n由HMM到CRF\n在CRF中，\nP(x,y) \\approx \\exp(W·\\phi(x,y))\n即这两项成正比，其中：\n\nW是权重向量，从训练数据中学得\nΦ(x,y) 是一个特征向量\nexp()由于不能保证小于1，故是正比于\n\n依然从问题入手，输入X输出Y，求最大化的P(Y|X)：\nP(y|x) = \\frac {P(x,y)}{P(x)} = \\frac {P(x,y)}{\\sum_{y&#039;}P(x,y&#039;)}\\\\\n由上述两者成正比可以推导得：\nP(x,y) = \\frac{\\exp(W·\\phi(x,y))}{R}\n故：\nP(y|x) =\\frac {P(x,y)}{\\sum_{y&#039;}P(x,y&#039;)}\\\\\n=\\frac{\\exp(W·\\phi(x,y))}{R} * \\frac{R}{\\sum_{y&#039;}\\exp(W·\\phi(x,y&#039;))}\n= \\frac{\\exp(W·\\phi(x,y))}{\\sum_{y&#039;}\\exp(W·\\phi(x,y&#039;))}\n现在问题来了，为什么P(x,y)可以写成后面那一项？······具体推导省略，过程可以看李宏毅老师视频\n\n其中，\n\n权重向量W可以对应为HMM中计算的概率取log\nNs,t(x,y)则是这一对出现的次数，记作Φ(z,y)\n\nΦ(x,y)特征向量由两部分组成，一部分是词性产生词性的pair的次数，另一部分是词性中产生相应词汇的次数，形状如下：\n\n\n\nTraining\n上述特征向量Φ可以自定义，故要求的参数即为权重w，即：\nW^* = \\arg max_wO(w)\n其中，对象方程为：\nO(w) = \\sum_{n=1}^N\\log P(\\hat y_n|x_n)\n其中：\n\\log P(\\hat y_n|x_n) = \\log P(x_n,\\hat y_n) - \\log {\\sum_{y&#039;}P(x_n,y&#039;)}\n所以目标就是最大化上述公式的前一项，最小化上述公式后一项。而前一项是观测出现的概率，后一项是没有观测到即随机组合出现的概率，这在直观上也是正确的。\n然后采用梯度下降法来求解：\n\n即：\n\\Delta O(W) = \\phi(x_n,y_n) - \\sum_{y&#039;}P(y&#039;|x_n)\\phi(x_n,y&#039;)\n由于是最大化所以采用 Stochastic Gradient Ascent 更新参数：\nw = w + \\eta(\\phi(x_n,y_n) - \\sum_{y&#039;}P(y&#039;|x_n)\\phi(x_n,y&#039;))\n在Inference时，\ny = argmax_yP(y|x) \\\\\n \\space= \\arg max_y{\\frac {P(x,y)}{P(x)}} \\\\\n  \\space= \\arg max_yP(x,y)\\\\\n \\space= \\arg max_y\\exp(W·\\phi(x,y))= \\arg max_yW·\\phi(x,y)\nCRF与HMM\nCRF在增加标签对出现的概率的同时，也减小了其他随意组合的对出现的概率，这就抑制了HMM容易脑补的问题。\nHMM是生成式模型，最终在求解联合概率；\nCRF实际上是判别式模型，从Inference可以看出，是直接求出了条件概率。"},"C_Research_Knowledge/Mamba-模型":{"slug":"C_Research_Knowledge/Mamba-模型","filePath":"C_Research_Knowledge/Mamba 模型.md","title":"Mamba 模型","links":["D_Paper_Related/Vision-Mamba---Efficient-Visual-Representation-Learning-with-Bidirectional-State-Space-Model","D_Paper_Related/VMamba---Visual-State-Space-Model","D_Paper_Related/U-Mamba---Enhancing-Long-range-Dependency-for-Biomedical-Image-Segmentation","D_Paper_Related/Mamba-UNet---UNet-Like-Pure-Visual-Mamba-for-Medical-Image-Segmentation","D_Paper_Related/U-shaped-Vision-Mamba-for-Single-Image-Dehazing"],"tags":["Mamba"],"content":"\n最近学习了很火的 Mamba 模型，此处做一个简单 High-level 的记录，后续有新理解在补充细节\nMamba 全称为 Selective State Space Models （SSMs），是一种新的模型架构，主要目的是解决长序列建模（long range dependency）。\n直观的说，该模型是为了改进 Transformer 计算效率以及 window attention，与序列长度成线性关系(Transformer为 O(N^2) )。\n\n相关学习资料\n\n原文/Code：《Mamba: Linear-Time Sequence Modeling with Selective State Spaces》)\n相关前身论文：《Efficiently Modeling Long Sequences with Structured State Spaces》\n知乎讲解：如何理解 Mamba 模型 Selective State Spaces?\nBlog： The Annotated S4\nGithub related papers: Awesome-Mamba-Collection\n\n系列论文更新\n\nVision Mamba - Efficient Visual Representation Learning with Bidirectional State Space Model\nVMamba - Visual State Space Model\nU-Mamba - Enhancing Long-range Dependency for Biomedical Image Segmentation\nMamba-UNet - UNet-Like Pure Visual Mamba for Medical Image Segmentation\nU-shaped Vision Mamba for Single Image Dehazing\n\n系列背景\n很长的序列建模用现有的 RNN/CNN/Transformer 很困难，状态空间模型  State Space Model (SSM) 则比较合适。\n\nVanilla SSM (A Continuous-time Latent State Model)\n\nStep1—Definition\n\n\\begin{aligned}\nx&#039;(t) = \\mathbf{A}x(t) + \\mathbf{B}u(t)\\\\\ny(t) = \\mathbf{C}x(t) + \\mathbf{D}u(t)\n\\end{aligned}\n原始的状态空间模型可以由 A  B  C  D 四个参数形式化。D 可以忽略，设为 0 就是一个残差连接，不影响计算。简单理解上述公式，SSM目的是将一个 1-D 的信号首先映射为一个状态表示向量x(t) 同时计算输出y(t)。公式x&#039;(t) [微分] 定义了状态向量x(t)的变化。\nTensor Dimension: u(t) -- 1D   x(t) -- ND    y(t) -- 1D\n\nStep2—Discrete-time：Recurrent Representation\n为了将连续的 SSM 应用于离散数据，需要通过 \\Delta(Step size) 对其进行离散化 (blinear method 双线性方法对状态矩阵 A 求了近似 \\bar{A})，此时上述公式形式化为：\n\n\\begin{aligned}\n\\bar{A} &amp;= (I - \\Delta / 2 \\cdot A)^{-1} (I + \\Delta / 2 \\cdot A) \\\\\n\\bar{B} &amp;=  (I - \\Delta / 2 \\cdot A)^{-1} \\Delta B \\\\\n\\bar{C} &amp;= C\n\\end{aligned}\n离散化之后可以学习 seuqence-to-sequence 的 mapping u_k​↦y_k​:\n\\begin{aligned}\nx_k &amp;= \\bar{\\mathbf{A}}x_{k-1} + \\bar{\\mathbf{B}}u_k\\\\\ny_k &amp;= \\bar{\\mathbf{C}}x_k \n\\end{aligned}\n这里的公式其实就和 RNN很类似了，考虑当前状态和历史状态。\n\nStep3—HiPPO Matrix (Addressing Long-Range Dependencies)\n在实际应用中，直接学习这几个参数矩阵结果很差，由于梯度爆炸/消失等问题。此时引入HiPPO 理论来学习一个 A \\in \\mathbb{R}^{N \\times N} 矩阵,可以解决实际学习问题。这里只记录一下基本过程/公式/理论，具体的内容可以先不用理解。\n\n\n\nStructured State Space Model (S4 Model)\nS4模型是 Mamba 的前置工作，主要是因为上述 SSM学习矩阵参数时计算量太大，因此采用了对角化，Normal Plus Low-Rank 来将上述学习的 A 矩阵进行分解:\nA = VΛV^∗ − P Q^\\mathsf{T} = V (Λ − (V^{∗}P) (V^{∗}Q)^{∗})V^∗\n\n\nSelective State Space Model (Mamba)\nMamba主要解决两个问题：\n\n之前的 S4 模型学习的时候参数是固定的，不能基于输入做出改变，也就是灵活性不高，影响建模能力 Time-invariant\n把 S4 参数矩阵改进成基于输入的函数，无法进行类卷积的并行运算了会很慢，需要提高计算效率，提出硬件感知的并行扫描算法 （关于 efficient 计算不是我关注的，所以我没有重点去看这部分）\n\nMamba 公式起源在上述其实已经提过了，这里在重新写一下：\n\n\n首先，(2a) (2b) 可以看出 Mamba 与 RNN 非常类似\n其次，公式 3 是通过将 2 进行展开得出的，相当于是一个序列卷积（Global Attention)。关于这点可以参考知乎上，我也手动推过确实没问题\n\n随后，这里用 ZOH （zero-order hold） 理论进行离散化：\n\n解决方案\n\n\n\n在提高建模能力方面，主要是把参数学习的维度和方式改了。\n相比于 S4，\n\nA 矩阵未发生变化（S4推导的分解后的矩阵）；D 表示对于每一维都有一个学习的 N\\times N 维的状态矩阵，但是不同输入都是 share 的；但是由于\\bar{A} = exp(\\Delta A), 这个学习矩阵会被  \\Delta 调制成 Time-Varying\nB / C 矩阵以前是每个通道有一个 N 维的，现在是对于每一个 sample（batch * length）都有一个 N 维的，但是注意这里每个通道是一样的，这里感觉就是降低计算量。\n\\Delta 以前是对于每一维有一个，现在是对于每个sample 的每个维度都有一个 time step 参数；\\Delta 越大，越聚焦于当前时刻输入；反之越小越关注历史记忆信息。\n\n\n**在计算方面，由于现在的时变性，就无法抽象出公式(3a)的归纳了，由于A_1, A_2, B_1, B_2, C_1, C_2 等等。这里作者提出了”Parallel Scan”来进行并行，提高效率，这里我就没关注了。\n\nMamba 和 RNN\n\n\n相比与典型的 RNN，Mamba 去掉了 tanh 非线性激活，促进并行运算；\nMamba 的 Hidden state 维度比较大，对于每维都单独学习\n在 Mamba 中，A 矩阵设计比较高级\nMamba可以 Input-dependent（像 Transformer），灵活性（建模能力）高\n\n\n以上就是 Mamba 整个系列知识。我感觉这个模型本身不难，重点是一系列的背景知识，包括状态空间模型 SSM 以及改进的 S4 模型 。了解完背景之后mamba 本身就会比较容易，改动实际上也并不大（不关注计算部分），就是 \\Delta 参数还未完全搞明白。\n整个理解下来，感觉要用在姿态估计领域，还是有点点悬，就是这里一直声称是超长序列建模，我感觉精度会不如 Transformer，不过比较新，值得一试，我感觉可以试试 sequence-sequence 的姿态估计方法。要么就是说，把像素看成 token，就会有很长的序列，就可以说 transformer 会退化，而 Mamba 效果会比较好。下一步应用的话需要看相关的论文 + 代码。"},"C_Research_Knowledge/Recurrent-Neural-Network-RNN":{"slug":"C_Research_Knowledge/Recurrent-Neural-Network-RNN","filePath":"C_Research_Knowledge/Recurrent-Neural-Network-RNN.md","title":"Recurrent-Neural-Network-RNN","links":[],"tags":["RNN","LSTM"],"content":"RNN,循环神经网络，一般用来处理序列化任务。例如一个句子就是一个词序列；一段视频就是一个图像序列；人讲一句话也是一个序列，处理序列就考虑到了时间维的信息。\nSIMPLE RNN\n最简单的一个RNN循环神经网络就是存储了上一时刻的隐藏状态的值。简单的讨论一下RNN的设计结构。\n\n假设只有一个隐藏层，有两个句子：\n\nleave Taipei\narrive Taipei\n\n如果采用一般的神经网络，见到Taipei这同一个词汇，输出的值都是相同的，与预期任务就不符合了。\n首先leave作为第一时刻的输入，产生一个值；隐藏状态值是a1要存储起来，如上图的蓝色a1，然后下一时刻输入Taipei就同时收到x2、a1 的影响。后面的也同理，因此同一个输入就有不同的输出。\n上面是一个简单的单向的RNN，还有双向的RNN，如下图：\n\n训练一个正向的和一个反向的循环神经网络，然后把同一时刻(xt)两个网络的隐藏层拿出来同时计算该时刻的输出(yt).也就是说计算每一时刻的输出都要考虑到整个序列。\nLong Short-term Memory（LSTM）\n上面的RNN是一个简单的循环神经网络的结构，它的记忆非常非常有限，只能记住前一个时刻的，这对我们来说仍然是不足的，因此需要记住更久的东西，就有了LSTM。（注意名字，是多个短期记忆，一般叫长短期记忆网络，应该是很长的短期记忆网络，而不是长-短 期记忆网络。）\nLSTM网络结构如下：\n\n它的核心在于中间蓝色部分的记忆细胞。\n它的计算过程观察上图，首先有 z,z_i,z_f,z_o 四个gate，这四个门可以理解为四个控制信号，其中\n\nz 表示的是要输入的信息的信号；\nz_i 表示的是输入门的信号；\nz_f 表示的是遗忘门的信号；\nz_o 表示的是输出门的信号；\n\nCell = \\sigma(z) * \\tanh(z_i) + c * \\sigma(z_f)\\\\\nOut = \\tanh(Cell) * \\sigma(z_o)\n其中， z,z_i,z_f,z_o的值就是输入x_t 乘上四个矩阵得到的，整个流程如下图：\n\n细胞贯穿主线，xt的输入得到输出yt嘛，中间经过一系列的组合计算；注意上图中z其实也是有激活函数的，z激活函数是tanh().\n上图只是展示了LSTM计算过程，实际操作时会照以下操作（单个LSTM）：\n\n会把前一时刻的输出值(h_{t-1})、记忆单元值(c_{t-1})、这一时刻的输入值 (x_T)堆在一起，一起再去计算，堆在一起就是torch中的级联：torch.cat()\n在实际训练网络时单个LSTM肯定是不够的，所以需要计算多个的，此时：\n\n横向的都是同一个LSTM，纵向的是堆起来的不同的LSTM模块。\n下面总结一下LSTM模块：\n\n这张图实际上前一时刻的细胞状态是没有和输入叠在一起的。以上就是整个LSTM架构，而它的几个变体都是大同小异。\nWhy Lstm not Simple-RNN?\nRNN基础模型一般不是很容易训练起来，面临梯度弥散、梯度爆炸的问题。\n \n如图，绿色的线条是实验的RNN的loss，它震荡不收敛。造成这个的原因在于Error Surface is rough：\n**\n如图，如果梯度下降法刚好更新到断层上，此时梯度骤升，学习率比较大就会飞出去很远，就直接没有了\n\n这就很直观的解释了原因，每次RNN都会用前一次的输出不加控制的影响下一个结果，也就是护送每次都hi把Memory的值完全变化掉。w是指数级就很容易有Gradient Vanish(explore) 的问题。\n而LSTM，\n\n\n它把Memory的值*一个值+input*一个值更新到Cell中，也就是说它的Memory和input是相加的；\n\n\n如果weight影响到记忆细胞的值，则一直会存在，除非遗忘门决定遗忘。而RNN则会一直洗掉。因此LSTM解决了梯度弥散问题。\n\n\n一般遗忘门经常要开着，bias要比较大。\n\n"},"C_Research_Knowledge/ResNet-网络分析":{"slug":"C_Research_Knowledge/ResNet-网络分析","filePath":"C_Research_Knowledge/ResNet-网络分析.md","title":"ResNet-网络分析","links":[],"tags":["CNN"],"content":"Resnet，网络层数非常深，是比较经典的CNN模型，Rennet实现了很深层次的网络的训练。本文主要讨论一下resnet的核心思想。\nUniversal approximation theorem\n万能近似定理，也称通用近似定理，指的是：一个具有两层的神经网络，即一个隐藏层、一个输出层，理论上就可以模拟任何的函数，无论多复杂的函数都可以，只要神经元数量够多。\n这个可以理解为 每个神经元都是一个线性的函数，w*x+b，但是当很多神经元组合在一起，就可以模拟出曲线，神经元越多，拟合出的曲线也就会越平滑，这里有点类似于微分，认为曲线是无线可切分的，切到最后会近似成直线，反之，无限多个线性函数自然可以拟合出很平滑的曲线。\n但是这只是理论上的，实际上如果真的只采用两层的神经网络，在可行性上就会有很大的问题，比如由于参数量过多可能根本训练不起来。\nResidual Block\n退化问题\n首先，一般认为神经网络层数越深，表达能力会越强。但是实际上，随着神经网络层数加深，却可能出现准确率降低，注意不是过拟合，称为退化问题。因此这种网络结构设计主要是解决了退化问题。而这个方法的出现，也意味着 更深的神经网络可以训练起来了。\n\n上图便是残差模块。x是输入，经过中间的隐藏层后不直接输出F(x)，而是输入x两一个分支连接到输出出，最终输出F(x)+x,即：\nH(x) = F(x) + x\n因为之前的层表现已经很好了，这个多出来的层的本来的学习目标是：让H(x)=x，也就是说尽量让输出不变，但是实际上很难学习到这一点，所以现在的目标就变成了让F(x)-&gt;0\n为什么说F(x)趋近于0更好训练呢？(借鉴别人)我认为可以从以下两方面考虑：\n\nHidden Layer 权重初始化都在0附近，输出的值自然会比较靠近0，因此可能会更好的训练；\n在该结构中，所有的激活函数都是 RELU, 而relu函数的特性是：小于等于0都置0，这就说明不管怎么样总有一半的概率可以直接置0，不用去计算\n\n\n这张图可以看出来，解决了退化问题后，34层的误差明显低于18层\n梯度弥散\n关于梯度弥散这个问题我纠结了比较久，发现问题在于我把x理解的狭隘了。从数学上理解一下这个问题 ：\nX_{l+1} = X_l + F(X_l,W_l)\nX_{l+2} = X_{l+1} + F(X_{l+1},W_{l+1}) =X_l + F(X_l,W_l) + F(X_{l+1},W_{l+1})\n故可以推出：\nX_L = X_l + \\sum_{i=1}^{L-1}F(X_i,W_i)\n此时反向传播计算，假设损失函数是C，此时反向传播要对w求导，就先对后面一个节点求导，然后链式法则再求导W，即：\n\\frac{\\partial C}{\\partial X_l} = \\frac{\\partial C}{\\partial X_L}*\\frac{\\partial X_L}{\\partial X_l} =  \\frac{\\partial C}{\\partial X_L}*(1+\\frac{\\partial}{\\partial x} \\sum_{i=1}^{L-1}F(X_i,W_i))\n实际上X_l就是一个节点，比如 wx+b，所以对w、b求导就可以求出来了。\n此时梯度里面多了一个1，而正是因为这个1的存在，使得梯度存在，可以很容易回流到千层，也就是说算出浅层 的梯度、更细参数。"},"C_Research_Knowledge/SVM支持向量机":{"slug":"C_Research_Knowledge/SVM支持向量机","filePath":"C_Research_Knowledge/SVM支持向量机.md","title":"SVM支持向量机","links":[],"tags":["SVM","Classification"],"content":"李宏毅老师视频：www.bilibili.com/video/BV14W411u78t\n知乎讲解  ：zhuanlan.zhihu.com/p/49331510\n数学完整推导视频：www.bilibili.com/video/BV12J41157qV （浙大机器学习课程）\n文中几张图引自改知乎文章。\nSupport Vector Machine\n概述\n整体上，支持向量机是一个分类算法，即寻找能分开两类数据的超平面，并且这个超平面满足距离超平面最近的数据点的到超平面距离最大。\n\n如图，H2和H3都实现了正确的分类，但是SVM会选择H3，因为两类数据中距离H3最近的点的距离比其他所有的点到H3的距离都要大，满足一开始所述的条件。\n\n目前求解目标是使得图中margin最大,经过推导得出最终的抽象的公式：\nmin_{W,b}\\frac{1}{2}||W||^2,\ns.t. y_i*(X_i^TW+B) &gt;= 1\n这个公式也就是硬匹配规则，简单解释一下：\n\nyi表示第i个数据的真实类别，取值为 1，-1；\n第i笔数据真实类别与计算出来的值同号，即保证分类准确的前提下，最大化边界。\n\n\n这些处于边界的几个数据点称为支持向量，在决定最佳超平面时只有支持向量起作用，而其他数据点并不起作用。\n推导\nSVM = Hingle_loss + Kernel_Method \nHingle Loss\nl(f(x_n),\\hat y_n) = max(0,1-\\hat y_n*f(x_n))\n其中，\n\\hat y_n = 1,-1\n当真值与预测值同号，并且乘积比比正确的好过一段距离，即乘积大于等于1时，loss为0，好过的这段距离即为margin。参考下图：\n\nLinear SVM\n\nModel:\n\nf(x) = \\sum_iw_ix_i +b =  \\begin{bmatrix}\n   w\\\\\n   b \\\\\n  \\end{bmatrix} ·\\begin{bmatrix}\n   x\\\\\n   1 \\\\\n  \\end{bmatrix}= W^Tx\n\nLoss function\n\nL(f) = \\sum_nl(f(x_n),\\hat y_n) + \\lambda||w||_2\n损失函数是一个凸函数，因此可以用梯度下降法来求解，具体看李宏毅老师视频。\n这里的SVM损失函数和一开始概述里面提到的目标函数似乎不太一样？经过如下转换，就得到了最初的形式。\n\n最后面1减去了一项，表示是软匹配，不一定能达到1的情况。\nKernel Method\n首先，要找的W*(w,b)是数据点的线性组合：\nw^* = \\sum_na^*_n*x^n\n为了解释这件事，一般就引入了拉格朗日方法来算一通，这里用另外一种思路：\n\n用梯度下降法求解时，cn(w)由于hingleloss的原因，会有一部分值为0，即不是所有的xn都会对w有影响，w初始化为0时很多数据点对更新w的值毫无用处，即a*是稀疏的。\n当a* != 0时，对应的数据点就叫做 Support vector.支持向量机名字也由此得来。\n此时w可以换一种形式写：\nw = \\sum_na_nx_n = X \\alpha\n故，Model：\nf(x) = w^Tx = \\alpha^TX^Tx =\\sum_na_n(x_n·x) = \\sum_na_nK(x_n,x)\n找一个最好的 α ，最小化损失函数：\nL(f) =\\sum_nl(f(x_n),\\hat y_n) = \\sum_nl(\\sum_{n&#039;}a_{n&#039;}K(x_{n&#039;},x_n),\\hat y_n)\n这里不需要知道具体的x，只要能求出K(x,z)的值即可。\nKernel Trick\n上述的损失函数如果直接用x，z是不好的，需要用他们的特征。用核方法就不用管他们的 特征是什么样子的。\n即两个向量转化为特征向量再求内积是比较麻烦的，直接计算两个向量的内积再求特征会更快。\n此时需要定义一个核函数，就可以简化计算。这里的\nK(x,z)\n就是一个核函数，可以选取不同的核函数。\n\n而核函数计算就是把原始数据投影到高维的一个内积，即在做分类时，先把二维数据投影到高维空间，在做一个 线性分类，上图就很明显了。\n疑问\n\nKernel Trick 是投影到高维的一个计算？\n逻辑是计算出K，就可以找出α了。而投影到高维只是计算K的一个方法，为什么可以代表svm呢?\n这些疑问以后再看。\n\n\n2020-6-7日更新：\nSVM通过数学理解更加合理！"},"C_Research_Knowledge/Softmax函数详解":{"slug":"C_Research_Knowledge/Softmax函数详解","filePath":"C_Research_Knowledge/Softmax函数详解.md","title":"Softmax函数详解","links":[],"tags":["Classification"],"content":"softmax 作用\nsoftmax 函数一般用在分类场景，尤其是多分类，它接受一组任意值作为输入，然后输出对应的一组0-1之间的数值，并且加起来为1，也就是做了归一化，实际上输出的值代表概率值。\nsoftmax 计算方式\n\n如上图，z1,z2,z3 作为输入，分别取exp, 然后加起来作为分母，分别的除以分母就得到对应的值。即zi的softmax值为：\nS_{z_i} = \\frac{\\exp(z_i)}{\\sum_{j=1}^3 \\exp(z_j)  }\n推广一下：\nS_{z_i} = \\frac{ \\exp(z_i)}{\\sum_{j} \\exp(z_j)  }\n其中，\\sum_{i} S_{z_i} = 1\n再看交叉熵\n对于二分类问题，交叉熵函数为：\nL = \\hat{y_i}*\\ln {y_i} + (1-\\hat{y_i})*\\ln{(1- y_i)}\n对于多分类问题：\nL = -\\hat{y}*\\ln(y)\n其中，\\hat{y}表示label，y表示预测的值，由softmax函数计算得到\nsoftmax求导\n上面已经知道多分类时交叉熵的公式，预测第i个时，认为它的label是1，则：\nL_i = -\\ln(y_i)\n则：\n\\frac{\\partial L_i}{\\partial i} = -\\frac{\\partial{\\ln(y_i)}}{\\partial i} = \\frac{\\partial \\ln (\\frac{e^i}{\\sum_j e^j})}{\\partial i} =··· ···=y_i-1\n具体推导如下图：\n\n\n以上便是softmax函数的相关知识了。"},"C_Research_Knowledge/Why-Deep":{"slug":"C_Research_Knowledge/Why-Deep","filePath":"C_Research_Knowledge/Why-Deep.md","title":"Why Deep","links":[],"tags":["Optimization"],"content":"相对于机器学习，深度学习为什么要 Deep 呢？\n直觉上是因为神经网络层数越多，参数就越多，拟合能力越强，表现也就更好，但是事实上并不是这样。\n要对比Deep与Shallow  ，首先要保证他们的参数两是一样的，这样对比才比较公平，如下图：\n\n更深的网络模型确实是有更好的表现，误差会更低。\n事实上更加Deep，有更多的 Hidden Layer  ，是在做模组化(Modularization) 这件事，这和模块化、函数式编程很相似，也就是说每层有具体的功能，例如：\n\n第一层的神经元是比较基础的分类器；第二层则是比较高级的…\n其中第二层以第一层的输出作为输入，第三层以第二层的输出作为输入，以此类推…\n模组化是机器自己学习出来的，模组化可以想象CNN —比较底层可以识别细节信息，高层则可以更好的识别语义信息\n\n\n做Modularization  这件事的好处就是网络可以举一反三，不再需要很多很多的data.\n有一种流行的说法：AI = BigData + DeepLearning  大数据与深度学习相结合所以可以做识别等任务(深度学习需要很大的数据支撑)，恰恰相反，深度学习只需要更少的Data，这里的**BigData ** 即使有100g，1000g 依然不能包含所有的Data，因此才需要DeepLearning来举一反三 ，如果真的有能包含所有东西的大数据那根本不需要深度学习这件事了，直接做标签分类就实现了。"},"C_Research_Knowledge/index":{"slug":"C_Research_Knowledge/index","filePath":"C_Research_Knowledge/index.md","title":"index","links":["扩散模型","C_Research_Knowledge/Mamba-模型","C_Research_Knowledge/Conformal-Prediction","C_Research_Knowledge/深度学习入门系列一-梯度下降法","C_Research_Knowledge/深度学习入门系列一-梯度下降法-②","C_Research_Knowledge/梯度下降法3","C_Research_Knowledge/再探梯度下降法之梯度","C_Research_Knowledge/深度学习入门系列4-反向传播BP算法","C_Research_Knowledge/深度学习入门系列-逻辑回归","C_Research_Knowledge/深度学习入门系列6-Convolution-Neural-Network-CNN-卷积神经网络","C_Research_Knowledge/深度学习入门系列7-Tips-For-DeepLearning-全程高能","C_Research_Knowledge/深度学习入门系列8-Tips-For-DeepLearning-2-全程高能","C_Research_Knowledge/从分类到逻辑回归","C_Research_Knowledge/高斯混合模型","C_Research_Knowledge/机器学习中判别式模型和生成式模型","C_Research_Knowledge/卷积、反卷积图像尺寸计算","C_Research_Knowledge/卷积之空洞卷积与形变卷积","C_Research_Knowledge/马尔可夫链-Markov-Chain","C_Research_Knowledge/牛顿法","C_Research_Knowledge/朴素贝叶斯分类器","C_Research_Knowledge/深度学习实践","C_Research_Knowledge/再探形变卷积","C_Research_Knowledge/最小二乘法和梯度下降法","C_Research_Knowledge/Attention方法","C_Research_Knowledge/Auto-Encoder-in-DeepLearning","C_Research_Knowledge/Batch-Normalization","C_Research_Knowledge/Classification","C_Research_Knowledge/CNN-Receptive-Field-感受野","C_Research_Knowledge/Contrastive-Loss-对比损失-分析","C_Research_Knowledge/Cross-Entropy-的前世今生","C_Research_Knowledge/GAN对抗网络初识","C_Research_Knowledge/GAN-Basic-Theory","C_Research_Knowledge/手动实现GAN网络生成动漫头像","C_Research_Knowledge/GAN-4-f-GAN推导","C_Research_Knowledge/HMM隐马尔可夫模型-到-CRF条件随机域","C_Research_Knowledge/Recurrent-Neural-Network-RNN","C_Research_Knowledge/Softmax函数详解","C_Research_Knowledge/SVM支持向量机","C_Research_Knowledge/Why-Deep","C_Research_Knowledge/ResNet-网络分析","C_Research_Knowledge/深度学习入门系列-深度学习的误差来自哪里","C_Research_Knowledge/概率论的贝叶斯公式","C_Research_Knowledge/均值与期望","C_Research_Knowledge/联合概率","C_Research_Knowledge/似然估计-Likelyhood","C_Research_Knowledge/数学中的李群初探","C_Research_Knowledge/图像仿射变换后的坐标求解","C_Research_Knowledge/图像直方图及其均衡化算法"],"tags":[],"content":"\nNew Techniques\n\n扩散模型\n**Mamba 模型\nConformal Prediction\n\n\nDeep Learning (History)\n\n深度学习入门系列一-梯度下降法\n深度学习入门系列一-梯度下降法-②\n梯度下降法3\n再探梯度下降法之梯度\n深度学习入门系列4-反向传播BP算法\n深度学习入门系列-逻辑回归\n深度学习入门系列6-Convolution-Neural-Network-CNN-卷积神经网络\n深度学习入门系列7-Tips-For-DeepLearning-全程高能\n深度学习入门系列8-Tips-For-DeepLearning-2-全程高能\n从分类到逻辑回归\n高斯混合模型\n机器学习中判别式模型和生成式模型\n卷积、反卷积图像尺寸计算\n卷积之空洞卷积与形变卷积\n马尔可夫链-Markov-Chain\n牛顿法\n朴素贝叶斯分类器\n深度学习实践\n再探形变卷积\n最小二乘法和梯度下降法\nAttention方法\nAuto-Encoder-in-DeepLearning\nBatch-Normalization\nClassification\nCNN-Receptive-Field-感受野\nContrastive-Loss-对比损失-分析\nCross-Entropy-的前世今生\nGAN对抗网络初识\nGAN-Basic-Theory\n手动实现GAN网络生成动漫头像\nGAN-4-f-GAN推导\nHMM隐马尔可夫模型-到-CRF条件随机域\nRecurrent-Neural-Network-RNN\nSoftmax函数详解\nSVM支持向量机\nWhy-Deep\nResNet-网络分析\n深度学习入门系列-深度学习的误差来自哪里\n\n\nMath\n\n概率论的贝叶斯公式\n均值与期望\n联合概率\n牛顿法\n似然估计-Likelyhood\n数学中的李群初探\n\n\nDigital Image\n\n图像仿射变换后的坐标求解\n图像直方图及其均衡化算法\n"},"C_Research_Knowledge/从分类到逻辑回归":{"slug":"C_Research_Knowledge/从分类到逻辑回归","filePath":"C_Research_Knowledge/从分类到逻辑回归.md","title":"从分类到逻辑回归","links":["C_Research_Knowledge/深度学习入门系列-逻辑回归","C_Research_Knowledge/Softmax函数详解","C_Research_Knowledge/Cross-Entropy-的前世今生"],"tags":["Classification"],"content":"关于分类，我博客里面写过很多 相关的：\n\n深度学习入门系列-逻辑回归\nSoftmax函数详解\nCross-Entropy-的前世今生\n\n但是这些并没有串起来，所以一直是零散的，这里来串一下彻底理解分类相关的东西。\n\n​\t一句题外话，开始学习的时候我一直说自己是做回归的不做分类，所以不去学。但是现在看来，作为一个研究CV方向的人，学习分类原理等等完全都是基本素养，需要用心学习。\n\n二分类\n首先在分类中，有概率判别式模型和概率生成式模型，现在用朴素贝叶斯来做二分类(属于概率生成式模型)。\n朴素贝叶斯分类器\n\n模型建立\n\nP(C_1|X) = \\frac{P(C_1)*P(X|C_1)}{P(X)} = \\frac{P(C_1)*P(X|C_1)}{\\sum _{i=1}^2P(C_i)*P(X|C_i)}\n其中，X为观测到的样本，求x属于C1的概率\n\n假设C1服从高斯分布，那就是求出均值、方差 这两个参数使得P(x|C_1)最大化，也就是最大似然估计：\n\nP(X|C_1) = \\prod_{k=1}^n P(x_k|C_1)\n\n选择最好的函数\n\n二元高斯分布下的贝叶斯分类器转换为回归问题\n具体推导文章开头第一篇文章有讲，这里只给结论：\n通过推导可以看出，分类问题和回归问题是统一的，我们不必再通过计算μ1μ1,μ2μ2,Σ1Σ1,Σ2Σ2,N1N1,N2N2来计算概率，而可以通过DeepLearning的手段直接计算w和b，从而求得概率，而与线性回归不同的是，我们需要将输出结果用σ()函数转化到0~1之间，这就是逻辑回归。\n多分类\nNote: 引自blog ：写的很棒！\nsoftmax文章开始也有提到，是在深度学习中常用的多分类函数。\n多分类计算有两种方式：\n\nK个独立的二元分类器——用于不互斥的分类\nsoftmax——用于互斥的分类\n\nsoftmax VS k个二元分类器\n如果你在开发一个音乐分类的应用，需要对k种类型的音乐进行识别，那么是选择使用 softmax 分类器呢，还是使用 logistic 回归算法建立 k 个独立的二元分类器呢？ 这一选择取决于你的类别之间是否互斥，例如，如果你有四个类别的音乐，分别为：古典音乐、乡村音乐、摇滚乐和爵士乐，那么你可以假设每个训练样本只会被打上一个标签（即：一首歌只能属于这四种音乐类型的其中一种），此时你应该使用类别数 k = 4 的softmax回归。（如果在你的数据集中，有的歌曲不属于以上四类的其中任何一类，那么你可以添加一个“其他类”，并将类别数 k 设为5。） 如果你的四个类别如下：人声音乐、舞曲、影视原声、流行歌曲，那么这些类别之间并不是互斥的。例如：一首歌曲可以来源于影视原声，同时也包含人声 。这种情况下，使用4个二分类的 logistic 回归分类器更为合适。这样，对于每个新的音乐作品 ，我们的算法可以分别判断它是否属于各个类别。 现在我们来看一个计算视觉领域的例子，你的任务是将图像分到三个不同类别中。(i) 假设这三个类别分别是：室内场景、户外城区场景、户外荒野场景。你会使用sofmax回归还是 3个logistic 回归分类器呢？ (ii) 现在假设这三个类别分别是室内场景、黑白图片、包含人物的图片，你又会选择 softmax 回归还是多个 logistic 回归分类器呢？ 在第一个例子中，三个类别是互斥的，因此更适于选择softmax回归分类器 。而在第二个例子中，建立三个独立的 logistic回归分类器更加合适。\n交叉熵\n在交叉熵一文中举的例子就用的逻辑回归，解释了为什么逻辑回归用交叉熵当损失函数，至此，一切都顺理成章了。\nCross-Entropy-的前世今生"},"C_Research_Knowledge/似然估计-Likelyhood":{"slug":"C_Research_Knowledge/似然估计-Likelyhood","filePath":"C_Research_Knowledge/似然估计-Likelyhood.md","title":"似然估计-Likelyhood","links":[],"tags":["概率论"],"content":"参考：zhuanlan.zhihu.com/p/36824006\n概率 (Probability) &amp; 似然 (Likelyhood)\n概率就是随机事件发生的可能性的度量，根据实验或者已知的模型来计算某种可能性，可以称之为概率。\n似然则是已经知道数据结果分布，由结果来推测模型的参数，这个过程就是似然。\n所以这两个过程大概上是相反的。似然更通俗的说就是给定样本X = x下参数 \\theta = \\theta_1相对于参数取另外的值\\theta = \\theta_2为真实值的可能性。\n似然函数\n设模型参数为θ，则：\nL(\\theta|x) = P(X=x|\\theta)\n也就是说估计θ值使得实验结果X=x。\n对于某一实验，我们可能包含多种情况，其中每个实验结果的概率可记为一个集合 ：\nP = \\{p_1,p_2,p_3,...p_n\\} \\  (\\sum_{i=1}^np_i = 1)\n假设做了m次实验，则实验结果出现概率为：\nE = \\prod_{l=2}^mp_k,\\space p_k \\in P\n极大似然估计\n似然函数L最大化时就是极大似然估计。极大似然估计因为是连乘，所以一般是取log之后变为求和，然后再去求最大值，具体可以参考文章开始提到的链接中的抓豆子实验，很形象。\n思考\n在深度学习中，用神经网络作分类问题实际上也就是建立了一个模型，这个模型的参数(w)就相当于似然估计中的参数θ，用大量的数据去学习，去调整w，也就是已经知道结果，由结果去估计参数(w),使得取参数w后可以更准确的估计真实数据概率。也就是在做最大似然估计，而最大似然估计已经证明就是最小化交叉熵。\n所以极大似然估计就是根据经验来推断规律。"},"C_Research_Knowledge/再探形变卷积":{"slug":"C_Research_Knowledge/再探形变卷积","filePath":"C_Research_Knowledge/再探形变卷积.md","title":"再探形变卷积","links":["C_Research_Knowledge/卷积之空洞卷积与形变卷积"],"tags":["CNN"],"content":"前一篇博文写过关于形变卷积的部分 卷积之空洞卷积与形变卷积，但是写的比较粗略，故再来分析、整理一下形变卷积相关知识。\n\n\n因为网络最终用的形变卷积，所以是非看不可。\n好读书，不求甚解。 不愧是我\n\n\n初衷\n适应几何变换\n传统卷积卷积核是正方形，扫描不规则图形也只能用方形区域，而实际任务中，很多东西都不是规则的，例如生物、汽车等。传统卷积核无法适应比较大的变换，故提出了可以变形以适应几何变换的形变卷积。\n\n卷积过程计算\n\n传统卷积计算\n\ny(p_0) = \\sum_{p_n \\in R} w(p_n) * x(p_0 + p_n)\n其中，p0表示输出特征图的一个点；pn则是卷积核的坐标。\n\n形变卷积计算\n\ny(p_0) = \\sum_{p_n \\in R} w(p_n) * x(p_0 + p_n + \\Delta p_n)\n其中。最后加了一项delta 表示偏移量， 取值从1-N，N是卷积核尺寸。\n\n这里P0表示输出特征图的点，之所以pn要加p0，因为是3X3卷积核，原始图片9个像素对应新特征图一个像素。\n2020-8-19更新\n\n\n采样\n\n此时采样的点加了一项delta, 可能是小数，因此需要用插值算法来确定采样位置，此处应用的是双线性插值来确定采样的位置，如下：\np = p_0 + p_n + \\Delta p_n\nx(p) = \\sum_q G(q,p)*x(q)\n大致意思是 ：先枚举输入特征图x所有的点q，用插值核G做个运算再乘以x(q).\n\n总之这一步就是确定位置。之前看过有关书上讲解双线性采样的问题，可以用公式直接算出来，这边看的不是很明白，但是没那么重要。\n从形变卷积公式可以看出，需要学习的参数有两个：卷积核的weight，偏移量Delta, 偏移量显得至关重要\n\n\n反向传播学习\n\n\\frac{\\partial y(p_0)}{\\partial \\Delta p_n} = \\sum_{p_n \\in R}w(p_n)*\n\\frac{\\partial x(p_0 + p_n + \\Delta p_n)}{\\partial \\Delta p_n}\n= \\sum_{p_n \\in R} \\left[w(p_n) * \\sum_q\\frac{\\partial G(q,p_0+p_n+\\Delta p_n)}{\\partial \\Delta p_n} * x(q)\\right]\n\n偏移量梯度即在上述计算公式中有偏移量这一项的都计算微分。\n\n\n输出特征图尺寸计算\n形变卷积与普通卷积无异，输出特征图尺寸计算需要先知道卷积核大小，再根据公式来计算。卷积核大小与是否使用空洞有关系。\n\n空洞卷积卷积核尺寸计算\n\nK = k+(k-1)*(r-1)\n\nr 为空洞率， r=1时卷积核尺寸不变，故Pytorch nn.Conv2D 默认 dilated=1.\n这里吐槽一下，看了三四个人写的博客，都说dlitated=1表示注入一个空洞，这是错的。不可信！\n\n\n特征图尺寸计算\n\nW = \\frac{w - K +2*P}{S} +1 = \\frac{w - [k+(K-1)*(r-1)] + 2P}{S} +1\nH = \\frac{h - K +2*P}{S} +1 = \\frac{h - [k+(K-1)*(r-1)] + 2P}{S} +1\n\n把空洞卷积卷积核带进去即可。\n\n\n关于卷积\n理论上，卷积核感受野随着卷积层的增加而线性增加；\n事实上，有效的感受野是呈高斯分布，即一个卷积核中心的区域的比较有效。\n因此实际中，有效的感受野随着卷积层的增加呈平方根增加，是远远不够的。空洞卷积恰好处理这个问题。\n2020.7.13更新！\nFRY\n\n2020-8-19 第二次更新\nDeformable Convolution V2\n可形变卷积V2版本，给原始每个偏移的采样点加入了权重，即：\ny(p) = \\sum_{k=1}^K w_k \\times x(p + p_k + \\Delta p_k)\\times\\Delta m_k\n​\tK表示采样位置；最后多了一项每个采样像素权重的控制。\n有个地方需要注意：\n\n\nV1和V2版本，Offsets 输出通道数均为2K, 因为每个点偏移需要x，y两个分量来表示，故用2个通道\n\n\nV2版本中输出通道数为K ，每个点一个权重，故一共K个权重\n\n"},"C_Research_Knowledge/再探梯度下降法之梯度":{"slug":"C_Research_Knowledge/再探梯度下降法之梯度","filePath":"C_Research_Knowledge/再探梯度下降法之梯度.md","title":"再探梯度下降法之梯度","links":[],"tags":["Optimization"],"content":"这里记录一下刚开始学比较疑惑的一个问题，就是梯度。\n推荐一篇文章：www.zhihu.com/question/36301367/answer/142096153\nGradient\n梯度的来由这里就不细讲了，可以参考上述知乎文章。\n首先梯度是一个向量，那么就有方向。梯度的方向就是函数变化最快的方向。\n函数变化最快的方向直观理解，就是随着自变量变化函数值变化最快。即：\nw_new = w + w_deta，此时函数值可能变大也可能变小，但是变化值不确定，当 w = w+g时，函数值就会增加最快。\n\n"},"C_Research_Knowledge/卷积、反卷积图像尺寸计算":{"slug":"C_Research_Knowledge/卷积、反卷积图像尺寸计算","filePath":"C_Research_Knowledge/卷积、反卷积图像尺寸计算.md","title":"卷积、反卷积图像尺寸计算","links":[],"tags":["CNN"],"content":"一般在设计深度网络架构时，神经网络中无非也就是卷积层和全连接层，而网络一般会对图像尺寸有限制，卷积改变图像的尺寸比较重要，需要会计算。因此来计算一下卷积和反卷积的尺寸计算。以pytorch为例.\nConvolution\ntorch.nn.Conv2D(in_channels,out_channels,kernel_size,stride,padding,bias)\n#以上几个参数比较常用\n卷积操作后图像计算尺寸为：\nW_{out} = \\frac {W_{input} - W_{filter} + 2*padding}{stride + 1}\nH_{out} = \\frac {H_{input} - H_{filter} + 2*padding}{stride + 1}\n\n一般情况下，均为正方形，即W = H\n如果计算结果不是正数，则向下取整\n\nDeconvolution\ntorch.nn.ConvTranspose2D(in_channels, out_channels, kernel_size, stride, padding,  output_padding，bias)\n# 其中，padding是输入图片补0个数，output_padding是输出图像补0个数\n# 直观上是扩大图片，边上填充0也可以用来扩大图像\n反卷积图像尺寸计算公式：\nW_{out} = (W_{input} - 1)*stride + Padding_{output} - 2*Padding_{input} + W_{Kernelsize}\nH_{out} = (H_{input} - 1)*stride + Padding_{output} - 2*Padding_{input} + H_{Kernelsize}\n其中，\n\nPadding_{output} 指的是ConvTranspose2D这个函数中的output_padding 参数；\nPadding_{input} 指的是上述函数的padding 参数\n"},"C_Research_Knowledge/卷积之空洞卷积与形变卷积":{"slug":"C_Research_Knowledge/卷积之空洞卷积与形变卷积","filePath":"C_Research_Knowledge/卷积之空洞卷积与形变卷积.md","title":"卷积之空洞卷积与形变卷积","links":[],"tags":["CNN"],"content":"最近看了空洞卷积核形变卷积，这里大致记录一下。\nConvolution  Filter\n普通卷积中，卷积核是方形的，例如 3X3，在图片上扫描得到特征图。个人认为大小卷积核，3*3，9X9区别在于：\n\n\n感受野变化\n\n\n9*9 感受野明显是非常大的，对应的提取到的特征更加的整体性；\n\n\n3*3 卷积是可以堆叠形成9X9的感受野，但是更多的卷积层堆叠，提取到的特征也就会更加的细腻\n\n\n\n对于姿态估计任务来说，看到的更多，人更整体似乎会更好\n\n\n\n计算量的变化\n3*3卷积核参数为9，9X9参数为81，参数翻了9倍，计算量大，需要更多的显存\n\n\nDilated Convolutions\n参考：blog.csdn.net/qq_36338754/article/details/96969208\n初步认识\n空洞卷积，也称扩张卷积、膨胀卷积，可以在不增加参数量的情况下起到大卷积核的效果。最早是用于语义分割的，\n顾名思义，在卷积核中注入空洞，扩大了感受野，\n\n几个要点\n\ndilated设置为2时，3*3卷积核感受野为7X7, 设置为4，感受野为9X9\nNote：\n\n\n红点表示感受野中心\n\n\n外围图像表示感受野\n\n\n扩张卷积不会影响图像尺寸，卷积之后图像尺寸计算依然是：\nw_{new} = \\frac{W-F+2*P}{S} + 1\n\n\nPytorch 中空洞卷积实现\nimport torch.nn as nn\nconv1 = nn.Conv2d(1, 1, 3, stride=1, bias=False, dilation=1)  # 普通卷积\nconv2 = nn.Conv2d(1, 1, 3, stride=1, bias=False, dilation=2)  # dilation就是空洞率，即间隔\nDeform Convolution\n形变卷积顾名思义，卷积核不再一定是方形了，在原来方形的基础上增加了一个offest 偏移量，于是可以发生形变，如下图：\n\n卷积核可以任意的变形。\n\n偏移量是通过一个普通的卷积层计算得到。\n\n\n看以看出随着训练，形变卷积核采样位置的变化，在形成一个个形状，而不在是方形了。\n\n多尺度\n\n借鉴于别人---在深度学习很多计算机视觉任务中，都用到了多尺度，这是在模仿人眼观察物体，\n\n\n近大远小\n\n\n远景完整，但是模糊\n\n\n近景细节丰富，但是不完整\n\n\n\n\n以上是对空对卷积、形变卷积的初步理解，后续有改进、错误会及时更新"},"C_Research_Knowledge/图像仿射变换后的坐标求解":{"slug":"C_Research_Knowledge/图像仿射变换后的坐标求解","filePath":"C_Research_Knowledge/图像仿射变换后的坐标求解.md","title":"图像仿射变换后的坐标求解","links":[],"tags":["仿射变换"],"content":"我是做人体姿态估计的方向，在姿态估计中有必不可少的 一步，就是坐标变换。最直观的从原始不规则图像变换到45*45（或者其他）尺寸的heatmap，原始图像的人体关键点的坐标就需要做相应的变换才可以生成正确的新的坐标，从而生成正确的heatmap。\n而这一问题看似很容易，却是在一直困扰着我，我在做的项目里这一问题一直没彻底解决，而今天经过反复测试找到一个比较合适的实现方案，实现由原图变换到heatmap，heatmap解出原坐标。下面进入正文。\n简单的坐标变换\n首先，图像直接resize变换，是可以直接用坐标除以比例得到新的坐标。注意直接用cv2.resize()图像可能会发生形变，如下图：\n\n这是我1280*720图像直接resize到 368*368的结果，红点是在原始图像画上去的，resize后到了上图中，上图中绿色的圈圈(下面的)则是经过简单的坐标计算：\nratio_{width} = width / 368\\\\\nratio_{height} = height / 368\\\\\nx&#039; = x / ratio_{width} \\\\\ny&#039; = y / ratio_{height}\n仿射变换\n仿射变换（Affine Transformation或 Affine Map）是二维情况下坐标之间的变换。它保持了二维图形的“平直性”（即：直线经过变换之后依然是直线）和“平行性”（即：二维图形之间的相对位置关系保持不变，平行线依然是平行线，且直线上点的位置顺序不变）。\n数学上，仿射变换就是用2*3的变换矩阵来计算。\n仿射变换可以写为如下的形式：(x,y)为原始坐标 （x&#039;,y&#039;）为变换后的坐标\n\\left\\{\n\\begin{aligned}\nx&#039;&amp; = ax + by + m\\\\\ny&#039;&amp; = cx + dy + n\n\\end{aligned}\n\\right.\n矩阵计算比较常用，可以写为：\n\\begin{bmatrix}\n{u}\\\\\n{v}\\\\\n\\end{bmatrix} \n= \n\\begin{bmatrix}\n{a_2}&amp;{a_1}&amp;{a_0}\\\\\n{b_2}&amp;{b_1}&amp;{b_0}\\\\\n\\end{bmatrix} *\n\n\\begin{bmatrix}\n{x}\\\\\n{y}\\\\\n{1}\n\\end{bmatrix}\n也就是简单的新的坐标 = 变换矩阵 * 原始坐标。而在姿态估计中，GT坐标都是三维，第三维为0胡总和1.1表示坐标正确，0表示不可信。因此很容易用仿射变换来求解变换后的坐标。\n仿射变换是一系列原子操作组合而成，平移、缩放、旋转、反转、错切 。这些操作组合可以完成各种变换。\n姿态估计中的坐标变换\n姿态估计中自上而下的方法，需要先从原图把每个人切出来，然后切出来的图片一般要变换成正方形，然后再缩放到对应的heatmap尺寸，一般是(64*64)。所以上面提到的直接求比例是行不通的。\n如果用常规方法去做，会非常麻烦，因为不只是需要heatmap的时候进行变换，再最后预测出heatmap后，依然需要求出坐标而且变换到原图。我之前都是直接做，所以一直出错。而如果借助仿射变换，这件事就很容易了。我借助的别人一个函数，来求仿射矩阵，变换图像；同时由heatmap反求坐标变换到原始图像。代码在我的工具类可以看到。下面看一下我的测试结果：\n\n&lt;1&gt; 原始图像（忽略上面的蓝色点，是我刚开始求出的错误的）：\n\n&lt;2&gt; 裁剪后的图像以及坐标变换：\n\n可以看出计算出的坐标是正确的。\n&lt;3&gt; 裁剪后的图像关键点生成heatmap再反变换到原始图像：\n\n可以看出是正确的坐标了。\n踩坑\n在上面&lt;2&gt;中，手动求坐标就是一开始提到公式，这里有两个坑：\n\n新的坐标 = 变换矩阵 * 原始坐标  , 我之前这一步错在用原始坐标 去乘 变换矩阵，不懂原理嘛。\n这里的乘法是正式的矩阵乘法，用numpy实现就是：np.dot(M,a) , 这样算出来的就是二维坐标，即 (x,y),我之前这里也错了，如果直接用 M * a就是对应位置相乘，得到的是 2*3形状的矩阵。\n还有就是以前求不对变换矩阵，后面需要就直接调用即可\n\n new_coordinates = np.dot(trans_final, coordinates)\n有个心得，就是在做姿态估计准备训练模型前，先试几组坐标变换看对不对在做后面的事。\n至此，困扰很久的坐标问题终于得到解决！"},"C_Research_Knowledge/图像直方图及其均衡化算法":{"slug":"C_Research_Knowledge/图像直方图及其均衡化算法","filePath":"C_Research_Knowledge/图像直方图及其均衡化算法.md","title":"图像直方图及其均衡化算法","links":[],"tags":["直方图"],"content":"直方图 Histogram\n每张图像都对应有直方图，再Ps软件、单反都可以轻易地看到图像的直方图，直方图则可以看出图像得风格。\n图像的直方图代表了不同灰度级的像素出现的次数。\n首先看一下灰度级是什么意思？\n\n这张图就清晰的展示了灰度级的概念，通俗来说就是明亮的程度。\n然后看一下直方图长什么样子？\n\n这里面对三个颜色通道都画了出来，横坐标代表灰度级，纵坐标代表数量，数量越多图像就越高。\n如何理解直方图？\n\n如图，再PS的Camera滤镜中把灰度级做了一个区分，它把直方图分成了五类，从左到右分别是：黑色、阴影、曝光、高光、白色，鼠标悬浮即会高亮一片区域并且显示该区域代表图像的什么部分。所以就很容易理解直方图了，调整图像的高光等都会使得直方图放生改变。\n这里有一个误区是不能把从左到右直方图对应到图像从左到右。因为图像的阴影、高光区域位置都很不规则。如果图像真的从左到右与直方图对应，那么就变成了第一张图这种形式了。\n代码实现\n# ------------------------------------------------------------------------------\n# # @Time    : 2020/3/22 上午 11:33\n# # @Author  : fry\n# @FileName: 6_2_his_rgb.py\n# ------------------------------------------------------------------------------\nimport cv2\nimport matplotlib.pyplot as plt\n \ndef image_hist(image): #画三通道图像的直方图\n   color = (&quot;blue&quot;, &quot;green&quot;, &quot;red&quot;)#画笔颜色的值可以为大写或小写或只写首字母或大小写混合\n   for i, color in enumerate(color):\n       # 计算直方图函数\n       # src-img; channel; mask; bin多少个灰度级; range[0-256],像素值范围； 除了mask其他都要 []\n       hist = cv2.calcHist([image], [i], None, [256], [0, 256])\n       plt.plot(hist, color=color)\n       plt.xlim([0, 256])\n   plt.show()\n \nimage = cv2.imread(&#039;./img/5.jpg&#039;, 1)\ncv2.namedWindow(&quot;img&quot;,cv2.WINDOW_NORMAL)\ncv2.imshow(&#039;img&#039;, image)\nimage_hist(image)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n \n三通道直方图：\n"},"C_Research_Knowledge/均值与期望":{"slug":"C_Research_Knowledge/均值与期望","filePath":"C_Research_Knowledge/均值与期望.md","title":"均值与期望","links":[],"tags":["概率论"],"content":"在深度学习经常会听到期望这个词，而且总觉得与均值差不多，在这里记录一下做一个区分。\n\n平均数是一个统计学概念\n期望则是一个概率论概念\n\n平均数是根据实验结果统计的样本的平均值，期望则是实验前根据样本概率分布预测样本的平均值。\n之所以说“预测”是因为在实验前能得到的期望与实际实验得到的样本的平均数总会不可避免地存在偏差，毕竟随机实验的结果永远充满着不确定性。如果我们能进行无穷次随机实验并计算出其样本的平均数的话，那么这个平均数其实就是期望。当然实际上根本不可能进行无穷次实验，但是实验样本的平均数会随着实验样本的增多越来越接近期望，就像频率随着实验样本的增多会越来越接近概率一样。\n如果说概率是频率随样本趋于无穷的极限，那么期望就是平均数随样本趋于无穷的极限。"},"C_Research_Knowledge/手动实现GAN网络生成动漫头像":{"slug":"C_Research_Knowledge/手动实现GAN网络生成动漫头像","filePath":"C_Research_Knowledge/手动实现GAN网络生成动漫头像.md","title":"GAN3_手动实现网络生成动漫头像","links":["C_Research_Knowledge/GAN对抗网络初识","C_Research_Knowledge/GAN-Basic-Theory","C_Research_Knowledge/手动实现GAN网络生成动漫头像","C_Research_Knowledge/GAN-4-f-GAN推导"],"tags":["GAN","Code"],"content":"*GAN系列知识\nGAN对抗网络初识\nGAN-Basic-Theory\n手动实现GAN网络生成动漫头像\nGAN-4-f-GAN推导\n学习了GAN的基本原理后，手动实现一个相当于GAN的 hello，world , 具体的工程见：\ngithub.com/tzwx/DeepLearning\n这里主要分析一下代码逻辑，贴上训练代码。\n写代码的时候发现自己很生疏了，所以总结一下流程\n先上图：训练了69个epoch的结果，设定的时2500epoch\n\n流程\n定义网络结构\n\n判别器就是几个卷积层（Conv）\n生成器则是几个反卷积层（_deConv），最后接一个sigma函数分类\n\n训练流程\ntrans = transforms.Compose(\n    [\n        transforms.ToTensor(),\n        transforms.Normalize((.5, .5, .5), (.5, .5, .5))\n    ]\n)\n数据处理\n\n数据预处理，数据要先经过以上代码\n定义label，用1表示realimg，0表示fakeimg，维度就是BATCH_SIZE\n获取数据集，循环把100张图作为一个Batch，然后开始训练流程。\n\nTraining\n\n定义损失函数 BCELoss ，也就是二值的交叉熵\n定义优化器Adam\n定义学习率\n优化器梯度清0\n求得数据输出\n用loss function计算损失loss\nloss 反向传播\n更新参数\n\n这个训练流程也是分类的流程。\n思考\n在训练过程中，判别器loss一直是比较小的(0.1,0.01,…)，而生成器loss很大，(7-11不等)，同时是一个互相博弈的过程，判别器loss减小时，生成器loss增加，反之亦然。\n因为之训练了69个epoch，所以看不到最终的比较优化的结果，最后的结果应该是判别器的loss很大，生成器的loss很小才算训练结束。\n代码\n# @Time    : 2020/2/29 17:31\n# @Author  : FRY--\n# @FileName: train.py\n# @Software: PyCharm\n# @Blog    ：fryddup.github.io\n \nfrom data_loader import  get_img\nfrom net.net import Generator\nfrom net.net import Discriminator\nimport torch\nimport torch.nn as nn\nimport cv2\nimport numpy as np\nimport torchvision.transforms as transforms\n \nBATCH_SIZE = 100\nG_LEARNING_RATE = 0.0001\nD_LEARNING_RATE = 0.0001\nEPOCHS = 2500\n \ntrans = transforms.Compose(\n    [\n        transforms.ToTensor(),\n        transforms.Normalize((.5, .5, .5), (.5, .5, .5))\n    ]\n)\n# Instance\ng = Generator().cuda()\nd = Discriminator().cuda()\n \n# define loss\ng_loss = nn.BCELoss() # Binary crossEntropy\nd_loss = nn.BCELoss()\n \n# define optimizer\ng_optimizer = torch.optim.Adam(g.parameters(),lr=G_LEARNING_RATE)\nd_optimizer = torch.optim.Adam(d.parameters(),lr=D_LEARNING_RATE)\n \n# define labels\nlabel_real = torch.ones(BATCH_SIZE).cuda()\nlabel_fake = torch.zeros(BATCH_SIZE).cuda()\n \n#all images\nimages_truth = get_img()\ndata_length = len(images_truth)\n \ndef train():\n    for epoch in range(EPOCHS):\n        # shuffle\n        np.random.shuffle(images_truth)\n        images_real_loader = []\n        count = 0\n        for index in range(data_length):\n            count = count + 1\n            images_real_loader.append(trans(images_truth[index]).numpy())\n            # images_real_loader[100,3,96,96]\n            if count == BATCH_SIZE:\n                count = 0 # reset\n \n                # Train Discriminator\n                # train real data to gpu\n                # if real -&gt; d(real_img) = 1\n                images_real_loader_tensor = torch.Tensor(images_real_loader)\n                images_real_loader_tensor = images_real_loader_tensor.permute(0,3,1,2).cuda()\n                images_real_loader.clear()\n                # graddient _ zero\n                d_optimizer.zero_grad()\n                # real image output_66\n                realimage_d = d(images_real_loader_tensor).squeeze() # descent  [100，1，1，1] -&gt; [100,1]\n                # loss\n                d_realimg_loss = d_loss(realimage_d, label_real)\n                # loss backward\n                d_realimg_loss.backward()\n \n                # train generate data\n                # if generator d(generate_img) = 0\n                images_fake_loader = torch.randn(BATCH_SIZE, 100, 1, 1).cuda()\n                # detach() g no gradient -&gt; fix generator\n                images_fake_loader_tensor = g(images_fake_loader).detach()\n                fakeimg_d = d(images_fake_loader_tensor).squeeze()\n                d_fakeimg_loss = d_loss(fakeimg_d, label_fake)\n                d_fakeimg_loss.backward()\n                d_optimizer.step()\n                \n \n                # Train Generator\n                fake_data = torch.randn(BATCH_SIZE, 100, 1, 1).cuda()\n                g_optimizer.zero_grad()\n                generator_images = g(fake_data)\n                generator_images_score = d(generator_images).squeeze()\n                gen_loss = g_loss(generator_images_score, label_real)\n                gen_loss.backward()\n                g_optimizer.step()\n \n                print(&quot;Current epoch:%d, Iteration: %d, Discriminator Loss: %f, Generator Loss: %f&quot;\n                      % (epoch, (index//BATCH_SIZE)+1,\n                         (d_realimg_loss+d_fakeimg_loss).detach().cpu().numpy(),\n                         gen_loss.detach().cpu().numpy()))\n \n                torch.save(g, &quot;pkl&quot;+str(epoch)+&quot;generator.pkl&quot;)\n \nif __name__ == &quot;__main__&quot;:\n    train()"},"C_Research_Knowledge/扩散模型":{"slug":"C_Research_Knowledge/扩散模型","filePath":"C_Research_Knowledge/扩散模型.md","title":"扩散模型","links":[],"tags":["Diffusion-Model"],"content":"原始的扩散模型里面变量繁多，需要稍微记录一下整个过程中出现的公式、变量等，方便对照代码理解。\n一些资料：\n【Diffusion模型】由浅入深了解Diffusion，不仅仅是震撼，感受它带给我们的无限可能！！（超详细的保姆级入门教程）_哔哩哔哩_bilibili\nWhat are Diffusion Models?\n前向过程\n主要计算：\nx_t = \\sqrt{\\alpha_t}x_{t-1} + \\sqrt{1-\\alpha_t}z_1\n其中，\n\\alpha_t = 1 - \\beta_t\n\\beta_t \\in (0.0001, 0.002)进行均匀采样，递增,\\alpha_t显然呈递减： 即图像的比重越来越小，噪声比重越来愈大\n推广一下，任意的 x_t 从 x_0 的计算过程为：\nx_t = \\sqrt{\\bar{\\alpha_t}}x_0 + \\sqrt{1-\\bar{\\alpha_t}}z_t\n其中，\\bar{\\alpha}_t = \\prod_{i=1}^t \\alpha_i, z_t 为t时刻采样的高斯噪声\n以上从概率论的视角看[暂时不用理会]：\n\n逆向过程\n根据x_t求解x_{t-1}:\n主目标\nq(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t, \\mathbf{x}_0) = q(\\mathbf{x}_t \\vert \\mathbf{x}_{t-1}, \\mathbf{x}_0) \\frac{ q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_0) }{ q(\\mathbf{x}_t \\vert \\mathbf{x}_0) }\nq(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t) = q(\\mathbf{x}_t \\vert \\mathbf{x}_{t-1}, \\mathbf{x}_0) \\frac{ q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_0) }{ q(\\mathbf{x}_t \\vert \\mathbf{x}_0) }\n简化，此目标正比于：\n\\propto\\exp \\Big( -\\frac{1}{2} \\big( \\color{red}{(\\frac{\\alpha_t}{\\beta_t} + \\frac{1}{1 - \\bar{\\alpha}_{t-1}})} \\mathbf{x}_{t-1}^2 - \\color{blue}{(\\frac{2\\sqrt{\\alpha_t}}{\\beta_t} \\mathbf{x}_t + \\frac{2\\sqrt{\\bar{\\alpha}_{t-1}}}{1 - \\bar{\\alpha}_{t-1}} \\mathbf{x}_0)} \\mathbf{x}_{t-1} \\color{black}{ + C(\\mathbf{x}_t, \\mathbf{x}_0) \\big) \\Big)}\n与标准的高斯分布进行对比：\n\\frac{1}{\\sqrt{2\\pi\\sigma}}\\exp\\Big(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\Big) \\propto \\exp\\Bigg( -\\frac{1}{2}\\bigg(\\frac{1}{\\sigma^2}x^2 - \\frac{2\\mu}{\\sigma^2}x +\\frac{\\mu^2}{\\sigma^2}\\bigg)\\Bigg)\n看出方差出现在平方之前即红色部分，均值则在蓝色部分出现，可以对均值进行一次计算，约分得到：\n\\tilde{\\boldsymbol{\\mu}}_t = \\color{cyan}{\\frac{1}{\\sqrt{\\alpha_t}} \\Big( \\mathbf{x}_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\boldsymbol{z}_t \\Big)}\n此时得到均值和方差，然后训练过程就是用神经网络估计这边的噪声z_t.\n以后就是关键公式了。\n\\begin{aligned}\\tilde{\\beta}_t = \\color{green}{\\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} \\cdot \\beta_t} \\\\\\tilde{\\boldsymbol{\\mu}}_t (\\mathbf{x}_t, \\mathbf{x}_0)&amp;= (\\frac{\\sqrt{\\alpha_t}}{\\beta_t} \\mathbf{x}_t + \\frac{\\sqrt{\\bar{\\alpha}_{t-1} }}{1 - \\bar{\\alpha}_{t-1}} \\mathbf{x}_0)/(\\frac{\\alpha_t}{\\beta_t} + \\frac{1}{1 - \\bar{\\alpha}_{t-1}}) \\\\&amp;= (\\frac{\\sqrt{\\alpha_t}}{\\beta_t} \\mathbf{x}_t + \\frac{\\sqrt{\\bar{\\alpha}_{t-1} }}{1 - \\bar{\\alpha}_{t-1}} \\mathbf{x}_0) \\color{green}{\\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} \\cdot \\beta_t} \\\\&amp;= \\frac{\\sqrt{\\alpha_t}(1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_t} \\mathbf{x}_t + \\frac{\\sqrt{\\bar{\\alpha}_{t-1}}\\beta_t}{1 - \\bar{\\alpha}_t} \\mathbf{x}_0\\\\\\end{aligned}\n\n小名同学推荐 B 站李宏毅课程，以后有机会也学习学习\n扩散模型 - Diffusion Model【李宏毅2023】"},"C_Research_Knowledge/数学中的李群初探":{"slug":"C_Research_Knowledge/数学中的李群初探","filePath":"C_Research_Knowledge/数学中的李群初探.md","title":"数学中的李群初探","links":[],"tags":["李群"],"content":"李群\n直观上的李群\n首先李群也是一种降维算法。降维就是为了使用更低维的向量最大程度上表示原来的特征，在机器学习中有很多降维算法：\n\n\nPCA(Principal Component Analysis)算法处理的是方形的矩阵(n*n)；\n\n\n奇异值分解 SVD (Singular Value Decomposition) 可以处理任意形状的矩阵(m*n)；\n\n\n李群则可以处理张量,对张量进行降维度。\n\n\n李群计算流程\n李群运动链模型\n\n这里以老鼠为例，假设老鼠身上一共有四根骨头，AB,BC,CD,DE，以每根骨头自身作为x轴，单独建立每根骨头的三维坐标系，C的坐标在AB骨头坐标系表示就是：\n\\begin{bmatrix}\n{q^*}\\\\\n{1}\n\\end{bmatrix}\n\\begin{bmatrix}\n{R}&amp;{t}\\\\\n{0}&amp;{1}\\\\\n\\end{bmatrix} *\n\\begin{bmatrix}\n{q}\\\\\n{1}\n\\end{bmatrix}\n其中，\n\n\nq是C在BC三维坐标系下的，形状是**3*1**；\n\n\nq^是C在AB坐标系下的坐标，形状是   3*1\n\n\nR是旋转矩阵，形状是   3*3\n\n\nt是平移矩阵，形状是3*1\n\n\n中间的矩阵就是变换矩阵，整体维度就是 4*1 = 4*4 * 4*1\n\n\n更一般的坐标变换：\n\\begin{bmatrix}\n{x^*}\\\\\n{1}\n\\end{bmatrix}\n\\begin{bmatrix}\n{R_i}&amp;{t_i}\\\\\n{0}&amp;{1}\\\\\n\\end{bmatrix} *\n\\begin{bmatrix}\n{x}\\\\\n{1}\n\\end{bmatrix}\n=\ng_i*\n\\begin{bmatrix}\n{x}\\\\\n{1}\n\\end{bmatrix}\ngi就是上图中的g1，g2…\n姿态变换：\n两种姿态都可以映射到全局的一个三维坐标系上，就可以进行转换。\n旋转矩阵 R\n首先R属于三阶特殊正交阵，(满足6个条件) 而本来的维度是9，所以自由度为3，所以可以用一个向量来代替这个旋转矩阵。\n所以下面的工作就变成了R的降维工作。\n\n预备知识1：斜对角矩阵\n\\mu = (\\mu_1,\\mu_2,\\mu_3)\n\\hat{\\mu} =\n\\begin{bmatrix} \n{0}&amp;{-\\mu_3}&amp;{\\mu_2}\\\\\n{\\mu_3}&amp;{0}&amp;{-\\mu_1}\\\\\n{-\\mu_2}&amp;{\\mu_1}&amp;{0}\n\\end{bmatrix}\n\n对于任意的μ，总有唯一的μ^与之对应；\n对于任意的μ^ ，总有唯一的μ与之对应；\n\n预备知识2 ：e^w ^\n\n\nR的变换\n\n直觉理解\n\n\n首先因为旋转矩阵的自由度为3，所以是在3维流形中；\n在流形曲面处的一个很小的切平面就是李代数，可以用三维坐标表示处原来在流形中的坐标；\n整个三维流形就是李群\n"},"C_Research_Knowledge/最小二乘法和梯度下降法":{"slug":"C_Research_Knowledge/最小二乘法和梯度下降法","filePath":"C_Research_Knowledge/最小二乘法和梯度下降法.md","title":"最小二乘法和梯度下降法","links":[],"tags":["Optimization"],"content":"最小二乘法是用来最小化误差函数从而寻找函数得最佳参数的一种方法，学习的时候最小二乘法与梯度下降法我感觉很类似，所以来记录一下。\n参考文章：zhuanlan.zhihu.com/p/109986821\n最小二乘法推导\n首先，一元线性回归的最小二乘法推导比较容易，上述知乎也有链接，这里直接给出结论：\ny = Wx + b \\\\\nx = {x_1,x_2,...x_m}\\\\\n\\begin{cases}\n\nW = \\frac{\\sum_{i=1}^m(x_i-\\bar{x})*(y_i-\\bar{y})}{\\sum_{i=1}^m(x_i-\\bar{x})^2}\\\\\nb = \\bar{y} - W*\\bar{x}\n\\end{cases}\n对于多元线性回归，采用矩阵求解：同样直接给出结论\ny = X\\beta\n\n\n两者区别\n由上述可以看出，\n最小二乘法：\n\n可以直接求矩阵求逆一步到位，求出所的参数，求的全局最优解，无需迭代；\n但是只适用于线性回归模型，并且矩阵复杂时计算复杂度高\n\n梯度下降法：\n\n需要设置学习率进行一步步迭代，求得局部最小值或者是全局最优解\n适用于任何模型，只要是凸函数均可以求求出解\n\n思考\n这里想起来分类为什么不适合用最小二乘法求解？\n最小二乘法(y-y-true)^2有个平方，对于离群的点平方会很致命，尽量拟合数据就会导致错误出现\n\n2020年4月7日夜"},"C_Research_Knowledge/朴素贝叶斯分类器":{"slug":"C_Research_Knowledge/朴素贝叶斯分类器","filePath":"C_Research_Knowledge/朴素贝叶斯分类器.md","title":"朴素贝叶斯分类器","links":[],"tags":["Classification"],"content":"分类流程\n（1）模型建立\nP(C_1|X) = \\frac{P(C_1)*P(X|C_1)}{P(X)} = \\frac{P(C_1)*P(X|C_1)}{\\sum _{i=1}^nP(C_i)*P(X|C_i)}\n其中，X为观测到的样本，求x属于C1的概率，同理多分类只是分母x计算发生变化\n（2）P(x)对所有的分类计算概率来说都只是个常数，并不影响最终结果，故可省略\n（3）分子中的P(C1)是先验概率，需要人为定义，一般假设等概率,亦可其他\n（4）综合2，3 此时变化的量只剩P(x|C_1)了，即从C1分布中采样出x的概率，只需要最大化这一项。\n假设C1服从高斯分布，那就是求出均值、方差 这两个参数使得P(x|C_1)最大化，也就是最大似然估计：\nP(X|C_1) = \\prod_{k=1}^n P(x_k|C_1)\n\n上述用似然估计计算有一个假设就是假设X的属性之间相互独立\n\n这里的xi表示的是样本的属性！\n\n我一直奇怪这里的小x到底是啥，X是观测到的样本，xk是样本的各个属性，而C1类别对各个属性有个分布，假设是高斯分布，就是调整参数X使得这些个属性满足C1!\n\n（5）对未知样本X分类，也就是对每个类Ci，计算\nP(C_i)*P(X|C_i)\n最终挑选出一个最大的值对应的类别即是X所属的分类。\n踩坑\n贝叶斯分类器中，x表示属性。"},"C_Research_Knowledge/机器学习中判别式模型和生成式模型":{"slug":"C_Research_Knowledge/机器学习中判别式模型和生成式模型","filePath":"C_Research_Knowledge/机器学习中判别式模型和生成式模型.md","title":"机器学习中判别式模型和生成式模型","links":[],"tags":[],"content":"参考：www.cnblogs.com/nolonely/p/6435213.html\n判别式模型\n直接对条件概率P(Y|X)进行建模，常见的有线性回归等。可以认为是找出了判别的边界。\n举例：要确定一个羊是山羊还是绵羊，用判别模型的方法是从历史数据中学习到模型，然后通过提取这只羊的特征来预测出这只羊是山羊的概率，是绵羊的概率。\n\n生成式模型\n对x和y的联合分布p(x,y)建模，然后通过贝叶斯公式来求得p(yi|x)，然后选取使得p(yi|x)最大的yi\n举例：利用生成模型是根据山羊的特征首先学习出一个山羊的模型，然后根据绵羊的特征学习出一个绵羊的模型，然后从这只羊中提取特征，放到山羊模型中看概率是多少，在放到绵羊模型中看概率是多少，哪个大就是哪个。\n"},"C_Research_Knowledge/梯度下降法3":{"slug":"C_Research_Knowledge/梯度下降法3","filePath":"C_Research_Knowledge/梯度下降法3.md","title":"梯度下降法 ③_数学公式起源深入","links":["C_Research_Knowledge/深度学习入门系列一-梯度下降法","C_Research_Knowledge/深度学习入门系列一-梯度下降法-②","C_Research_Knowledge/梯度下降法3","C_Research_Knowledge/深度学习入门系列4-反向传播BP算法","C_Research_Knowledge/深度学习入门系列-逻辑回归","C_Research_Knowledge/深度学习入门系列6-Convolution-Neural-Network-CNN-卷积神经网络","C_Research_Knowledge/深度学习入门系列7-Tips-For-DeepLearning-全程高能","C_Research_Knowledge/深度学习入门系列8-Tips-For-DeepLearning-2-全程高能"],"tags":["Optimization"],"content":"入门系列文章：\n深度学习入门系列一-梯度下降法\n深度学习入门系列一-梯度下降法-②\n梯度下降法3\n深度学习入门系列4-反向传播BP算法\n深度学习入门系列-逻辑回归\n深度学习入门系列6-Convolution-Neural-Network-CNN-卷积神经网络\n深度学习入门系列7-Tips-For-DeepLearning-全程高能\n深度学习入门系列8-Tips-For-DeepLearning-2-全程高能\nGradient Descent 前面的文章讲了基本的流程，可以知道基本的梯度下降法更新参数流程是 w_new = w_origin - のL/のw， 这里为什么是负号呢？为甚要乘以梯度方向呢？这一切要从泰勒级数说起。\nTaylor Series\n假设函数 h(x) 在 x = x0 处无限可导，那么h(x)可以展开成如下：其中，x很接近x0时，后面的高次项边都可以忽略，因此约等于前两项。\n\n上图是泰勒级数在只有一个变量时的展开式，同样多个变量也可以展开：\n\nBased Taylor Series\n\n如图，在点(a,b)处取一个足够小的圆，此时损失函数 L(θ) 便可以根据泰勒公式在(a,b)处展开，其中 s，u，v, 三项均为常数。u是L在θ1方向的偏导数，v是L在θ2方向的偏导数。\n\n此时要最小化L，把上图公式中 **θ 1 -  a **用▲θ1表示，**θ 2 - b **用▲θ2表示，s与θ无关因此可以忽略，公式L 看起来就比较简单了，可以看成是 vector(u, v)  与  vector(▲θ1, ▲θ2) 的内积，则当(▲θ1, ▲θ2)处于圆上(长度最大)，方向相反(cos  = -1)时内积最小，因此可以得到图中最下面的推导公式，乘以 -n 表示的是比例，使得(▲θ1, ▲θ2)位于圆上，负号则是与(u, v) 方向相反。将(▲θ1, ▲θ2)换算之后便会得出通常的更新参数的公式，因此得出了文章最初的w_new = w_origin - n*のL/のw，因此梯度方向下降最快，要乘以负的学习率。\n至此梯度下降法的来源也都已清楚！"},"C_Research_Knowledge/概率论的贝叶斯公式":{"slug":"C_Research_Knowledge/概率论的贝叶斯公式","filePath":"C_Research_Knowledge/概率论的贝叶斯公式.md","title":"概率论的贝叶斯公式","links":[],"tags":["概率论"],"content":"概率论是深度学习的基础，但是我发现已经忘记了概率论中很多知识，所以复习了一下，做一下记录，也算是想起一些以前疑惑的东西。\n关于贝叶斯与全概率，\n数学上张宇讲的比较透彻，也比较容易理解：www.bilibili.com/video/av9735632\n但是可以从另外一个给人启发的角度看待：www.bilibili.com/video/av84799361\n互斥与独立\n\n两个事件互斥 == 两个事件互不相容 , 这一点可以从几何关系理解，也就是说两个集合没有交集；\n两个事件独立则是从概率的角度来讲的，举一个简单的公式，若A、B事件互相独立，则：\n\nP(AB) = P(A) * P(B)\n然后由条件概率可以知道，在A发生的条件下，B发生的概率为：\nP(B|A) = \\frac {P(AB)}{P(A)}\\\\\nP(AB) = P(A)*P(B|A)\n若AB事件独立，可以推出\nP(B) = P(B|A)\n通俗来说，就是B发生的概率与在A发生的条件下B发生的概率相同，也就是A事件发生并不影响B，这就是AB相互独立。\n这两者其实是没什么关系的，不用刻意去区分这两个概念。\n贝叶斯公式\n数学上理解，贝叶斯公式就是条件概率，执果索因：\nP(H|E) = \\frac {P(H)P(E|H)}{P(E)} = \\frac{P(H)P(E|H)}{P(H)P(E|H)+P(\\neg H)P(E|\\neg H)}\n其中，\n\n\nP(H)也叫先验概率，H常被视为导致试验结果E发生的”原因“\n\n\nP(E|H)也叫似然概率，likelyhood\n\n\nP(H|E)这个计算出来的概率则成为后验概率，当试验产生了结果A之后，再对各种原因概率的新认识，故称后验概率。\n\n\n贝叶斯公式给我们的思考是：\n\n见到所有的证据从而限制了概率空间之后，在考虑比例，这就是贝叶斯公式的精髓\n证据不应该直接决定了你的看法，而是应该更新你的看法\n"},"C_Research_Knowledge/深度学习入门系列-深度学习的误差来自哪里":{"slug":"C_Research_Knowledge/深度学习入门系列-深度学习的误差来自哪里","filePath":"C_Research_Knowledge/深度学习入门系列-深度学习的误差来自哪里.md","title":"深度学习入门系列-深度学习的误差来自哪里","links":[],"tags":["Optimization"],"content":"之前文章讲到过用梯度下降法来更新参数。深度学习反向传播，根据误差调整参数，那么深度学习的error到底来自哪里？这个是比较必要的，可以给我们提高自己的模型提供方案。\n这个李宏毅老师讲的很清楚，有需要可以学习一下，附上链接:www.bilibili.com/video/av48285039\nError来源概述\n深度学习种Error 来源于两方面：\n\nbias 偏置\nvariance 方差\n\n\nVariance\n直观上，方差可以了解为散布的分散程度。上图可以明显的看出比较高的方差，点的散布很分散，比较低的方差则比较紧凑。方差可以用来形容模型的稳定性。一般来说，\n比较简单的模型，方差较小，分布比较紧密；比较复杂的模型，方差比较大，分布比较分散\n可以如下解释：\n\n比较简单的模型，比如 f(x) = c，受数据的影响为0，因此所有值相同，方差为0，分布都在一起\n比价复杂的模型，比如五次方方程，受到数据的影响会比较大，取到的值很多，因此每次的预测值可能相差很远，此时方差就比较大了\n\nBias\n将所有预测出来的函数求出期望值得到的函数距离真实函数依然会有一段距离，这就是bias，如下图所示，\n红线表示100个预测出来的函数，蓝线表示这100个函数的平均，黑线表示真实函数\n\n可以看出，简单的模型bias比较大，而复杂的模型bias比较小\n可以解释为，上图的底部，我们所设计模型的复杂程度其实也就决定了函数所能表达的范围，过于简单的函数所包含的范围可能根本没有包含到真值，因此bias比较大；而比较复杂的函数范围比较大，包含到了真值，因此bias会比较小。\n比较\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n模型复杂程度方差 Variance(精确性)偏置Bias(准确性)Simple Model较小较大Complex Model较大较小\n总结与改进\n了解了误差的来源，那么如何判断自己的模型是什么问题呢？\n\n\n如果模型无法适应训练数据，也就是说在训练集上误差比较大，此时就是 bias 比较大，此时处于**欠拟合状态，（underfitting）**也就是模型过于简单，此时改进：\n\n载入更多的特征\n创建更复杂的模型\n注意，此时收集数据集增加数据集并起不到作用\n\n\n\n如果模型在训练集表现得很好，但是在测试集表现得很差，此时就是Variance比较大，也就是**过拟合（overfitting）**了，此时改进：\n\n增加数据集，对于过拟合是一个很好的解决办法，如果采集不到更多的数据，则可以利用现有的数据去生成更多新的数据\nRegularization 正则化 ，正则化的效果是使得到的曲线更加的平滑，如下图\n\n\n\n\nRegularization\n以线性回归为例，在损失函数的最后加上一项 参数 wi的平方和，这就会要求w越小越好，起到限制作用\n\n\n\n此时函数会比较平滑，平滑即对输入不会非常敏感，至于为什么会比较平滑，可以看上图下面部分，当xi 变化时，输出结果就加了一项 wi * 变化量， 若wi 很小接近0，对结果影响并不大，因此会比较平滑。\n\n\n“莱姆大” 正则化项的系数控制了函数的平滑程度，函数不是月平滑越好，有个度，因此有一个系数控制\n\n\n对于偏置b，不用加正则化项是因为 ： 正则化使得函数更加平滑，能过滤掉杂色信息，而偏置 b 只能使得函数上下移动，对本来的目的并没有帮助。因此正则化项没有偏置 b 。\n\n"},"C_Research_Knowledge/深度学习入门系列-逻辑回归":{"slug":"C_Research_Knowledge/深度学习入门系列-逻辑回归","filePath":"C_Research_Knowledge/深度学习入门系列-逻辑回归.md","title":"Logistic Regression","links":["C_Research_Knowledge/深度学习入门系列一-梯度下降法","C_Research_Knowledge/深度学习入门系列一-梯度下降法-②","C_Research_Knowledge/梯度下降法3","C_Research_Knowledge/深度学习入门系列4-反向传播BP算法","C_Research_Knowledge/深度学习入门系列-逻辑回归","C_Research_Knowledge/深度学习入门系列6-Convolution-Neural-Network-CNN-卷积神经网络","C_Research_Knowledge/深度学习入门系列7-Tips-For-DeepLearning-全程高能","C_Research_Knowledge/深度学习入门系列8-Tips-For-DeepLearning-2-全程高能"],"tags":["Classification"],"content":"入门系列文章：\n深度学习入门系列一-梯度下降法\n深度学习入门系列一-梯度下降法-②\n梯度下降法3\n深度学习入门系列4-反向传播BP算法\n深度学习入门系列-逻辑回归\n深度学习入门系列6-Convolution-Neural-Network-CNN-卷积神经网络\n深度学习入门系列7-Tips-For-DeepLearning-全程高能\n深度学习入门系列8-Tips-For-DeepLearning-2-全程高能\n逻辑回归，是用于处理因变量为分类变量的回归问题，比如二分类问题，其实是一种分类算法。本文主要对比逻辑回归回归与线性回归的处理过程并探讨其中一些公式、细节。\n逻辑回归&amp;线性回归\n\n如上图所示，\n\nstep1确定函数时，逻辑回归函数输出的是个概率值，线性回归输出的可以是任何一个数\nstep2确定损失函数，逻辑回归用的交叉熵，线性回归则是均方误差\nstep3比较神奇，两个损失函数长得差别很大，但是算到最后更新参数公式竟然是一样的！(后面有推导)\n\nCross Entropy\n背景，一系列函数设置：\n \n设计损失函数：这里有一个假设， x1 x2 …xN 属于 C1 类，真值为1，其余为0\n\n**这里的之所以要取对数，是因为拆开之后求微分比较容易。**蓝色线画的，即是交叉熵，也就是表格中step2的C函数。\n\n上述函数对w求偏微分之后，也就是求出了梯度，（过程可以自己算一下，挺简单的），就得到了下面的公式，此时更新参数就和线性回归相同也推出来了。\n交叉熵和均方误差\n\n图中黑色曲面的代表交叉熵，红色的曲面代表均方误差，这幅图表达了参数的变化对于整个loss值的影响大小\n\n对于Cross Entropy，越边缘的点梯度越大，此时更新参数会比较快\n对于 Square Error，边缘的点梯度并不大，而接近最小值的点梯度也比较小\n\nConclusion\n由上面的对比可知：逻辑回归如果用均方误差，会导致更新参数非常慢，而直接提高学习率也并不是一个好的选择，因为真正靠近真值时，梯度本来就比较小，较大的学习率此时也并不合理，因此一般来说，逻辑回归(也就是分类问题)可以采用交叉熵作为损失函数。\n\n2020/6/8更新\n说实话看以上的确实是一头雾水，可能看完还是不知道什么是逻辑回归，因为上述文章只是说到逻辑回归常用于二分类问题，这里做一些补充、再学习。\n白话来说，二分类网络最后一层使用sigmoid函数就是逻辑回归方法，具体原因是：\n\n鉴于要写的东西比较多，决定 新写一篇文章。"},"C_Research_Knowledge/深度学习入门系列4-反向传播BP算法":{"slug":"C_Research_Knowledge/深度学习入门系列4-反向传播BP算法","filePath":"C_Research_Knowledge/深度学习入门系列4-反向传播BP算法.md","title":"反向传播BP算法","links":["C_Research_Knowledge/深度学习入门系列一-梯度下降法","C_Research_Knowledge/深度学习入门系列一-梯度下降法-②","C_Research_Knowledge/梯度下降法3","C_Research_Knowledge/深度学习入门系列4-反向传播BP算法","C_Research_Knowledge/深度学习入门系列-逻辑回归","C_Research_Knowledge/深度学习入门系列6-Convolution-Neural-Network-CNN-卷积神经网络","C_Research_Knowledge/深度学习入门系列7-Tips-For-DeepLearning-全程高能","C_Research_Knowledge/深度学习入门系列8-Tips-For-DeepLearning-2-全程高能"],"tags":["Optimization"],"content":"入门系列文章：\n深度学习入门系列一-梯度下降法\n深度学习入门系列一-梯度下降法-②\n梯度下降法3\n深度学习入门系列4-反向传播BP算法\n深度学习入门系列-逻辑回归\n深度学习入门系列6-Convolution-Neural-Network-CNN-卷积神经网络\n深度学习入门系列7-Tips-For-DeepLearning-全程高能\n深度学习入门系列8-Tips-For-DeepLearning-2-全程高能\n之前讲过机器学习的三个步骤，深度学习Deep Learning非常的类似，可以概括为以下几步：\n\n(设置函数) → 搭建神经网络\n(函数的好坏定义) → 设置损失函数\n(找出最优函数) → 反向传播更新参数\n\n第一步之前的设置函数，在这里用神经网络来替代了；\n在线性回归逻辑回归中可以直接计算梯度，但是深度学习神经网络比较深，不能一下子求出梯度，因此本文主要来探讨一下反向传播 Back Propagation算法。\n同时，本文会附上手动搭建神经网络、计算梯度、实现反向传播的代码，纯手写只用到了numpy库！\n前置知识\n神经网络的反向传播并不需要很高深的数学知识，需要掌握链式求导法则 (Chain Rule)。下面会一步步从数学上求出微分 ，并且理解这种算法的精妙之处。\n案例背景\n\n一个有代表性的例子，输入层经过神经网络得到输出值，与真实值之间存在误差，这里用交叉熵作为损失函数，因此在这里，要求梯度也就是求损失函数对于w的偏微分。\n层层深入\n\n把上面具体的神经网络展开，假设只有两个神经元，这里只对w求微分，b的方式是一样的，所以以w为例。\n首先，根据上图的函数以及chain rules，损失函数C对w的偏微分可以拆解为两部分\n\nC 对于 z 的偏导数\nz 对于 w 的偏导数\n\nForward Pass\n（先讲第二部分）其中，z 对于 w 的偏导数比较容易，可以很容易的看出来 z 对于 w1 的偏导数就是w之前的输入值，也就是 x1，同理 z 对于 w2 的偏导数就是 x2，下面的图强化一下 z 对于 w 的偏导数\n\n可以观察到z对于每个w的偏导数就是 当前权重所之前的输入值，这样比较拗口，英语会比较好理解：\nThe value of the input connected by the weight .通俗的说就是这条线从哪里出来，出来的那个节点值就是z对于这条线也就是这个w的偏微分。\n这里也可以看出，前向传播可以算出每个中间值，也就是计算出了每个梯度的上述第二部分。\nBackward Pass\n下面看第一部分比较复杂的，也就是 C 对于 z 的偏导数。\n\n上图中 z 经过 sigmoid 函数 得到 a，a 继续传播到下一层，此时 C 对于 z 的偏导数可以转化为上图中的下面公式所写的。在求和的两部分中，同样的各自又都分为两部分，与上述的两部分类似。z’ 对于 a 的偏导数很容易，就直接是 w3 ，相应的 z’‘ 就是 w4.\n所以此时问题就转化为 C对于z’ C对于z‘’ 的偏微分，如果这两部分知道那么就可以求出来 C对a的偏微分，同样的C对z的偏微分也就求出来了，也就解决了这部分问题。以下内容是关键：\n反求\n\n上图的下面的公式只是把值带入了前一步中的公式，但是可以想象一下，这里面的 乘法、加法 操作 很像是神经网络的前向传播，所以这里就可以想象成一个新的神经网络，只不过是反过来计算的，这时候就会计算出来 C 对于 z 的偏微分。值得注意的是 上图中 sigmoid’(z) 是个常数，因为z前向传播时已经计算出来了，所以这里就是计算一个数而已。因此现在到这一步，说明知道后面两项的偏微分可以求出前面的。 此时问题依然是 C对于z’ C对于z‘’ 的偏微分。\n大胆假设、细心求证\n假设一 (easy)\n假设 z‘ z’‘ 之后经过激活函数直接是最终的输出，此时求微分就很简单了，如下图：\n\n其中，C 对于 y1 的偏微分就是损失函数的偏微分，y1 对于 z’ 的偏微分就是根据激活函数（上图最后的橙色圆圈）求出微分很容易，z‘’ 同理。\n在此种假设下，此时已经得出了 C 对于 z‘、 C 对于 z’‘ 的偏微分，回溯到前一个步骤，就求出了 C对于z的偏导，在往前回到最初步，发现此时已经求出了两个 需要的条件，此时就可以算出 C 对于 w1 ，C  对于 w2 的偏微分。\n也就是说，忙活到现在，也就只是算出来了第一个神经元的两条线(2个w)的梯度！\n假设二 (Normal)\n假设 z‘ z’‘ 之后 依然有很多层，如下图：\n)\n由反求部分我们已经知道想要求 C对于 z’（or z‘’） 的偏导数，需要知道后面 C 对于 Za 及 Zb 的偏导数，所以需求会一直往后寻找，递归这个过程，知道到达输出层，然后一层层往前就会求出来最初始的梯度，如下图：\n\n至此，如果理解了这些，就已经理解了反向传播的原理了。\nConclusion &amp; Question\nBP算法总结\nBack Propagation  算法分为两部分\n\n前向传播 求出 z 对于 w 的偏导数\n反向传播 求出 C 对于 z 的偏导数\n两个值相乘就是梯度\n\n思考\n从前到后的传播直接计算每个参数的梯度为什么比BP算法差?\n前向传播计算梯度\n从前到后直接传播计算梯度，第一层的w需要知道后面所有的层的梯度，此时会进行一趟计算；2-end；\n继续求第二层w 的梯度，需要知道后面所有层的梯度，也就是 3 - end； 最后加起来就是：\nend - 2 + end - 3 + end - 4 + … + end-0,z明显计算量不小！\n反向传播计算梯度\n从最后一层开始计算，先计算出最后一层梯度，可以直接计算出来，这样每次往前计算不用再一直累加，因此计算量小很多。所以说BP算法刚好就是利用了原来的网络和参数而且可以用和前向传播相同的计算量计算出所有w的梯度。这就是BP算法的精妙之处！\n代码实现\n在这里手动搭建了一个神经网络，暂时没有考虑b，因为只是用来加深理解，又一个输入层，两个隐藏层，一个输出层，每层四个神经元。所有参数都是手动计算梯度。\n根据以上分析的反向传播算法可以总结出以下几步：\n\n前向传播一遍计算出所有节点的值\n反向传播一遍计算出所有结点的偏微分\n做乘法求出所有的梯度进行更新\n\nimport numpy as np\n#Generate data\n# Forward Node\nx_F = np.random.rand(4)\ny_F = np.random.rand(4)\nz_F = np.random.rand(4)\np_F = np.random.rand(4)\n \n#weight\nx_y_w = np.random.rand(4,4)\ny_z_w = np.random.rand(4,4)\nz_p_w = np.random.rand(4,4)\n#backward node\nx_B = np.random.rand(4)\ny_B = np.random.rand(4)\nz_B = np.random.rand(4)\np_B = np.random.rand(4)\n \n#TARGET\ntarget = np.array([0.5,0.7,0.3,0.1])\n#loss\ndef SquareErrorLoss(output, target):\n    loss = 0\n    for i in range(len(output)):\n        loss = loss + (output[i] - target[i])**2\n    loss = loss\n    return loss\n \n# Graident\nlr = 0.0000001\nfor epoch in range(500000):\n    # forward0\n    y_F = np.matmul(x_F, x_y_w)\n    z_F = np.matmul(y_F, y_z_w)\n    p_F = np.matmul(z_F, z_p_w)  # 得到输出\n    loss_end = SquareErrorLoss(p_F, target)\n    # backward\n    p_B = 2*p_F  # end grad\n    z_B = np.matmul(p_B, z_p_w.T)\n    y_B = np.matmul(z_B, y_z_w.T)\n    # print(z_F[0])\n \n    # grad\n    z_p_w_grad = [np.dot(z_F[0], p_B),\n                  np.dot(z_F[1], p_B),\n                  np.dot(z_F[2], p_B),\n                  np.dot(z_F[3], p_B)]  # 4*4\n    y_z_w_grad = [np.dot(y_F[0], z_B),\n                  np.dot(y_F[1], z_B),\n                  np.dot(y_F[2], z_B),\n                  np.dot(y_F[3], z_B)]  # 4*4\n    x_y_w_grad = [np.dot(x_F[0], y_B),\n                  np.dot(x_F[1], y_B),\n                  np.dot(x_F[2], y_B),\n                  np.dot(x_F[3], y_B)]  # 4*4\n    # update\n    x_y_w = x_y_w - lr * np.array(x_y_w_grad)\n    y_z_w = y_z_w - lr * np.array(y_z_w_grad)\n    z_p_w = z_p_w - lr * np.array(z_p_w_grad)\n \n    if epoch % 5000 == 0:\n        print(&quot;当前loss值为&quot;)\n        print(loss_end)\n        print(p_F)\n \n#截取输出片段打印\n当前loss值为\n165.06777280622663\n[8.38287281 5.70538503 7.00228248 5.84052431]\n当前loss值为\n133.11940656598998\n[7.56299701 5.15781875 6.33931068 5.28536964]\n当前loss值为\n109.77775387801975\n[6.89894847 4.7147705  5.8028157  4.83622736]\n当前loss值为\n92.14372363473903\n[6.34836332 4.34779557 5.35839007 4.4642465 ]\n当前loss值为\n78.45934876430728\n[5.88318774 4.03806177 4.98325151 4.1503256 ]\n当前loss值为\n67.60388807433274\n[5.48405883 3.77257388 4.66167746 3.88128352]\n当前loss值为\n58.833090378271265\n[5.13715314 3.54205652 4.38244476 3.64771203]\n当前loss值为\n51.6356735906825\n[4.83232077 3.3397007  4.13731374 3.44270456]\n... ... ... ... ... ... ... ... ... ... ... ...\n当前loss值为\n0.34148441938663887\n[0.38019982 0.42063884 0.60690939 0.49356868]\n当前loss值为\n0.33762403308295663\n[0.37029135 0.41401346 0.59891746 0.48685882]\n当前loss值为\n0.3343170568629769\n[0.36058041 0.40751086 0.5910724  0.48027121]\n当前loss值为\n0.3315343883568477\n[0.3510625  0.40112821 0.58337076 0.47380298]\n当前loss值为\n0.3292483287899748\n[0.3417333  0.39486274 0.57580922 0.46745137]\n以上代码可以看出，loss值不断不断的下降，并且最终趋于稳定，可以说明由之前的推导总结出的方法思路并无问题！"},"C_Research_Knowledge/深度学习入门系列6-Convolution-Neural-Network-CNN-卷积神经网络":{"slug":"C_Research_Knowledge/深度学习入门系列6-Convolution-Neural-Network-CNN-卷积神经网络","filePath":"C_Research_Knowledge/深度学习入门系列6-Convolution-Neural-Network-CNN-卷积神经网络.md","title":"Convolution Neural Network(CNN)卷积神经网络","links":["C_Research_Knowledge/深度学习入门系列一-梯度下降法","C_Research_Knowledge/深度学习入门系列一-梯度下降法-②","C_Research_Knowledge/梯度下降法3","C_Research_Knowledge/深度学习入门系列4-反向传播BP算法","C_Research_Knowledge/深度学习入门系列-逻辑回归","C_Research_Knowledge/深度学习入门系列6-Convolution-Neural-Network-CNN-卷积神经网络","C_Research_Knowledge/深度学习入门系列7-Tips-For-DeepLearning-全程高能","C_Research_Knowledge/深度学习入门系列8-Tips-For-DeepLearning-2-全程高能"],"tags":["CNN"],"content":"入门系列文章：\n深度学习入门系列一-梯度下降法\n深度学习入门系列一-梯度下降法-②\n梯度下降法3\n深度学习入门系列4-反向传播BP算法\n深度学习入门系列-逻辑回归\n深度学习入门系列6-Convolution-Neural-Network-CNN-卷积神经网络\n深度学习入门系列7-Tips-For-DeepLearning-全程高能\n深度学习入门系列8-Tips-For-DeepLearning-2-全程高能\nCNN的提出\n其一，之前所提到的线性回归、比较简单逻辑回归都是全连接层(Full-Connected) ,那么在图像处理领域输入数据都是图像，现实中一张很小的图 100 * 100 ，分辨率已经很低了，然是依然有30000 维数据，(这里默认图象是彩色图三通道的)，然后后面再堆几层网络，参数量实在是巨大，这是全连接网络的缺陷；\n**其二，**基于现实的观察有以下基点：\n\n\n假设CNN中每一个神经元都是用来识别某一个pattern[例如：鼻子，嘴，手臂] (实际上大概也是这样工作的)\n\n\n人们在辨识一些小的部分比如鸟喙时，并不需要遍历一张图的所有信息，而是看到图片的一小部分就可以捕捉到需要的信息；\n\n\n同一个部分(鸟喙)在图像中可能会出现在不同的位置，因此CNN的神经元以相同的参数就可以发现不同位置的鸟喙而不用重新学习参数\n\n\n图像进行下采样，并不会影响我们对图片的观察(不包括比较极端的)；而图像较小的时候像素比较少此时也会减少参数\n\n\n基于以上，CNN卷积神经网络就正式提出了，并且在计算机视觉领域(影像处理)非常有效，几乎所有的任务，第一步都是要用卷积神经网络来提取特征。\nCNN一般架构\n卷积神经网络一般是输入图像，然后经过 （卷积层、池化层）这两个一直重复，然后输出的像素拉平(flatten操作将当前值转变为一维向量)，连接上全连结网络输出，如下图：\n\nConvolution计算流程\n首先，CNN中要训练的参数就是卷积核的每个像素的数值\n单通道卷积计算\n\n如上图，用filter1在图像6*6图像上滑动，从左上角开始，步长为1，在每个窗格对应位置相乘然后加起来输出一个新的值，此时就会形成一个新的4 ** 4的 img ，称为特征图**Feature Map **。\n此时有一个卷积核，就输出一张特征图，两个卷积核就输出两张特征图，以此类推。\n多通道卷积计算\n\n如果输入的图像是三通道的，那么每个卷积核对应的也是三通道的，注意此时计算可能是：\n卷积核的第一个通道与图像红色通道进行卷积运算，卷积核的第二个通道与图像绿色通道进行卷积运算，卷积核的第三个通道与图像蓝色通道进行卷积运算，然后 卷积核三个通道输出的img对应位置相加，形成一个新的1个通道的img，就是这个卷积核所输出的 Feature Map  。这里注意的是： 对于多通道图像，一个卷积核进行卷积运算后所输出的依然是一个 feature map，而不是9个(3*3).\nConvolution &amp; Neural Network\n以上讲了卷积的运算方式，那么卷积与神经网络，与全连接网络有什么关系呢？\n卷积实际上就是全连接网络(去掉一些weight) !\n\n分析一下这张图，\n\n\n首先右边蓝色的 1 2 3 4 ···一直到16，表示的是将左边6*6的图像拉平（这里没有画完），蓝色的框里的数字是每个像素的值；\n\n\n然后上面是个3*3的卷积核，每个像素用不同颜色的⚪圈了起来；\n\n\n然后上图右边部分橙色的 3，-1 就是 卷积核与图像滑动过的区域做的卷积计算得到的数值，将卷积核卷积后的4*4的img也拉平，就得到了右边的 3 ，-1  （这里用3和-1举例子所以没有画完）\n\n\n大量参数的减少\n卷积之后得到的图像的每个像素也就是右边的3，-1 等，可以看作是一个神经元，其中 卷积核做图像左上角的时候，计算刚好是与原来的6*6图像的 编号为 1 2 3 7 8 9 13 14 15 的像素进行的，因此“3”这个神经元就连接到了编号为 1 2 3 7 8 9 13 14 15 的像素，-1是同样的道理。这时候，如果计算参数量，就是 16 * 9 = 144 个参数，而此时如果用全连接层的话，就是 36 * 16 = 576 个参数，已经少了很多了\n参数共享\n上图中右边部分的神经元，并不是说所有的参数都要计算。一个卷积核中同一个像素滑动过的值他们之间的权重都是强迫相等的。举个例子，卷积核中的第一个像素(深红色圆圈)，与6*6的图像在左上角计算卷积时对应的编号为1的像素，卷积核向右滑动一次后，该像素(深红色圆圈)对应的是编号为2的像素，因此 上图右边部分 1 号像素和 右边的神经元3 ， 2号像素与右边的神经元-1之间连接都用的是深红色，这两条线的参数就是相等的。所以同理，上图右边部分连线中颜色相同的权值都是共享的。(Share Weights) 此来再来计算一下参数量，就只有9个了。\n这其实也不难理解，一开始文章就提到，卷积神经网络的参数就是卷积核的像素值，这里是3*3的卷积核9个像素，所以也就只有9个参数了。到这里已经是全连接网络的 1/64 了，也就是减少了64倍的参数，这在 上百万参数是减少的就更明显了！\n到这里，已经理解了卷积神经网络的计算方式以及如何减少参数\n池化 Max pooling\n\n在卷积输出的特征图基础上，以2*2为单位，每个红色框里选出最的值组成一个新的img，这就是最大池化；\n平均池化就是一个红色框里所有的像素值取平均。\n经过池化，图像尺寸变为 2*2\n池化层采用最大池化方式，那么怎么求微分呢？不可导就不能梯度下降，这个下一篇文章会说。\nConclusion\n以上就是CNN，卷积神经网络，工作方式可以理解为某一层的神经元识别一个 pattern ，然后全连接层组合这些个 pattern 最后提取出高质量的特征。 这个可以自己求证一下。大概可以这么解释。"},"C_Research_Knowledge/深度学习入门系列7-Tips-For-DeepLearning-全程高能":{"slug":"C_Research_Knowledge/深度学习入门系列7-Tips-For-DeepLearning-全程高能","filePath":"C_Research_Knowledge/深度学习入门系列7-Tips-For-DeepLearning-全程高能.md","title":"Tips For DeepLearning_1","links":["C_Research_Knowledge/深度学习入门系列一-梯度下降法","C_Research_Knowledge/深度学习入门系列一-梯度下降法-②","C_Research_Knowledge/梯度下降法3","C_Research_Knowledge/深度学习入门系列4-反向传播BP算法","C_Research_Knowledge/深度学习入门系列-逻辑回归","C_Research_Knowledge/深度学习入门系列6-Convolution-Neural-Network-CNN-卷积神经网络","C_Research_Knowledge/深度学习入门系列7-Tips-For-DeepLearning-全程高能","C_Research_Knowledge/深度学习入门系列8-Tips-For-DeepLearning-2-全程高能","[https:/huaqi.blue/2020/02/10/深度学习入门系列一-梯度下降法-②/"],"tags":["Optimization"],"content":"入门系列文章：\n深度学习入门系列一-梯度下降法\n深度学习入门系列一-梯度下降法-②\n梯度下降法3\n深度学习入门系列4-反向传播BP算法\n深度学习入门系列-逻辑回归\n深度学习入门系列6-Convolution-Neural-Network-CNN-卷积神经网络\n深度学习入门系列7-Tips-For-DeepLearning-全程高能\n深度学习入门系列8-Tips-For-DeepLearning-2-全程高能\n本文主要讲深度学习中常见的一些但是又理解不透彻的方法原理以及用途，全程高能！\n主要有：\n\n深度学习流程\nActivation Function（各种）\n各种梯度下降法\nL1、L2 正则化\ndropout\n\nDeepLearning 流程\n\n\n首先是之前文章提到过的深度学习的三个步骤，这三个步骤会搭建起一个神经网络；\n训练神经网络，检测网络对于训练集的效果,此时如果效果很差就要回去调整模型，继续训练，知道得到比较好的结果\n此时在训练集效果达到了，开始在测试集(验证集)进行测试，如果效果不好就说明过拟合了，需要采取一些措施。\n\n这里要注意，不是说效果一差就是过拟合，要理性分析。\nTraining set 效果差的原因及其解决\nVanishing Gradient\n首先思考一个问题，为什么需要激活函数？\n一个普通的全连接网络，如果没有激活函数，那么所有的操作都是乘积、求和，这就是一个线性的网络，表达能力非常差，也不是我们所需要的，因此传统的在每个层后面输出时会加个sigmoid这样的非线性函数。也就是说，激活函数可以使我们的网络编程非线性的。\nSigmoid 函数\n\n如图所示，所有的输入经过sigmoid会强行压缩到0-1之间，这样会使函数变成非线性的，但是也随之带来了问题：强行将所有之压缩到0-1，随着网络层数的加深，反向传播时靠近输出层的梯度值还比较大，但是输入层附近的层梯度都会很小，梯度从后往前越来越小直到消失，这就是梯度弥散 (gradient Vanish )问题.\n\n靠近输出层的参数更新幅度比较快，但是此时靠近输入层的梯度已经很小接近于0，参数基本无更新，也就是还是随机值的状态来更新后面的参数，所以此时后面更新的参数问题就很大！\n当输入层某个w发生变化就算变化非常大，而sigmoid的值变化幅度确非常的小，因为函数曲线很缓，如下图\n\n\nReLu 函数\n\n如图所示，输出值a小于0，则 Relu(a) = 0  否则，Relu(a) = a\n这个激活函数的出现解决了梯度弥散问题\n\n解决方式如图，值为0的神经元可以去掉，整个网络就是一个线性函数了。\n这里有一个问题是：0的神经元消除，整个网络就是一个线性函数，这不符合我们要求啊？\n可以这样解释，改变了数据输入，神经元的连线发生变化，整个网络依然是非线性的。\n着重理解Relu，它的一系列的变体函数就不说了，比较相似\nCNN文章提到的Max Pooling 不可导，和这个就很相似了，每次都是取最大值，然后其他的神经元都丢掉，整个网络又是一个细长的线性神经网络。其实是和下面的 Max-out函数比较相似\nMax-Out 函数\n\n这个方法是打组思想，Relu就是它的一个个例，这里就不细说了，relu用的多一点。\n有个问题：这个每次只取最大的这样有的神经元就不会被train到。因为每次数据不同得到的最大值也会不同，所以所有的weight都会得到更新，都会被train到。\nLearning Rate\n梯度下降法的时候，会有局学习率问题，所以出现了一些梯度下降法的变体。\nGradient Descent、Adagrad Gradient Descent 、以及 SGD 在之前文章都有讲过,不清楚可以返回看一下。\nAdagrad梯度下降法实现了自适应的学习率，但是实际上我们面对的比这个所能解决的问题更加复杂，如下图。\n\n\n左边的在w1方向，从左到右是比较平坦的，所以一个学习率就可以；\n右边的图同样在w1方向比较缓的地方学习率就应该小点儿，突然陡峭学习率又应该大一点。因此需要更加动态的调整学习率。\n\nRoot Mean Square Prop(RMSProp)\n\n与之前的Adagrad方法非常类似，不同的是，这个方法可以控制权重，更偏向于过去的信息还是新的梯度信息，可以通过设置权重值来控制\nLocal  Minimum\nMomentum (冲量、惯性)算法\n如下图，在求出当前阶段的梯度值后，不只是考虑当前的梯度方向，同时考虑了前一个梯度方向，然后做个加权和得到新的方向。\n\n这可以说是根据现实生活中的惯性，会继续向前走一点。\nAdam 优化算法\nAdam则是 RMSProp、Momentum两个梯度下降算法的集大成者。在深度学习框架中是已经封装好的。这个并未深究，我觉得理解了基础的就可以。\n关于测试集效果差，且听下回分解。\n\n主要讲正则化、Dropout这两块。上图中前两种方法都是比较经典的，不只是深度学习的，而正则化用的多一点，所以主要理解一下正则化； Dropout则是非常具有深度学习风格，所以需要深究。"},"C_Research_Knowledge/深度学习入门系列8-Tips-For-DeepLearning-2-全程高能":{"slug":"C_Research_Knowledge/深度学习入门系列8-Tips-For-DeepLearning-2-全程高能","filePath":"C_Research_Knowledge/深度学习入门系列8-Tips-For-DeepLearning-2-全程高能.md","title":"Tips For DeepLearning_2","links":["C_Research_Knowledge/深度学习入门系列一-梯度下降法","C_Research_Knowledge/深度学习入门系列一-梯度下降法-②","C_Research_Knowledge/梯度下降法3","C_Research_Knowledge/深度学习入门系列4-反向传播BP算法","C_Research_Knowledge/深度学习入门系列-逻辑回归","C_Research_Knowledge/深度学习入门系列6-Convolution-Neural-Network-CNN-卷积神经网络","C_Research_Knowledge/深度学习入门系列7-Tips-For-DeepLearning-全程高能","C_Research_Knowledge/深度学习入门系列8-Tips-For-DeepLearning-2-全程高能"],"tags":["Optimization"],"content":"入门系列文章：\n深度学习入门系列一-梯度下降法\n深度学习入门系列一-梯度下降法-②\n梯度下降法3\n深度学习入门系列4-反向传播BP算法\n深度学习入门系列-逻辑回归\n深度学习入门系列6-Convolution-Neural-Network-CNN-卷积神经网络\n深度学习入门系列7-Tips-For-DeepLearning-全程高能\n深度学习入门系列8-Tips-For-DeepLearning-2-全程高能\n前一篇中文讨论到，在深度学习过程中如果在训练集效果差怎么办，这里接着讨论后半部分，在训练集得到了想要了的效果，但是测试集（验证集，或者是有标签的一些数据）效果并不理想应该怎么办？有如下几种方案、方法可以参考一下：\n\nEarly Stopping\nRegularization\nDropout\n\nEarly Stopping\n\n这个方法大致提一下。深度学习中我们有一个假设：训练集和测试集分布是一样的。但是实际上可能并不会如此，我们训练模型应该在验证机loss最低的时候停下来，这就是这个方法的基本思想，具体的可以查阅下 文档。这里我自己主要也是了解一下即可，知道有这么一回事。\nRegularization\n\n正则化的目的是使得函数更加的平滑，因此正则化一般不对 bias 偏置做；\n正则化会使得参数变小\n正则化并不是非常的重要，效果不会非常显著\n\nL2 正则化\n\n首先是右上角L2范数，是w的平方和。\n然后是下面的公式推导，L‘是损失函数，L’对某一个w微分，就是后面的结果，然后更新参数公式也很顺理成章，整理到最后就是会在w之前✖一个系数，并且这个系数通常是一个很小的值，整个这个系数接近1，因此每个参数每次更新前会越来越接近0；第一项与第二项最后会实现平衡。\n这个手段也成为 Weight Decay，权重衰减\nL1 正则化\nL1范数是绝对值求和。那么绝对值微分问题就是在真的走到0时直接随便丢一个value比如0当作微分。\n\nL‘对于某一个w的微分就是上图中所推导的，后面的sgn(w)意为：w为正则为1，w为负则为-1.\n更新参数时总是在后面减去一项学习率 * 权重 * （1或-1），w为正数时减去一个数，w为负数时加上一个数，总之就是使得参数更加地接近0\nContrast\nL1、L2正则化都是使得参数变小，但是略有不同；\n\nL1每次剪掉固定的值，\nL2则是每次乘以一个固定的值，\n\n如果有一个参数很大，那么用L2正则化更新就比较快；用L1正则化更新依然很慢；\n如果有一个参数很小，那么L2正则化更新就很慢；L1正则化 更新会比较快\n正则化中的权值衰减，与人的神经网络有异曲同工之妙\nDropout\n\nDropout方法训练流程是这样：\n\n\n设置一个 dropout rate ：p%,也就是说在训练时每层会有 p% 的神经元被丢掉；\n\n\n然后每次更新参数都会重新采样丢掉的神经元。\n\n\nDropout的测试：\n测试期间所有的神经元保留，然后最后丢失率是多少，每个w 都要乘上（1 - 丢失率）。\n原理\ndropout可以解释为训练了一系列的神经网络，然后最后输出的值取了个平均\n\n如上图，dropout在所有的weight✖ (1-p%) 就取得了数个神经网络做平均的相近的结果。这也就是这个方法最神奇的地方。\nSummary\n在深度学习中遇到的问题分为 training data set  test set data 效果差\n在训练集效果差\n梯度弥散\n\nrelu 激活函数\nmaxout 激活函数\n\n学习率\n\nAdagrad\nRMSProp\n\n局部最小化\n\nMomentum 算法\nAdam\n\n在训练集效果好但是在测试集效果差(过拟合)\n\nEasy Stopping\nL1、L2正则化\nDropout\n"},"C_Research_Knowledge/深度学习入门系列一-梯度下降法-②":{"slug":"C_Research_Knowledge/深度学习入门系列一-梯度下降法-②","filePath":"C_Research_Knowledge/深度学习入门系列一-梯度下降法-②.md","title":"梯度下降法 ②_学习率","links":["C_Research_Knowledge/深度学习入门系列一-梯度下降法","C_Research_Knowledge/深度学习入门系列一-梯度下降法-②","C_Research_Knowledge/梯度下降法3","C_Research_Knowledge/深度学习入门系列4-反向传播BP算法","C_Research_Knowledge/深度学习入门系列-逻辑回归","C_Research_Knowledge/深度学习入门系列6-Convolution-Neural-Network-CNN-卷积神经网络","C_Research_Knowledge/深度学习入门系列7-Tips-For-DeepLearning-全程高能","C_Research_Knowledge/深度学习入门系列8-Tips-For-DeepLearning-2-全程高能"],"tags":["Optimization"],"content":"入门系列文章：\n深度学习入门系列一-梯度下降法\n深度学习入门系列一-梯度下降法-②\n梯度下降法3\n深度学习入门系列4-反向传播BP算法\n深度学习入门系列-逻辑回归\n深度学习入门系列6-Convolution-Neural-Network-CNN-卷积神经网络\n深度学习入门系列7-Tips-For-DeepLearning-全程高能\n深度学习入门系列8-Tips-For-DeepLearning-2-全程高能\n梯度下降法及其优化\n在上一篇文章中深度学习入门系列一-梯度下降法已经讲述了梯度下降法的基本流程，也讲了梯度下降法存在的问题，这篇文章就来继续讲解梯度下降法的后续。\n前一篇文章提到学习率的问题，如果学习率过大会导致震荡而难以收敛，过小的话又会收敛太慢，耗费时间，因此出现了Adagrad.\nTip1：Adaptive Learning Rate\n学习率对于参数调整影响很大，因此需要仔细地调整，手动调节参数肯定是不太现实，而自适应的调整参数有以下两个原则：\n\n开始阶段学习率比较大\n随着迭代次数的增加越来越小\n\nAdagrad Gradient Descent\n与Gradient Descent的比较\n\n数学公式推导\n原始学习率变成了 常数/梯度的平方和开根号，这个公式也是又推导过程的，如下(笔记字比较丑)：\n\n其中的矛盾\n观察最后一步，可以看出后面的  **梯度 ** 和  **分母的梯度的平方和开根号 ** （最后一行红笔画出来的）是矛盾的，梯度越大，分母也会越大，约束整个函数，但是效果会比较好。这个可以解释为，不只是考虑了一阶偏导数g(t),同时也考虑了二阶偏导数，所以结果会更加准确。(具体的可以参考李宏毅深度学习视频，前一篇文章有提到。)\nAdagrad总结\nAdagrad梯度下降法其实是实现了自适应的去调节学习率，不在需要手动的仔细去调节。\nTip2：Stochastic Gradient Descent\n随机梯度下降法也是比较常见的，是梯度下降法的一种变体，改变也不多，它与梯度下降法的不同之处在于：\n​\t\t假设有十组训练数据，也就是十个函数，梯度下降法会把每一组参数带进去计算损失求和，每组参数的梯度也相应会进行求和嘛，然后才更新一次参数；随机梯度下降法不再需要求和这一过程，即随便一组数据带入损失函数求出梯度直接更新参数。\n这种方法听起来也比较一般，它的主要优点在于更新参数快，  天下武功，唯快不破嘛。\nTip3：Feature scaling\n特征归一化。在做梯度下降法时，只要梯度分不一样，可以归一化之后在做，主要是因为如果某一个特征值非常大，那么它所占的比重就会非常大，这对学习非常不利。\n常用的一个归一化方法：\n对于特征 x1,x2,x3······xn,每种特征的某一个维度求一下这列数的均值和标准差，然后原始数据减去均值除以标准差即可归一化，如下图：\n\n这样做之后这一维数据就会服从均值为0，方差为1的高斯分布。\nCode:梯度下降法python实现\n背景：\n函数（model）：y = w1* x^2 + w2*x + b ,计算出参数 w1 w2 b，给顶的真值是w1_truth = 1.8，w2_truth=2.4，b_truth = 5.6\nloss function   ：均方误差，公式写出太麻烦，就是y的真实值减去y的预测值的平方和\n代码实现\n#写一个深度学习的回归案例\n#y = w1*x^2 + w2*x + b ,计算出参数 w1 w2 b\nimport numpy as np\n \n#生成数据 \nx_data = np.random.rand(10)\nw1_truth = 1.8\nw2_truth = 2.4\nb_truth = 5.6\ny_data = np.random.rand(10)\nfor i in range(10):\n    y_data[i] = w1_truth*x_data[i]*x_data[i] + w2_truth*x_data[i] + b_truth\nprint(y_data.shape)\nprint(x_data.shape)\n \n# 设置参数\nw1 = 6\nw2 = 4\nb = 8\nlr = 1\nsteps = 100000\nlr_w1 = 0\nlr_w2 = 0\nlr_b = 0\nfor epoch in range(steps):\n    w1_grad = 0\n    w2_grad = 0\n    b_grad = 0\n    for i in range(10):\n        w1_grad = w1_grad - 2*(y_data[i] - b-w1*x_data[i]*x_data[i]-w2*x_data[i])* (x_data[i]**2)\n        w2_grad = w2_grad - 2*(y_data[i] - b-w1*x_data[i]*x_data[i]-w2*x_data[i])* x_data[i]\n        b_grad = b_grad - 2*(y_data[i] - b-w1*x_data[i]*x_data[i]-w2*x_data[i])* 1.0\n    lr_w1 = lr_w1 + w1_grad**2\n    lr_w2 = lr_w2 + w2_grad**2\n    lr_b = lr_b + b_grad**2\n    # update\n    w1 = w1 - lr/np.sqrt(lr_w1) * w1_grad\n    w2 = w2 - lr/np.sqrt(lr_w2) * w2_grad\n    b  = b  - lr/np.sqrt(lr_b) * b_grad\nprint(&quot;w1= %f,w2 = %f, b= %f&quot; % (w1,w2,b))\n#以下是代码的输出结果，可以自己测试一下，完美找到了真实值\n# w1= 1.800000,w2 = 2.400000, b= 5.600000\n代码中的一些细节\n\n\n以上代码如果只用一个学习率，收敛会很慢，而且10万个迭代也不会迭代导最终结果，差距还是比较大的，因此代码用的是Adagrad梯度下降法，纯粹手工实现的，可以完美的预测出结果\n\n\n至于代码中的计算 w1 w2 b 的梯度公式，可以手工推导出来，如下图：\n\n\n"},"C_Research_Knowledge/深度学习入门系列一-梯度下降法":{"slug":"C_Research_Knowledge/深度学习入门系列一-梯度下降法","filePath":"C_Research_Knowledge/深度学习入门系列一-梯度下降法.md","title":"梯度下降法 ①_概述","links":["C_Research_Knowledge/深度学习入门系列一-梯度下降法","C_Research_Knowledge/深度学习入门系列一-梯度下降法-②","C_Research_Knowledge/梯度下降法3","C_Research_Knowledge/深度学习入门系列4-反向传播BP算法","C_Research_Knowledge/深度学习入门系列-逻辑回归","C_Research_Knowledge/深度学习入门系列6-Convolution-Neural-Network-CNN-卷积神经网络","C_Research_Knowledge/深度学习入门系列7-Tips-For-DeepLearning-全程高能","C_Research_Knowledge/深度学习入门系列8-Tips-For-DeepLearning-2-全程高能"],"tags":["Optimization"],"content":"入门系列文章：\n深度学习入门系列一-梯度下降法\n深度学习入门系列一-梯度下降法-②\n梯度下降法3\n深度学习入门系列4-反向传播BP算法\n深度学习入门系列-逻辑回归\n深度学习入门系列6-Convolution-Neural-Network-CNN-卷积神经网络\n深度学习入门系列7-Tips-For-DeepLearning-全程高能\n深度学习入门系列8-Tips-For-DeepLearning-2-全程高能\n我是研究CV方向的，但是深度学习我只是在学习Tensorflow书的时候学过里面的东西，很多东西讲的过于简单，理解并不直观，所以最近就在学习深度学习的视频，是李宏毅老师讲的，内容很生动，解决了我很多疑惑，附上视频地址→，看视频就要跟着老师推导一遍公式，也是理解了很多内在的东西。\nbilibili : www.bilibili.com/video/av48285039\nYoutube: www.youtube.com/watch\n概述：深度学习流程\n深度学习过程于机器学习类似，这里就先讲一下机器学习的，深度学习后面会讲到。对于一个特定的机器学习 Machine Learning任务，简单来说有以下固定的三个步骤：\n\n\n定义一系列的函数 ；\n\n\n评价函数的好坏，也就是定义损失函数；\n\n\n找出最好的函数(model)\n\n\n\n在定义好函数和确定损失函数之后，就要开始调整参数。这时候就需要用到梯度下降法了。\n梯度下降法概述\n调整参数就是为了师让损失函数最小，也就是选择出最好的函数 （Model）。先讲一下梯度下降法更新参数的真个流程。\n梯度\n首先，梯度Gradient，就是在这一点的微分（偏导数），也就是曲线在该点切线的斜率，高等数学就有讲过；不理解微分就可以当作是等高线图的法线方向。梯度代表了函数上升最快的方向，所以一个函数在一点求出梯度，然后按照梯度的反方向偏移就是减小最快的。\n所以这里也可以得知，首先需要一个可微的函数。\n流程\n\n\n\n假设对于参数w，要更新w，首先要初始化w的值，可以随机初始化一个值，当然也有其他方法这里并不关心\n\n\n计算损失函数 Loss function，L(·)对于参数w在该点处的偏导数，这就是函数在这一点的梯度。为什么是偏导数，因为参数不只是w一个，也有bias偏置b。\n\n\n计算出梯度就知道在那个方向上损失函数降幅最大，就开始更新参数，直到一个局部最小值。\nw_new = w_original - lr*w_original_graident\n\n\n重复 2、3 步骤，就可以不断调整参数\n\n\nNote :\n\n上述伪代码中有一个 lr参数，这是学习率Learning Rate，学习率决定了每次下降的步长，学习率影响还是比较大的，过小需要的时间太久，过大的话又会震荡不能收敛，这也是梯度下降法的一个存在的问题，后续会继续讲。\n上述第三步提到会函数会降低到局部最小值，这里整个是以线性函数为例，故不用考虑是否会降到局部最小值。\n\n\n关于梯度下降法，第二篇文章会继续讲解一些细节及其优化。"},"C_Research_Knowledge/深度学习实践":{"slug":"C_Research_Knowledge/深度学习实践","filePath":"C_Research_Knowledge/深度学习实践.md","title":"深度学习实践","links":[],"tags":["Code"],"content":"这里是自己实现过的深度学习代码，放上github地址：\ngithub.com/tzwx/DeepLearning\n会不断更新代码。\n1. Cartoon_GAN\n实现简单的GAN网络，生成动漫头像，这是DC-GAN的原型。\n\n网络结构：\n\n遗留的问题：\n\n关于反卷积，padding等参数设置现在依然未搞清楚\n\n2. Regression\n一个简单的回归案例，用到了Adagrad算法。原理可以看梯度下降法2 这篇文章。\n3. Back Propagation\n手动实现一个简单的反向传播BP算法，理清楚原理之后发现就是矩阵的连乘。具体原理可以参考反向传播bp算法这篇文章。\n4. My Tools\n实用工具类，我自己最常用的工具，包括图像、视频、姿态估计、可视化相关。\n已实现功能：\n\n\n 视频分解成图片\n\n\n 图片合成视频\n\n\n 图片裁剪\n\n\n 热图得到坐标\n\n\n 坐标生成热图\n\n\n 2D关键点可视化\n\n\n Json文件读写\n\n\n5. LSTM Cycle网络\n利用CycleLoss进行人体姿态估计。以下是大致的网络结构。\n\n\nLSTM_Cycle 项目情况\n\n\n项目初衷是做弱监督下的人体姿态估计，但是事实上并不是弱监督，事实上训练弱监督效果太差，一方面是idea本身比较弱，另一方面则可能是代码的问题，我也不知道是哪方面原因。\n\n\n这个项目是我做的第一个比较完整的项目，从0开始手撸，未用到现在任何的经典卷积网络，都是自己搭建起来的。\n\n\n但是现在由于种种原因可能要放弃了，所以在这里来记录一下原理，防止遗忘。\n\n"},"C_Research_Knowledge/牛顿法":{"slug":"C_Research_Knowledge/牛顿法","filePath":"C_Research_Knowledge/牛顿法.md","title":"牛顿法","links":[],"tags":["Optimization"],"content":"牛顿法一般有两个用途：方程求根；最优化。\n求根问题\n对于高次方程，求根公式要么没有要么就很复杂，此时可以用牛顿法来求解。首先根据泰勒公式，把f(x)在x0出展开到一阶（随机取一个x0求f(x)）：\nf(x) = f(x_0) + f&#039;(x_0)(x-x_0)\n令f(x) = 0得：\nx = x_0 - \\frac{f(x_0)}{f&#039;(x_0)} = x_1\n由于f(x)用泰勒公式展开到一阶，严格意义上展开式并不相等，只是可以说f(x_1)比f(x_0)更接近0 而已，由此迭代便自然而然了:\nx_{n+1} = x_n - f(x_n) / f&#039;(x_n)\n通过迭代，必然可以在f(x*) = 0 ,x=x*时收敛。也就是求出了根。\n把上面得泰勒公式展开求解迭代过程在几何表示如下图：\n\n这也是牛顿法的基本原理。\n优化问题\n前置知识\n海塞矩阵：是一个由多变量实值函数的所有二阶偏导数组成的方块矩阵。\n使用下标记号表示为：\nH_{ij} = \\frac{\\partial^2f}{\\partial_{xi}\\partial_{xj}}\n泰勒展开与海塞矩阵：\n\n流程\n\n这就是牛顿法更新的公式。此时下降最快的方向就是 海塞矩阵逆矩阵 * 梯度 \n思考梯度下降法与牛顿法\n收敛情况\n由求根迭代可以看出，牛顿法显然收敛速度比较快。\n\n牛顿法采用二阶海塞矩阵逆矩阵求解\n梯度下降法采用梯度求解\n\n通俗来说，二阶比一阶收敛更快，因为采用二阶逆矩阵求解，不仅考虑了梯度，也考虑了下一步的梯度，看得更远，所以收敛更快。（来自知乎）\n缺陷\n牛顿法既然比梯度下降法收敛快，那么为什么在深度学习中并未广泛应用，而是梯度下降法用的更多？原因大致如下：\n\n海塞矩阵难以求解，矩阵运算复杂度过高；\n我认为比较可接受得原因是牛顿法不具有普适性， 用自己得应用范围，而深度学习数据量大且复杂，则更应该采用普适性方法\n"},"C_Research_Knowledge/联合概率":{"slug":"C_Research_Knowledge/联合概率","filePath":"C_Research_Knowledge/联合概率.md","title":"联合概率","links":[],"tags":["概率论"],"content":"推荐一篇知乎文章：zhuanlan.zhihu.com/p/53005534\n联合概率\n联合概率P(x,y)表示同时满足x和y的概率，公式如下：\nP(x,y) = p(x)*p(y|x)\n对于多变量的，\nP(x,y,z) = P(x,y)*P(z|x,y) = P(x)*P(y|x)*p(z|x,y)\n同理，\nP(x_1,x_2,x_3,···，x_n) = P(x_1)*P(x_2|x_1)*P(x_3|x_1,x_2)*···*P(x_n|x_1,x_2,···x_{n-1})\n联合概率和条件概率\n联合概率P(x,y),x和y是对等的，没有相互依赖，可以理解为它的范围是x的所有取值和y的所有取值；\n条件概率p(x|y) 则是以y为条件，是限定在y的范围内，在寻找对应的x，它的范围更小"},"C_Research_Knowledge/马尔可夫链-Markov-Chain":{"slug":"C_Research_Knowledge/马尔可夫链-Markov-Chain","filePath":"C_Research_Knowledge/马尔可夫链-Markov-Chain.md","title":"马尔可夫链-Markov-Chain","links":[],"tags":["Markov"],"content":"首先推荐一篇文章：www.zhihu.com/question/26665048/answer/157852228\nBasic Theory\n马尔可夫是链随机过程中的一个过程。随机，就表示状态是不确定的，例如交通红绿灯就不适合用马尔可夫模型来计算。但是对于天气系统预测，马尔可夫可以用来建模、分析不同时间的天气状态，即使产生的结果不会完全准确我们也会接受这种假设。\n一个马尔科夫过程是状态间的转移仅依赖于前n个状态的过程。这个过程被称之为n阶马尔科夫模型，其中n是影响下一个状态选择的（前）n个状态。\n最简单的马尔科夫过程是一阶模型，它的状态选择仅与前一个状态有关，本文也着重讲解一阶马尔科夫链。\nCalculate\n对于有M个状态的一阶马尔可夫模型，共有M*M 个状态转移，因为任何一个状态都有可能是所有状态的下一个转移状态。\n每一个状态转移都有一个概率值，称为状态转移概率——这是从一个状态转移到另一个状态的概率。所有的M*M个概率可以用一个状态转移矩阵表示。注意这些概率并不随时间变化而不同——这是一个非常重要（但常常不符合实际）的假设。转移概率矩阵一个示意图如下图，有三种状态，所以有九种状态转移概率：\n\n计算时用当前状态 * 转移概率矩阵（矩阵乘法）,来从当前时刻状态计算出下一时刻状态。\n简单来说，一阶马尔可夫模型就是对联合概率做了简化。联合概率中：\nP(x_1,x_2,x_3,···，x_n) = P(x_1)*P(x_2|x_1)*P(x_3|x_1,x_2)*···*P(x_n|x_1,x_2,···x_{n-1})\n按时序信息理解即xn时刻的状态与前面所有的时刻都有关，而马尔科夫链则假设每一时刻状态仅仅与前一个时刻状态有关，即：\nP(X) = \\prod_{i=1}^n  P(x_i|x_{x-1})\nSummarize\n\n马尔可夫是一个随机过程\n假设当前状态仅仅与前一时间状态有关\n假设有着唯一的转移概率矩阵\n"},"C_Research_Knowledge/高斯混合模型":{"slug":"C_Research_Knowledge/高斯混合模型","filePath":"C_Research_Knowledge/高斯混合模型.md","title":"高斯混合模型","links":[],"tags":[],"content":"Gaussian Distribution\n首先复习一下高斯分布：\nN(x,\\mu,\\sigma) = \\frac{1}{\\sqrt {2\\pi\\sigma}}\\exp(-\\frac{(x-\\mu)^2}{2\\sigma^2})\n用高斯分布估计数据分布是有很大的局限的，它一定对称，只有一个峰值，这些使得它再估计真实数据时表现很乏力，因此引入了高斯混合模型。\nGaussion Mixture Model\n顾名思义，高斯混合模型就是很多个高斯模型混合在一起\n\n如图，黑色的分布就是下面彩色高斯分布混合而成。实际上就是很多高斯的加权和：\np(x) = \\sum_{k=1}^K W_k*g_k(x|\\mu_k,\\sigma_k)\n其中，\n\n\ng_k表示第k个高斯分布，具体公式就是上面列的高斯分布公式；\n\n\nw_k是它的权重系数，因此满足：\n\n\nW_k&gt;0 \\space\\space\\space\\space\\space\\space\\space\\space\\space\\space\\sum_{k=1}^K W_k = 1"},"Catalog":{"slug":"Catalog","filePath":"Catalog.md","title":"Catalog","links":["A_Navigation/Experimental-Tracking","A_Navigation/Paper-Related","A_Navigation/Research-Knowledge","A_Navigation/Technique-Explores","A_Navigation/Others","A_Navigation/Summary"],"tags":[],"content":"\n这是我的 Obsidian 知识库，构建于 2024-3-26。\n\n构建原则:\n\n放弃海量标签（确实没啥用）\n可以像 book 一样去索引一个东西，目录，这样可以解放复杂分类\n尝试重建联系\n\n\n按这个原则我去尝试一下，至少目前的状态我不喜欢，笔记就是给自己读的，不该这么难受，目录一个个去找，相反和 book 的 index 一样会比较方便。\n\n知识库目录\n一、Experimental Tracking\n二、Paper Related\n三、Research Knowledge\n四、Technique Explores\n五、Others\n六、Summary"},"D_Paper_Related/DBLP-检索式":{"slug":"D_Paper_Related/DBLP-检索式","filePath":"D_Paper_Related/DBLP 检索式.md","title":"DBLP 检索式","links":[],"tags":[],"content":"\n批量搜索指定期刊、日期、关键词的论文\n\n经常可以使用的匹配方法有：标题，作者，期刊（或会议）名称，年份，其对应的检索词分别为：title，author，venue和year。\n如果某项关键字可能包含多个单词的时候，包括下列情况：\n1、默认单词用空格间隔，表示单词同时出现，但是顺序无关，也可以使用下划线_分隔单词，如：\ntitle:voting_election\t\ntitle:voting election\t\n\n2、如果需要表达或的关系，可以使用|分隔单词，如：\nauthor:zhang|wang\t\n\n3、如果需要单词精确匹配，可以在词尾添加$，如：\ngraph$\n\n4、组合检索\ntitle:verifiable_privacy_voting year:2019 venue:comput author:zhang_thomas\n\n检索目标 题目包含三个单词：verfifiable, privacy和voting；文献属于2019年；期刊或会议名称包含comput单词（需要注意，有些期刊和会议使用了简称，所以事先最好搞清楚目标期刊在dblp的简称是什么）；作者包括了zhang和thomas两个单词"},"D_Paper_Related/Disentangled-Representation-for-Age-Invariant-Face-Recognition--A-Mutual-Information-Minimization-Perspective":{"slug":"D_Paper_Related/Disentangled-Representation-for-Age-Invariant-Face-Recognition--A-Mutual-Information-Minimization-Perspective","filePath":"D_Paper_Related/Disentangled Representation for Age-Invariant Face Recognition- A Mutual Information Minimization Perspective.md","title":"Disentangled Representation for Age-Invariant Face Recognition- A Mutual Information Minimization Perspective","links":["B_Experiments_Tracking/基于互信息的时序差分学习"],"tags":["Disentanglement"],"content":"\n做年龄无关的人脸识别 age-invariant face recognition (AIFR)，需要将人脸表征分解为 identity-dependent &amp; age-dependent components，解纠缠学习这两部分特征，且用互信息来监督学习到的特征；然后只使用身份特征进行人脸识别。\n差分建模\n\n\nWe obtain the age-related features through a FC layer from the initial features, and the identity-related features are obtained from the subtract between initial features and age-related features. 用减法得到解纠缠的特征表示\n互信息估计时，使用Contrastive Log-ratio Upper Bound (CLUB)，估计互信息的上界来进行最小化\n可以画特征解纠缠前后的分布图来证明解纠缠学习效果\n\n\n"},"D_Paper_Related/Dynamic-Context-Sensitive-Filtering-Network-for-Video-Salient-Object-Detection":{"slug":"D_Paper_Related/Dynamic-Context-Sensitive-Filtering-Network-for-Video-Salient-Object-Detection","filePath":"D_Paper_Related/Dynamic Context-Sensitive Filtering Network for Video Salient Object Detection.md","title":"Dynamic Context-Sensitive Filtering Network for Video Salient Object Detection","links":[],"tags":["CNN"],"content":"\n动态卷积\n\nGitHub - OIPLab-DUT/DCFNet: Dynamic Context-Sensitive Filtering Network for Video Salient Object Detection\n\n更复杂的卷积核生成方式；\n\n\nDCFM estimates the location-related afﬁnity weights by introducing matrix multiplication into the kernels’ generation process.\n其实矩阵乘法到卷积核的位置关联特征我不太懂，感觉没什么道理\n\n双向特征融合\n\n主要就是一个可学习的稀疏来加权两种特征\n整体网络结构特别复杂，感觉可以借鉴的不是很多"},"D_Paper_Related/H-vmunet---High-order-Vision-Mamba-UNet-for-Medical-Image-Segmentation":{"slug":"D_Paper_Related/H-vmunet---High-order-Vision-Mamba-UNet-for-Medical-Image-Segmentation","filePath":"D_Paper_Related/H-vmunet - High-order Vision Mamba UNet for Medical Image Segmentation.md","title":"H-vmunet - High-order Vision Mamba UNet for Medical Image Segmentation","links":[],"tags":[],"content":"论文信息\nTitle: H-vmunet: High-order Vision Mamba UNet for Medical Image Segmentation\nAuthors: RenkaiWu, YinghaoLiu, PengchenLiang, QingChang\n期刊:\n类别: preprint\nLevel:\n\n\nUrl: Open online\nzotero entry: Full Text PDF\nopen pdf: zotero\n\n\n\nAbstract:\nIn the field of medical image segmentation, variant models based on Convolutional Neural Networks (CNNs) and Visual Transformers (ViTs) as the base modules have been very widely developed and applied. However, CNNs are often limited in their ability to deal with long sequences of information, while the low sensitivity of ViTs to local feature information and the problem of secondary computational complexity limit their development. Recently, the emergence of state-space models (SSMs), especially 2D-selective-scan (SS2D), has had an impact on the longtime dominance of traditional CNNs and ViTs as the foundational modules of visual neural networks. In this paper, we extend the adaptability of SS2D by proposing a High-order Vision Mamba UNet (H-vmunet) for medical image segmentation. Among them, the proposed High-order 2D-selective-scan (H-SS2D) progressively reduces the introduction of redundant information during SS2D operations through higher-order interactions. In addition, the proposed Local-SS2D module improves the learning ability of local features of SS2D at each order of interaction. We conducted comparison and ablation experiments on three publicly available medical image datasets (ISIC2017, Spleen, and CVC-ClinicDB), and the results all demonstrate the strong competitiveness of H-vmunet in medical image segmentation tasks. The code is available from github.com/wurenkai/H-vmunet .\n概要\nUNET SS2D 有个 High-order 的 SSM Block\n方法\n\n\nImported: 2024-04-13 11:25 上午"},"D_Paper_Related/InternImage--Exploring-Large-Scale-Vision-Foundation-Models-with-Deformable-Convolutions":{"slug":"D_Paper_Related/InternImage--Exploring-Large-Scale-Vision-Foundation-Models-with-Deformable-Convolutions","filePath":"D_Paper_Related/InternImage- Exploring Large-Scale Vision Foundation Models with Deformable Convolutions.md","title":"InternImage- Exploring Large-Scale Vision Foundation Models with Deformable Convolutions","links":[],"tags":[],"content":"\nCNN vs Transformer：\n\nlong-range dependence\nadaptive spatial aggregation （transformer QKV都是依赖于输入进行计算的）\n\n分析可形变卷积，Offsets相当于长距离依赖，O、M的计算相当于自适应聚合\nDCNV3\n\n具体的卷积操作尚不清楚\n分组机制也不清楚\n"},"D_Paper_Related/Learning-Continuous-Image-Representation-with-Local-Implicit-Image-Function":{"slug":"D_Paper_Related/Learning-Continuous-Image-Representation-with-Local-Implicit-Image-Function","filePath":"D_Paper_Related/Learning Continuous Image Representation with Local Implicit Image Function.md","title":"Learning Continuous Image Representation with Local Implicit Image Function","links":[],"tags":[],"content":"Learning Continuous Image Representation with Local Implicit Image Function\n\n学习连续的图像表征方法，根据图片坐标及周围的特征预测RGB值\n\n\n因为连续，可以做任意分辨率超分\n隐式函数表示本质是用NN学习坐标到相应的信号的映射\n\nThe key idea of implicit neural representation is to represent an object as a function that maps coordinates to the corresponding signal (e.g. signed distance to a 3D object surface, RGB value in an image), where the function is parameterized by a deep neural network."},"D_Paper_Related/Local-Texture-Estimator-for-Implicit-Representation-Function":{"slug":"D_Paper_Related/Local-Texture-Estimator-for-Implicit-Representation-Function","filePath":"D_Paper_Related/Local Texture Estimator for Implicit Representation Function.md","title":"Local Texture Estimator for Implicit Representation Function","links":["B_Experiments_Tracking/Implicit-Representation"],"tags":[],"content":"GitHub - jaewon-lee-b/lte: Local Texture Estimator for Implicit Representation Function, in CVPR 2022\n\n受到Nerf启发，用 Implicit Neural Function 来表征图像，做频率估计\n\n\n任意分辨率高质量重建\n\nImplicit Representation"},"D_Paper_Related/Look-Back-and-Forth--Video-Super-Resolution-with-Explicit-Temporal-Difference-Modeling":{"slug":"D_Paper_Related/Look-Back-and-Forth--Video-Super-Resolution-with-Explicit-Temporal-Difference-Modeling","filePath":"D_Paper_Related/Look Back and Forth- Video Super-Resolution with Explicit Temporal Difference Modeling.md","title":"Look Back and Forth- Video Super-Resolution with Explicit Temporal Difference Modeling","links":[],"tags":[],"content":"\n做视频超分，显式建模直接上采样帧与GT的差异来实现，不同之处在于直接学习残差，没有用残差加到原图然后约束原图.\n\n未开源\n\n图片作差后使用卷积，阶段比较早，比 heatmap 作差来得早\n分离出 变化大[运动] 和 变化小 的区域, 分别使用不同的卷积 ｜ 不是所有的均使用多个空洞卷积\n低分辨率空间 和 高分辨率空间 的时间差分建模 （DeCoupling）\n\n"},"D_Paper_Related/Mamba-UNet---UNet-Like-Pure-Visual-Mamba-for-Medical-Image-Segmentation":{"slug":"D_Paper_Related/Mamba-UNet---UNet-Like-Pure-Visual-Mamba-for-Medical-Image-Segmentation","filePath":"D_Paper_Related/Mamba-UNet - UNet-Like Pure Visual Mamba for Medical Image Segmentation.md","title":"Mamba-UNet - UNet-Like Pure Visual Mamba for Medical Image Segmentation","links":["D_Paper_Related/U-shaped-Vision-Mamba-for-Single-Image-Dehazing","D_Paper_Related/Mamba-UNet---UNet-Like-Pure-Visual-Mamba-for-Medical-Image-Segmentation","D_Paper_Related/U-Mamba---Enhancing-Long-range-Dependency-for-Biomedical-Image-Segmentation"],"tags":["Mamba"],"content":"\nMamba-UNet 系列论文：\nU-shaped Vision Mamba for Single Image Dehazing\nMamba-UNet - UNet-Like Pure Visual Mamba for Medical Image Segmentation\nU-Mamba - Enhancing Long-range Dependency for Biomedical Image Segmentation\n\n论文信息\nTitle: Mamba-UNet: UNet-Like Pure Visual Mamba for Medical Image Segmentation\nAuthors: ZiyangWang, Jian-QingZheng, YichiZhang, GeCui, LeiLi\n期刊:\n类别: preprint\nLevel:\n\n\nUrl: Open online\nzotero entry: Full Text PDF\nopen pdf: zotero\n\n\n\nAbstract:\nIn recent advancements in medical image analysis, Convolutional Neural Networks (CNN) and Vision Transformers (ViT) have set significant benchmarks. While the former excels in capturing local features through its convolution operations, the latter achieves remarkable global context understanding by leveraging self-attention mechanisms. However, both architectures exhibit limitations in efficiently modeling long-range dependencies within medical images, which is a critical aspect for precise segmentation. Inspired by the Mamba architecture, known for its proficiency in handling long sequences and global contextual information with enhanced computational efficiency as a State Space Model (SSM), we propose Mamba-UNet, a novel architecture that synergizes the U-Net in medical image segmentation with Mamba’s capability. Mamba-UNet adopts a pure Visual Mamba (VMamba)-based encoder-decoder structure, infused with skip connections to preserve spatial information across different scales of the network. This design facilitates a comprehensive feature learning process, capturing intricate details and broader semantic contexts within medical images. We introduce a novel integration mechanism within the VMamba blocks to ensure seamless connectivity and information flow between the encoder and decoder paths, enhancing the segmentation performance. We conducted experiments on publicly available MRI cardiac multi-structures segmentation dataset. The results show that Mamba-UNet outperforms UNet, Swin-UNet in medical image segmentation under the same hyper-parameter setting. The source code and baseline implementations are available.\n概要\nCode：github.com/ziyangwang007/Mamba-UNet\n纯 Mamba 结构\n以 SwinUNet 结构为模板，替换了其中的 building block， 用 SS2D，来源于（VMamba）。\n方法\n\n\nImported: 2024-04-02 2:52 下午"},"D_Paper_Related/MedMamba---Vision-Mamba-for-Medical-Image-Classification":{"slug":"D_Paper_Related/MedMamba---Vision-Mamba-for-Medical-Image-Classification","filePath":"D_Paper_Related/MedMamba - Vision Mamba for Medical Image Classification.md","title":"MedMamba - Vision Mamba for Medical Image Classification","links":["D_Paper_Related/VMamba---Visual-State-Space-Model"],"tags":[],"content":"论文信息\nTitle: MedMamba: Vision Mamba for Medical Image Classification\nAuthors: YubiaoYue, ZhenzhangLi\n期刊:\n类别: preprint\nLevel:\n\n\nUrl: Open online\nzotero entry: Full Text PDF\nopen pdf: zotero\n\n\n\nAbstract:\nMedical image classification is a very fundamental and crucial task in the field of computer vision. These years, CNN-based and Transformer-based models have been widely used to classify various medical images. Unfortunately, The limitation of CNNs in long-range modeling capabilities prevents them from effectively extracting features in medical images, while Transformers are hampered by their quadratic computational complexity. Recent research has shown that the state space model (SSM) represented by Mamba can efficiently model long-range interactions while maintaining linear computational complexity. Inspired by this, we propose Vision Mamba for medical image classification (MedMamba). More specifically, we introduce a novel Conv-SSM module. Conv-SSM combines the local feature extraction ability of convolutional layers with the ability of SSM to capture long-range dependency, thereby modeling medical images with different modalities. To demonstrate the potential of MedMamba, we conducted extensive experiments using 14 publicly available medical datasets with different imaging techniques and two private datasets built by ourselves. Extensive experimental results demonstrate that the proposed MedMamba performs well in detecting lesions in various medical images. To the best of our knowledge, this is the first Vision Mamba tailored for medical image classification. The purpose of this work is to establish a new baseline for medical image classification tasks and provide valuable insights for the future development of more efficient and effective SSM-based artificial intelligence algorithms and application systems in the medical. Source code has been available at github.com/YubiaoYue/MedMamba.\n概要\n用 Mamba 做分类。感觉 Mamba 在医学图像很火 [这个领域比较水吧]\nSSM 用的 SS2D 模型，感觉这个很适合做基础模块 VMamba - Visual State Space Model\n方法\n\nImported: 2024-04-13 11:24 上午"},"D_Paper_Related/Motion-Guided-Attention-for-Video-Salient-Object-Detection":{"slug":"D_Paper_Related/Motion-Guided-Attention-for-Video-Salient-Object-Detection","filePath":"D_Paper_Related/Motion Guided Attention for Video Salient Object Detection.md","title":"Motion Guided Attention for Video Salient Object Detection","links":[],"tags":[],"content":"\n从光流图提取运动特征；多任务；新的运动特征融合方法\n\nGitHub - lhaof/Motion-Guided-Attention: Motion Guided Attention for Video Salient Object Detection, ICCV 2019\n\n蓝色方块表示Appearance；绿色方块表示Motion；\n上面就是几种不同的计算Attention然后进行特征融合的方法\n\n似乎挺有意思，后续详细读一哈"},"D_Paper_Related/Not-All-Tokens-Are-Equal--Human-centric-Visual-Analysis-via-Token-Clustering-Transformer":{"slug":"D_Paper_Related/Not-All-Tokens-Are-Equal--Human-centric-Visual-Analysis-via-Token-Clustering-Transformer","filePath":"D_Paper_Related/Not All Tokens Are Equal- Human-centric Visual Analysis via Token Clustering Transformer.md","title":"Not All Tokens Are Equal- Human-centric Visual Analysis via Token Clustering Transformer","links":[],"tags":["Transformer"],"content":"GitHub - zengwang430521/TCFormer: The codes for TCFormer in paper: Not All Tokens Are Equal: Human-centric Visual Analysis via Token Clustering Transformer\n\nVIT中的Token一般是图像patch转换来的，所有token比重相同，本文提出基于聚类的Token，自适应调整Token的shape， size\n\n\nClustering-based Token Merge (CTM) Block generates vision tokens of various locations, sizes, and shapes for each image by progressive clustering and merging tokens.\nMulti-stage Token Aggregation (MTA) head aggregates token features in multiple stages\n\n\n\n在Transformer Block 中去掉了位置编码，使用 depth-wise 卷积层来获得位置特征\n聚类算法\nWhole Body Pose Estimation\n"},"D_Paper_Related/Recognize-Actions-by-Disentangling-Components-of-Dynamics":{"slug":"D_Paper_Related/Recognize-Actions-by-Disentangling-Components-of-Dynamics","filePath":"D_Paper_Related/Recognize Actions by Disentangling Components of Dynamics.md","title":"Recognize Actions by Disentangling Components of Dynamics","links":["B_Experiments_Tracking/基于互信息的时序差分学习"],"tags":[],"content":"\n解纠缠的视频表征学习，建模短期内的动态信息，分别根据多种特征分别预测，然后做动作识别，CVPR 2018，时间较早\n差分建模\n\nCode：未开源\n\n\n将视频表征划分为三方面：外观特征，视觉运动，外观变化。首先Concat连续帧提取浅层特征，然后分别学习三种解耦合的特征\n\n外观特征，Convolution + Temporal Pooling\n视觉运动（Object / Camera Motion），构造基于 cost volume（即两帧像素的相似度，不过是一个像素与一个区域的匹配，H×W×(2∆H+1)×(2∆W+1) ，一个图是HW个像素，每个像素与一个区域里(2∆H+1)×(2∆W+1)个像素的匹配） **的运动表示，最终采用每个像素的偏移量作为运动信息（Optical-FLow，H×W×2）\n外观变化，特指由光照变化等非运动因素引起的外观变化，做法上可以理解为先对齐两帧，然后相减，做法合理，我们以前有过类似做法，但是没有合理解释\n\n\n\n这种划分看上去很合理，但是三种信息直接分别预测最终结果然后再平均有点草率\n感觉可以结合Temporal Difference，维护一个多粒度运动场（Multi-Level Motion Field），迭代式更新关节的预测（补全）"},"D_Paper_Related/SCI-写作常用高级词法":{"slug":"D_Paper_Related/SCI-写作常用高级词法","filePath":"D_Paper_Related/SCI 写作常用高级词法.md","title":"SCI 写作常用高级词法","links":[],"tags":["writting"],"content":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nabstracted / out输出的access访问accumulation积累，集聚achieves promising performance with  beyond 0.5 improvements in terms of mAP over a, b, c, d, on posetrack17性能提高advance / propose / come up with /  present提出aggravate加重Aided辅助ambiguities模棱两可的ample /adequate丰富的Analogous / Similar to相似的annoying讨厌的 令人烦恼的Anticipating / Predicting / Forecasting预测arbitrary任意的are handed to被交给arise from / stem from /derive from源于，由什么引起as the network goes deeper随着网络加深assess评估at scale大规模地attend to / focus on / concentrate on  / endeavor on / delves into关注于,致力于barely / only仅仅be aligned with与xxx相同be exposed to被执行bring 12.4 mAP improvement带来性能提升broader广大的built upon基于xx建立built-in内置的by a large margin大幅度by nature天生的calibration / correct校准canonical经典的capably能干地characterizing / denotes / indicates /  represent / symbolize描述，刻画 / 代表cherry pick做出最佳选择circumstances / environment / case /  configuration / landscape环境，情况，配置cluttered凌乱的coarsely粗糙的codec编码解码器coincide with符合，一致collecting-distributing集散式compact / compactness紧密compliant / good顺从的compromised妥协的concise简洁的concise简明的，简洁的concise / simple简单的Concretely/particularly/Specifically具体地conforms to / in line with our intuitions  / in accordance with our expectations / ensures the validity of our  assumptions of / be aligned with / coincide with符合，一致consistently / invariably一贯地 / 始终如一地constitute组成contend / advocate主张，声称 / 提倡corrupted毁坏的，损坏的counterparts副本Coupling / Combining组合，链接de facto实际上的delivering / achieving / providing递交，实现，提供depicted / illustrated描述deploy部署，采用deteriorates / degradation / reduce /  diminish / decrease / degrade / sacrificing / inhibits恶化,降低，减少，抑制detrimental有害的devise设计difficulties / obstacles困难disambiguating cues消除歧义的线索disparity / poorly aligned不一致distractors干扰项drastically大幅度的，彻底的，激烈的dubbed / termed / namely被称为，命名为effectiveness / efficient / efficacy效果好(连续) / 效率高(连续) / 生效(离散，0 &amp; 1)efforts have been directed to努力做xxxembodied使具像化，具体表达emphasize强调engaged /capitalizes on/harness/impose利用，使用entities实体equipped with具有，具备era阶段，期间，时代erroneously错误地feasible可行的fine-grained细粒度地follow-up后续的foster / promote / facilitate / provoke /  stimulates促进，激发 / 引起fraction of frames很少的帧，一小部分帧From this perspective从这个角度看fulfill / achieve / accomplish / obtain实现，完成，获得gathers xxx to / aggregate /  integrate聚合，收集go-to model首选模型grasp抓牢grasp the useful information掌握有用信息great threat to极大影响了，对…有威胁heterogeneous混杂的，多种多样的heterogeneous多样化的hierarchical分层的high level / order高阶的holistic整体的Ideally理想的，观念上的image corruptions毁坏image corruptions / deteriorates /  degraded恶化，退化的impedes阻碍，妨碍impressive / superior /good / competitive  / compelling / admirable / leading / state-of-the-art极好的，印象深刻的 / 有竞争力的 / 极其出色的 / 最先进的improving the state-of-the-art results by  a large margin大大提高了最先进的结果In a nutshell简而言之in an effort to / aim to企图，目标是In light of that考虑到这一点in part在某种程度上in turn轮流inferior / sub-optimal / deteriorate /  degeneration / fatal / deviated / impeded / subpar / adversely / detrimental差的，不好的/致命的，偏离的, 阻碍，不利地informed见多识广的，有根据的inhibits抑制，阻止insights洞察力；眼力；深刻见解intractable / tricky / difficult /  challenging /adversely棘手的/苦难的/赋有挑战的/有害地intrinsic / inherent / essential本质的，固有的invoke / bring调用，引发，带来landmark标志merits优点mine开采mitigate缓和mitigate / alleviate减轻，缓解，缓和model-agnostic与模型无关monitor监视multiple granularities多粒度natural coherence自然一致性negatively消极地non-trivial / admirable意义非凡的,极其出色的Notwithstanding / Although尽管off-the-shelf现成的orthogonal正交的out-of-box开箱即用out-of-distribution data分布之外的over- and under-exposure曝光过度/不足paradigm / regime / mechanism范式 / 组织方法 / 机制parameter budget参数预算，负载participate参与，使用phenomenon现象plasticity可塑性plausible看似可信的points out a promising way to指出一条有希望的路，使xxx成为可能predominantly主要地preferred / better更好的prepend预先考虑；使用prerequisite先决条件problems mentioned above can be settled  thereof上述问题可以解决propose / advocate提倡，建议pure纯粹的pursue追求，得到pushes the performance boundary of human  pose estimation提高了上限Recent years have witnessed fast progress近年来进步很快recipe食谱；方法，手段，程序Reciprocally反之，相反地remedied纠正resort to doing… / virtue of凭借,依靠于xxxresultant / final / resulting结果的 / 最终的retrieved恢复reveal / uncover / shed light on doing暴露，揭示sacrificing牺牲-减少，降低salient显著的seamlessly无缝地seldom很少seldom很少sheds light on阐明sophisticated/ complex复杂的start by… followed by…以什么作为开端，然后xxxstretches拉伸，提高strive to / aim at努力做，试图做xxxsuperfluous/ redundancy多余的superseded / replace取代，克制，替换symbolize象征；采用符号代表systematization体系化take a deeper look at深入研究take into account考虑take/accept .. as inputs ; / feed into /  handed to / go through /processed以什么作为输入；馈入；交给；经过；处理template bank模板库temporal dynamics embedded across  consecutive frames嵌入在连续帧的时间动态To conquer this dilemma为了克服这种困境to some extent在一定程度上tractable易处理的Trivially / common / modest平平无奇的，正常的ubiquitous / prevalent / common普遍的Under rigorously theoretical guarantee在严格的理论保证下under some circumstances / In cases在某些情况下variance方差viable可望成功的，切实可行的；能存活的，能生长发育的vice versa反之亦然vital / key / crucial / nontrivial /  meaningful关键的, 至关重要的vulnerable（robust的反义词）/ susceptible易受攻击的，脆弱的when presented with inputs当输入为xxxWith a new and strong observation that,  …, we propose a …鉴于一个新的观察, 我们提出了xxxWithout bells and whistles / vanilla / plain没有花里胡哨的proficient / adept擅长的，熟练的prohibitive昂贵的，令人望而却步的de-facto实际上的"},"D_Paper_Related/STM--SpatioTemporal-and-Motion-Encoding-for-Action-Recognition":{"slug":"D_Paper_Related/STM--SpatioTemporal-and-Motion-Encoding-for-Action-Recognition","filePath":"D_Paper_Related/STM- SpatioTemporal and Motion Encoding for Action Recognition.md","title":"STM- SpatioTemporal and Motion Encoding for Action Recognition","links":[],"tags":[],"content":"Code：未开源\n\n同时提取了 时空特征 和 运动特征 ，做动作识别，网络结构比较简单\n\n\nChannel-wise SpatioTemporal Module + Channel-wise Motion Module\n\n\n\n结构上感觉无需借鉴，均在深度方向进行卷积，融合时序信息\n\n"},"D_Paper_Related/Searching-Central-Difference-Convolutional-Networks-for-Face-Anti-Spooﬁng":{"slug":"D_Paper_Related/Searching-Central-Difference-Convolutional-Networks-for-Face-Anti-Spooﬁng","filePath":"D_Paper_Related/Searching Central Difference Convolutional Networks for Face Anti-Spooﬁng.md","title":"Searching Central Difference Convolutional Networks for Face Anti-Spooﬁng","links":[],"tags":["CNN"],"content":"\n差分卷积，提取一致性的细粒度信息\n\nCode：虽然已经开源，但是issue提问精度难以复现\nGitHub - ZitongYu/CDCN: Central Difference Convolutional Networks (CVPR’20)\n\n\n常规卷积：\n  ![[Pasted image 20240326173535.png]]\n\n\n\n差分卷积\n\n\n"},"D_Paper_Related/Self-Constrained-Inference-Optimization-on-Structural-Groups-for-Human-Pose-Estimation":{"slug":"D_Paper_Related/Self-Constrained-Inference-Optimization-on-Structural-Groups-for-Human-Pose-Estimation","filePath":"D_Paper_Related/Self-Constrained Inference Optimization on Structural Groups for Human Pose Estimation.md","title":"Self-Constrained Inference Optimization on Structural Groups for Human Pose Estimation","links":["B_Experiments_Tracking/关节结构化约束"],"tags":[],"content":"\n做图片姿态估计，针对 人体关节的结构化约束 &amp; 高低置信度关键点预测 提出解决方案。\n结构化\n\nMotivation &amp; 基本观点:\n\nWe observe that human poses exhibit strong group-wise structural correlation and spatial coupling between keypoints due to the biological constraints of different body parts. This group-wise structural correlation can be explored to improve the accuracy and robustness of human pose estimation.\n\n代码未开源\n\n要点：\n整体方法可以分为两部分，分别从训练和测试阶段进行理解比较方便，前提是先进行关键点分组。\n\n关节分组：\n\n按人体结构对关节进行分组，划分为6组，每组4个关节\n手动划分置信度高低的点：在每个组内靠近躯干的点认为是base的点，一个边缘的点认为是端点 terminal keypoints\n\n\n\n\n\nTraining: 用 prediction-verification 网络结构来学习关节内部结构化关系\n\n首先基于HRNET拿到初始的 heatmap &amp; features\n训练预测网络：以一个关节组为例，用三个base关节的heatmap以及feature输入预测网络，推理出terminal 关节的heatmap；把预测网络的输出与其他输入堆叠作为验证网络的输入，来预测`第一个base关节的heatmap，这样可以形成一个闭环，同时约束两个网络的输出；训练期间冻结验证网络\n训练验证网络：反一下上述过程就行，先验证再预测，loss约束从pipeline图可以看到\n\n\nTesting：在推理期间优化低置信度的关键点（就是优化Terminal Keypoint）\n\n送来一组测试sample，先用预测网络预测出关节组里面端点的heatmap\n划定一个范围，对heatmap加上扰动，进行采样，\n将该heatmap以及相应的输入输入到验证网络进行反向预测，得到第一个base关节的heatmap，优化即保证将所选择的预测的terminal关节的heatmap输入验证网络后所得结果能与HRNet预测的初始的结果loss最小\nHere, the basic idea is that: if the prediction X D becomes accurate during local search, then, using it as the input, the verification network should be able to accurately predict the high-confidence keypoint H A , which implies that the self-constraint loss || H A − H A || 2 on the high-confidence keypoint X A should be small.\n\n\n\n反思\n\n这篇论文整体挺复杂的，所提的这种cycle-like的学习方法来学习关节关系比较有趣，但是论文的说法** Self-Constrained Learning** 与一般的自监督学习并不一样\nheatmap加上扰动再重采样这个做法确实不错，看上去可以实现测试期间的优化\n有个问题是直接自己定义好了低置信度的关节，这篇论文避开了这个关键问题。按理来说应该每次都能判断出哪些预测的关节精度较低，而不是手动定义，这个可以作为后续扩展方向\n"},"D_Paper_Related/Spatio-Temporal-Representation-Factorization-for-Video-based-Person-Re-Identification":{"slug":"D_Paper_Related/Spatio-Temporal-Representation-Factorization-for-Video-based-Person-Re-Identification","filePath":"D_Paper_Related/Spatio-Temporal Representation Factorization for Video-based Person Re-Identification.md","title":"Spatio-Temporal Representation Factorization for Video-based Person Re-Identification","links":[],"tags":[],"content":"\n这篇论文提出的一些思想、说法很有趣，但是做法就比较普通了，可以作为备选的做法之一。\n\n创新：\n\n将序列3D特征进行时间分解和空间分解，在时间上分解为静态特征和动态特征，在空间上分解为全局和局部特征。\n采用计算Attention Mask的方式来实现\n\n\n优缺点&amp;改进利用\n\n采用Attention Mask方式来实现不同特征的过滤，但是缺乏针对性的设计以及监督，采用end-to-end的方式并无法保证能提取到这些特征。Attention Mask的实现方式倒是可以作为一种可行做法\n所有的特征都是 intuitional 的，刚好加上互信息监督\n"},"D_Paper_Related/TDN--Temporal-Difference-Networks-for-Efﬁcient-Action-Recognition":{"slug":"D_Paper_Related/TDN--Temporal-Difference-Networks-for-Efﬁcient-Action-Recognition","filePath":"D_Paper_Related/TDN- Temporal Difference Networks for Efﬁcient Action Recognition.md","title":"TDN- Temporal Difference Networks for Efﬁcient Action Recognition","links":["B_Experiments_Tracking/基于互信息的时序差分学习"],"tags":[],"content":"\n学习时序差分特征作动作识别，网络结构平平无奇，很朴素，但是复杂\n差分建模\n\n**Code Link： github.com/MCG-NJU/TDN**\nGitHub - MCG-NJU/TDN: [CVPR 2021] TDN: Temporal Difference Networks for Efficient Action Recognition\n\n时序差分建模网络，输入为图片的差，捕获视频的运动表征 （直接concat连续帧效果一般般）*\n分别建模 外观 + 运动表征 ， 用相加操作融合*\n短期运动模式加长期的\n稀疏采样，把视频分成多个segment，每个片段随机取一帧，以此采样多帧\n\n"},"D_Paper_Related/TEA--Temporal-Excitation-and-Aggregation-for-Action-Recognition":{"slug":"D_Paper_Related/TEA--Temporal-Excitation-and-Aggregation-for-Action-Recognition","filePath":"D_Paper_Related/TEA- Temporal Excitation and Aggregation for Action Recognition.md","title":"TEA- Temporal Excitation and Aggregation for Action Recognition","links":["B_Experiments_Tracking/基于互信息的时序差分学习"],"tags":[],"content":"\n偏向做网络结构设计，与SENet有点类似，做cross-frame的通道attention，同时有点像Residual Step Block，聚合多帧信息，总体来说网络结构一般新\n差分建模\n\nCode：未开源\n\n聚合Temporal 信息会用一个 1D Conv ，在通道方向聚合，这个操作可以学习\n\n"},"D_Paper_Related/TEINet--Towards-an-Efﬁcient-Architecture-for-Video-Recognition":{"slug":"D_Paper_Related/TEINet--Towards-an-Efﬁcient-Architecture-for-Video-Recognition","filePath":"D_Paper_Related/TEINet- Towards an Efﬁcient Architecture for Video Recognition.md","title":"TEINet- Towards an Efﬁcient Architecture for Video Recognition","links":["B_Experiments_Tracking/基于互信息的时序差分学习"],"tags":[],"content":"\nThe TEI module presents a different paradigm to learn temporal features by decoupling the modeling of channel correlation and temporal interaction.\n差分建模\n\nCode：未开源\n\n网络包含两个模块：\n\nMotion Enhanced Module（MEM） 相当于提了一个结合difference的通道Attention模块，用另外一帧对当前帧重新进行权重调整；\n**Temporal Interaction Module（TIM）**对不同帧进行channel-wise的融合 [感觉channel方向融合1*1卷积并不是最佳选择，例如分析optical flow 和 Deformable Convolution的那篇]\n\n综合一下就是先根据邻居帧对当前帧进行调整，然后再融合"},"D_Paper_Related/TF-Blender--Temporal-Feature-Blender-for-Video-Object-Detection":{"slug":"D_Paper_Related/TF-Blender--Temporal-Feature-Blender-for-Video-Object-Detection","filePath":"D_Paper_Related/TF-Blender- Temporal Feature Blender for Video Object Detection.md","title":"TF-Blender- Temporal Feature Blender for Video Object Detection","links":[],"tags":[],"content":"\n一种新的多帧特征聚合方式，更充分的对每帧进行增强；建模当前帧与其他帧的关系权重\n\nGitHub - goodproj13/TF-Blender\nDespite achieving improvements in detection, existing methods focus on the selection of higher-level video frames for aggregation rather than modeling lower-level temporal relations to increase the feature representation.\n\n三个步骤：\n\n建模当前帧与每个辅助帧的时序关系；\n建模每个辅助帧与其他辅助帧的关系以增强该辅助帧；\n组合上述结果进行特征聚合\n\n\n\n感觉有点工程化，充分建模各种相关关系；称为“Temporal Relation”"},"D_Paper_Related/U-Mamba---Enhancing-Long-range-Dependency-for-Biomedical-Image-Segmentation":{"slug":"D_Paper_Related/U-Mamba---Enhancing-Long-range-Dependency-for-Biomedical-Image-Segmentation","filePath":"D_Paper_Related/U-Mamba - Enhancing Long-range Dependency for Biomedical Image Segmentation.md","title":"U-Mamba - Enhancing Long-range Dependency for Biomedical Image Segmentation","links":["D_Paper_Related/U-shaped-Vision-Mamba-for-Single-Image-Dehazing","D_Paper_Related/Mamba-UNet---UNet-Like-Pure-Visual-Mamba-for-Medical-Image-Segmentation","D_Paper_Related/U-Mamba---Enhancing-Long-range-Dependency-for-Biomedical-Image-Segmentation"],"tags":["Mamba"],"content":"\nMamba-UNet 系列论文：\nU-shaped Vision Mamba for Single Image Dehazing\nMamba-UNet - UNet-Like Pure Visual Mamba for Medical Image Segmentation\nU-Mamba - Enhancing Long-range Dependency for Biomedical Image Segmentation\n\n论文信息\nTitle: U-Mamba: Enhancing Long-range Dependency for Biomedical Image Segmentation\nAuthors: JunMa, FeifeiLi, BoWang\n期刊:\n类别: preprint\nLevel:\n\n\nUrl: Open online\nzotero entry: Full Text PDF\nopen pdf: zotero\n\n\n\nAbstract:\nConvolutional Neural Networks (CNNs) and Transformers have been the most popular architectures for biomedical image segmentation, but both of them have limited ability to handle long-range dependencies because of inherent locality or computational complexity. To address this challenge, we introduce U-Mamba, a general-purpose network for biomedical image segmentation. Inspired by the State Space Sequence Models (SSMs), a new family of deep sequence models known for their strong capability in handling long sequences, we design a hybrid CNN-SSM block that integrates the local feature extraction power of convolutional layers with the abilities of SSMs for capturing the long-range dependency. Moreover, U-Mamba enjoys a self-configuring mechanism, allowing it to automatically adapt to various datasets without manual intervention. We conduct extensive experiments on four diverse tasks, including the 3D abdominal organ segmentation in CT and MR images, instrument segmentation in endoscopy images, and cell segmentation in microscopy images. The results reveal that U-Mamba outperforms state-of-the-art CNN-based and Transformer-based segmentation networks across all tasks. This opens new avenues for efficient long-range dependency modeling in biomedical image analysis. The code, models, and data are publicly available at wanglab.ai/u-mamba.html.\n概要\nCode: wanglab.ai/u-mamba.html\n这篇是首次 Mamba + Unet (CNN)，应用于医学图像分割。学习局部特征+长距离依赖。\n方法\n\nImported: 2024-04-02 2:26 下午"},"D_Paper_Related/U-shaped-Vision-Mamba-for-Single-Image-Dehazing":{"slug":"D_Paper_Related/U-shaped-Vision-Mamba-for-Single-Image-Dehazing","filePath":"D_Paper_Related/U-shaped Vision Mamba for Single Image Dehazing.md","title":"U-shaped Vision Mamba for Single Image Dehazing","links":["D_Paper_Related/U-shaped-Vision-Mamba-for-Single-Image-Dehazing","D_Paper_Related/Mamba-UNet---UNet-Like-Pure-Visual-Mamba-for-Medical-Image-Segmentation","D_Paper_Related/U-Mamba---Enhancing-Long-range-Dependency-for-Biomedical-Image-Segmentation"],"tags":["Mamba"],"content":"\nMamba-UNet 系列论文：\nU-shaped Vision Mamba for Single Image Dehazing\nMamba-UNet - UNet-Like Pure Visual Mamba for Medical Image Segmentation\nU-Mamba - Enhancing Long-range Dependency for Biomedical Image Segmentation\n\n论文信息\nTitle: U-shaped Vision Mamba for Single Image Dehazing\nAuthors: ZhuoranZheng, ChenWu\n期刊:\n类别: preprint\nLevel:\n\n\nUrl: Open online\nzotero entry: Full Text PDF\nopen pdf: zotero\n\n\n\nAbstract:\nCurrently, Transformer is the most popular architecture for image dehazing, but due to its large computational complexity, its ability to handle long-range dependency is limited on resource-constrained devices. To tackle this challenge, we introduce the U-shaped Vision Mamba (UVM-Net), an efficient single-image dehazing network. Inspired by the State Space Sequence Models (SSMs), a new deep sequence model known for its power to handle long sequences, we design a Bi-SSM block that integrates the local feature extraction ability of the convolutional layer with the ability of the SSM to capture long-range dependencies. Extensive experimental results demonstrate the effectiveness of our method. Our method provides a more highly efficient idea of long-range dependency modeling for image dehazing as well as other image restoration tasks. The URL of the code is \\url{github.com/zzr-idam/UVM-Net}. Our method takes only \\textbf{0.009} seconds to infer a 325 \\times 325 resolution image (100FPS) without I/O handling time.\n概要\nCode: github.com/zzr-idam/UVM-Net\n用 UNet形状包裹起来的Mamba 网络，首次提出用于去雾的 vision mamba。\nCNN+SSM，局部特征+全局依赖性\n方法\n\nImported: 2024-04-02 1:45 下午"},"D_Paper_Related/VM-UNET-V2-Rethinking-Vision-Mamba-UNet-for-Medical-Image-Segmentation":{"slug":"D_Paper_Related/VM-UNET-V2-Rethinking-Vision-Mamba-UNet-for-Medical-Image-Segmentation","filePath":"D_Paper_Related/VM-UNET-V2 Rethinking Vision Mamba UNet for Medical Image Segmentation.md","title":"VM-UNET-V2 Rethinking Vision Mamba UNet for Medical Image Segmentation","links":["D_Paper_Related/VM-UNet---Vision-Mamba-UNet-for-Medical-Image-Segmentation"],"tags":[],"content":"论文信息\nTitle: VM-UNET-V2 Rethinking Vision Mamba UNet for Medical Image Segmentation\nAuthors: MingyaZhang, YueYu, LimeiGu, TingshengLin, XianpingTao\n期刊:\n类别: preprint\nLevel:\n\n\nUrl: Open online\nzotero entry: Full Text PDF\nopen pdf: zotero\n\n\n\nAbstract:\nIn the field of medical image segmentation, models based on both CNN and Transformer have been thoroughly investigated. However, CNNs have limited modeling capabilities for long-range dependencies, making it challenging to exploit the semantic information within images fully. On the other hand, the quadratic computational complexity poses a challenge for Transformers. Recently, State Space Models (SSMs), such as Mamba, have been recognized as a promising method. They not only demonstrate superior performance in modeling long-range interactions, but also preserve a linear computational complexity. Inspired by the Mamba architecture, We proposed Vison Mamba-UNetV2, the Visual State Space (VSS) Block is introduced to capture extensive contextual information, the Semantics and Detail Infusion (SDI) is introduced to augment the infusion of low-level and high-level features. We conduct comprehensive experiments on the ISIC17, ISIC18, CVC-300, CVC-ClinicDB, Kvasir, CVC-ColonDB and ETIS-LaribPolypDB public datasets. The results indicate that VM-UNetV2 exhibits competitive performance in medical image segmentation tasks. Our code is available at github.com/nobodyplayer1/VM-UNetV2.\n概要\n前置工作：VM-UNet - Vision Mamba UNet for Medical Image Segmentation\n改进：加了一个SDI 模块，类似于 HRNet 的融合\n\n方法\n\nImported: 2024-04-09 7:44 晚上"},"D_Paper_Related/VM-UNet---Vision-Mamba-UNet-for-Medical-Image-Segmentation":{"slug":"D_Paper_Related/VM-UNet---Vision-Mamba-UNet-for-Medical-Image-Segmentation","filePath":"D_Paper_Related/VM-UNet - Vision Mamba UNet for Medical Image Segmentation.md","title":"VM-UNet - Vision Mamba UNet for Medical Image Segmentation","links":["D_Paper_Related/Vision-Mamba---Efficient-Visual-Representation-Learning-with-Bidirectional-State-Space-Model"],"tags":[],"content":"论文信息\nTitle: VM-UNet: Vision Mamba UNet for Medical Image Segmentation\nAuthors: JiachengRuan, SunchengXiang\n期刊:\n类别: preprint\nLevel:\n\n\nUrl: Open online\nzotero entry: Full Text PDF\nopen pdf: zotero\n\n\n\nAbstract:\nIn the realm of medical image segmentation, both CNN-based and Transformer-based models have been extensively explored. However, CNNs exhibit limitations in long-range modeling capabilities, whereas Transformers are hampered by their quadratic computational complexity. Recently, State Space Models (SSMs), exemplified by Mamba, have emerged as a promising approach. They not only excel in modeling long-range interactions but also maintain a linear computational complexity. In this paper, leveraging state space models, we propose a U-shape architecture model for medical image segmentation, named Vision Mamba UNet (VM-UNet). Specifically, the Visual State Space (VSS) block is introduced as the foundation block to capture extensive contextual information, and an asymmetrical encoder-decoder structure is constructed. We conduct comprehensive experiments on the ISIC17, ISIC18, and Synapse datasets, and the results indicate that VM-UNet performs competitively in medical image segmentation tasks. To our best knowledge, this is the first medical image segmentation model constructed based on the pure SSM-based model. We aim to establish a baseline and provide valuable insights for the future development of more efficient and effective SSM-based segmentation systems. Our code is available at github.com/JCruan519/VM-UNet.\n概要\nVSSBlock Vision Mamba - Efficient Visual Representation Learning with Bidirectional State Space Model + UNet\n方法\n没有什么新的\n\nImported: 2024-04-02 11:07 晚上"},"D_Paper_Related/VMamba---Visual-State-Space-Model":{"slug":"D_Paper_Related/VMamba---Visual-State-Space-Model","filePath":"D_Paper_Related/VMamba - Visual State Space Model.md","title":"VMamba - Visual State Space Model","links":["D_Paper_Related/Vision-Mamba---Efficient-Visual-Representation-Learning-with-Bidirectional-State-Space-Model","D_Paper_Related/VMamba---Visual-State-Space-Model"],"tags":["Mamba"],"content":"\nMamba Backbone 系列论文\nVision Mamba - Efficient Visual Representation Learning with Bidirectional State Space Model\nVMamba - Visual State Space Model\n\n论文信息\nTitle: VMamba: Visual State Space Model\nAuthors: YueLiu, YunjieTian, YuzhongZhao, HongtianYu, LingxiXie, YaoweiWang, QixiangYe, YunfanLiu\n期刊:\n类别: preprint\nLevel:\n\n\nUrl: Open online\nzotero entry: Liu 等 - 2024 - VMamba Visual State Space Model.pdf\nopen pdf: zotero\n\n\n\nAbstract:\nConvolutional Neural Networks (CNNs) and Vision Transformers (ViTs) stand as the two most popular foundation models for visual representation learning. While CNNs exhibit remarkable scalability with linear complexity w.r.t. image resolution, ViTs surpass them in fitting capabilities despite contending with quadratic complexity. A closer inspection reveals that ViTs achieve superior visual modeling performance through the incorporation of global receptive fields and dynamic weights. This observation motivates us to propose a novel architecture that inherits these components while enhancing computational efficiency. To this end, we draw inspiration from the recently introduced state space model and propose the Visual State Space Model (VMamba), which achieves linear complexity without sacrificing global receptive fields. To address the encountered direction-sensitive issue, we introduce the Cross-Scan Module (CSM) to traverse the spatial domain and convert any non-causal visual image into order patch sequences. Extensive experimental results substantiate that VMamba not only demonstrates promising capabilities across various visual perception tasks, but also exhibits more pronounced advantages over established benchmarks as the image resolution increases. Source code has been available at github.com/MzeroMiko/VMamba.\n概要\n这篇也是做视觉表征学习的 Backbone， 类似前面的 Vision Mamba。\n一个重点创新：图像是非因果性的，不能直视从前到后scan 一遍去建模。所以提出 4 个方向建模法，也就是 Cross-Scan Module (CSM)：**\n\n\n方法\n\n结构类似于 SwinTransformer\nImported: 2024-04-02 3:43 下午"},"D_Paper_Related/Vision-Mamba---Efficient-Visual-Representation-Learning-with-Bidirectional-State-Space-Model":{"slug":"D_Paper_Related/Vision-Mamba---Efficient-Visual-Representation-Learning-with-Bidirectional-State-Space-Model","filePath":"D_Paper_Related/Vision Mamba - Efficient Visual Representation Learning with Bidirectional State Space Model.md","title":"Vision Mamba - Efficient Visual Representation Learning with Bidirectional State Space Model","links":["D_Paper_Related/Vision-Mamba---Efficient-Visual-Representation-Learning-with-Bidirectional-State-Space-Model","D_Paper_Related/VMamba---Visual-State-Space-Model"],"tags":["Mamba"],"content":"\nMamba Backbone 系列论文\nVision Mamba - Efficient Visual Representation Learning with Bidirectional State Space Model\nVMamba - Visual State Space Model\n\n论文信息\nTitle: Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model\nAuthors: LianghuiZhu, BenchengLiao, QianZhang, XinlongWang, WenyuLiu, XinggangWang\n期刊:\n类别: preprint\nLevel:\n\n\nUrl: Open online\nzotero entry: Full Text PDF\nopen pdf: zotero\n\n\n\nAbstract:\nRecently the state space models (SSMs) with efficient hardware-aware designs, i.e., the Mamba deep learning model, have shown great potential for long sequence modeling. Meanwhile building efficient and generic vision backbones purely upon SSMs is an appealing direction. However, representing visual data is challenging for SSMs due to the position-sensitivity of visual data and the requirement of global context for visual understanding. In this paper, we show that the reliance on self-attention for visual representation learning is not necessary and propose a new generic vision backbone with bidirectional Mamba blocks (Vim), which marks the image sequences with position embeddings and compresses the visual representation with bidirectional state space models. On ImageNet classification, COCO object detection, and ADE20k semantic segmentation tasks, Vim achieves higher performance compared to well-established vision transformers like DeiT, while also demonstrating significantly improved computation &amp; memory efficiency. For example, Vim is 2.8\\times faster than DeiT and saves 86.8% GPU memory when performing batch inference to extract features on images with a resolution of 1248\\times1248. The results demonstrate that Vim is capable of overcoming the computation &amp; memory constraints on performing Transformer-style understanding for high-resolution images and it has great potential to be the next-generation backbone for vision foundation models. Code is available at github.com/hustvl/Vim.\n概要\nCode： github.com/hustvl/Vim\n提出纯 SSM Backbone，主要包括两个重点设计：\n\n双向 Mamba 建模，相当于前向处理输入 + 逆向处理输入 — 原始的只能单向处理\n给 visual token 加上了位置编码，实现位置感知 — 原始 mamba 处理一位数据，不能感知位置\n\n方法\nImage2Token → Mamba Encoder → MLP Projection\n\n几个重要影响性能的：\n\nSSM 之前的Conv1D很重要\n分类 Token 放到中间效果好\n\n\nImported: 2024-04-01 4:06 下午"},"D_Paper_Related/index":{"slug":"D_Paper_Related/index","filePath":"D_Paper_Related/index.md","title":"index","links":["D_Paper_Related/写作基础-and-总结","D_Paper_Related/SCI-写作常用高级词法","D_Paper_Related/论文创新点观察","D_Paper_Related/英文论文写作查重","D_Paper_Related/工具合集整理","D_Paper_Related/DBLP-检索式","A_Navigation/Experimental-Tracking","D_Paper_Related/Self-Constrained-Inference-Optimization-on-Structural-Groups-for-Human-Pose-Estimation","D_Paper_Related/Look-Back-and-Forth--Video-Super-Resolution-with-Explicit-Temporal-Difference-Modeling","D_Paper_Related/Local-Texture-Estimator-for-Implicit-Representation-Function","D_Paper_Related/Learning-Continuous-Image-Representation-with-Local-Implicit-Image-Function","D_Paper_Related/TDN--Temporal-Difference-Networks-for-Efﬁcient-Action-Recognition","D_Paper_Related/STM--SpatioTemporal-and-Motion-Encoding-for-Action-Recognition","D_Paper_Related/TEA--Temporal-Excitation-and-Aggregation-for-Action-Recognition","D_Paper_Related/TEINet--Towards-an-Efﬁcient-Architecture-for-Video-Recognition","D_Paper_Related/Recognize-Actions-by-Disentangling-Components-of-Dynamics","D_Paper_Related/Searching-Central-Difference-Convolutional-Networks-for-Face-Anti-Spooﬁng","D_Paper_Related/InternImage--Exploring-Large-Scale-Vision-Foundation-Models-with-Deformable-Convolutions","D_Paper_Related/Dynamic-Context-Sensitive-Filtering-Network-for-Video-Salient-Object-Detection","D_Paper_Related/Motion-Guided-Attention-for-Video-Salient-Object-Detection","D_Paper_Related/TF-Blender--Temporal-Feature-Blender-for-Video-Object-Detection","D_Paper_Related/Not-All-Tokens-Are-Equal--Human-centric-Visual-Analysis-via-Token-Clustering-Transformer","D_Paper_Related/Spatio-Temporal-Representation-Factorization-for-Video-based-Person-Re-Identification","D_Paper_Related/Disentangled-Representation-for-Age-Invariant-Face-Recognition--A-Mutual-Information-Minimization-Perspective","D_Paper_Related/Vision-Mamba---Efficient-Visual-Representation-Learning-with-Bidirectional-State-Space-Model","D_Paper_Related/VMamba---Visual-State-Space-Model","D_Paper_Related/U-Mamba---Enhancing-Long-range-Dependency-for-Biomedical-Image-Segmentation","D_Paper_Related/Mamba-UNet---UNet-Like-Pure-Visual-Mamba-for-Medical-Image-Segmentation","D_Paper_Related/U-shaped-Vision-Mamba-for-Single-Image-Dehazing","D_Paper_Related/VM-UNet---Vision-Mamba-UNet-for-Medical-Image-Segmentation","D_Paper_Related/VM-UNET-V2-Rethinking-Vision-Mamba-UNet-for-Medical-Image-Segmentation"],"tags":[],"content":"\nWriting\n\n写作基础&amp;总结\nSCI 写作常用高级词法\n论文创新点观察\n英文论文写作查重\n工具合集整理\nDBLP 检索式\n\n\nReading\n\n这里不会所有的都放，大概率会放到Experimental Tracking\n\nHuman Pose Estimation\n\nSelf-Constrained Inference Optimization on Structural Groups for Human Pose Estimation\n\nSuper-Resolution\n\nLook Back and Forth- Video Super-Resolution with Explicit Temporal Difference Modeling\nLocal Texture Estimator for Implicit Representation Function\nLearning Continuous Image Representation with Local Implicit Image Function\n\nAction Recognition\n\nTDN- Temporal Difference Networks for Efﬁcient Action Recognition\nSTM- SpatioTemporal and Motion Encoding for Action Recognition\nTEA- Temporal Excitation and Aggregation for Action Recognition\nTEINet- Towards an Efﬁcient Architecture for Video Recognition\nRecognize Actions by Disentangling Components of Dynamics\n\nConvolution\n\nSearching Central Difference Convolutional Networks for Face Anti-Spooﬁng\nInternImage- Exploring Large-Scale Vision Foundation Models with Deformable Convolutions\n\nObject Detection\n\nDynamic Context-Sensitive Filtering Network for Video Salient Object Detection\nMotion Guided Attention for Video Salient Object Detection\nTF-Blender- Temporal Feature Blender for Video Object Detection\n\nHuman-centric Visual Analysis\n\nNot All Tokens Are Equal- Human-centric Visual Analysis via Token Clustering Transformer\n\nPerson Re-Identification\n\nSpatio-Temporal Representation Factorization for Video-based Person Re-Identification\n\nFace Recognition\n\nDisentangled Representation for Age-Invariant Face Recognition- A Mutual Information Minimization Perspective\n\nMamba\n\nVision Mamba - Efficient Visual Representation Learning with Bidirectional State Space Model\nVMamba - Visual State Space Model\nU-Mamba - Enhancing Long-range Dependency for Biomedical Image Segmentation\nMamba-UNet - UNet-Like Pure Visual Mamba for Medical Image Segmentation\nU-shaped Vision Mamba for Single Image Dehazing\nVM-UNet - Vision Mamba UNet for Medical Image Segmentation\nVM-UNET-V2 Rethinking Vision Mamba UNet for Medical Image Segmentation\n"},"D_Paper_Related/写作基础-and-总结":{"slug":"D_Paper_Related/写作基础-and-总结","filePath":"D_Paper_Related/写作基础&总结.md","title":"写作基础&总结","links":[],"tags":["writting"],"content":"写作基础&amp;总结\n\n原则:\n\n写作要站在 读者 的角度来写，而不是作者的角度，以 reviewer 的角度来看；\n论文不能锋芒太强，容易有负面印象；\n每个很小的东西要考虑读者懂不懂，即使前面已经提过也要考虑是否再次提示；\n故事要讲完整，且故事前后要串起来，逻辑不能有漏洞；\n实验越充分越好\n时常问自己Motivation对后期写作很有利\n\n\n\n关于写作时间分配\n\n于我而言，需要大把时间来写，集中大块时间写，不分心则会效率很高。\n论文越不写越不想写，因此不能拖，越早完成越好。\n\n关于中英文使用\n\n可以中文打草稿，写逻辑框架，英文具体在写。\n\n基本格式规范\n\n\n一、命名规范：\n论文建好一个latex工程，先按照此模板建立目录结构，命名规范便于引用、修改\n\n图片 命名：\n新建 Figures 文件夹，图片放在文件夹下；\n图片以 Fig1-PTM.jpg 方式明命，即 序号+描述\n文件 命名：\ncvpr.tex 会议/期刊名称.tex\ncvpr_supplementary.tex 补充材料 会议/期刊名称_supplementary.tex\nReferences.bib 参考文献 bib 文件命名\n\n二、写作格式规范\n写作要规范统一，任何一个小问题都很致命，例如图片引用、空格问题等\n\n空格问题\n每两个单词之间、两句话间要有一个空格，不多不少\n\\\\cite{} 前后有空格\n加粗问题\n粗体表示强调，粗体不能随意使用；注意统一问题\n简写格式\n举例说明 e.g. 后面要跟 逗号，空格，用斜体包裹\n等等 etc. 后面小点不能省\n解释说明 i.e., 后面的逗号，前后空格，用斜体包裹\n\nkeypoints (\\\\emph{e.g.}, wrist, ankle, etc.)\n\\\\emph{i.e.,}\nw.r.t. % 关于\n\n图片表格的引用\n图的引用 Fig.~\\\\ref{fig:results} Fig. 后面要加空格 Fig. 1,\n表的引用 Table 1 Table加空格，不加点， 后面跟引用\n\nFig.~\\\\ref{fig:results}, in Table \\\\ref{tab3} and Table \\\\ref{tab4},\n三、斜体的使用\n除了上述提到的斜体的使用，斜体恰当使用可以方便reviewer阅读，起到拔高作用\n\n（强调）\n\n（并列）\n四、图的描述\n如果论文图中用一个小图代替一个网络，则需要在 caption 中讲明白，网络细节将在之后提出，让reviewer不要太纠结。\n五、表格的描述\n表格的caption应放在表格上面；图的caption则放在下方\nTips: 期刊中其他注意事项\n\n期刊论文中的 Index Terms\n\nIndex Terms-Pose estimation，Multi person，Sparsely labelde，occlusion，Convolutional neural networks\nIndex Terms 为paper检索服务，关键词可以检索到该论文, 更多应考虑读者搜索习惯，用段词作为索引词\n\n\nLatex相关\n\nSpringer 模板碰到引用文献的坑\nLatex相关问题\n\nTips: 论文提升\n\n\n看写作好的论文，找出比较好的几句话，几个词法描述，用到论文中。\n保持句式一致性，同一句话 主动、被动用相同的语法，方便阅读\n修饰词不能超过4个\n"},"D_Paper_Related/工具合集整理":{"slug":"D_Paper_Related/工具合集整理","filePath":"D_Paper_Related/工具合集整理.md","title":"工具合集整理","links":[],"tags":[],"content":"\n多个浏览器收藏夹都有东西，且safria浏览器收藏夹不是很好用，所以这里类似一个合集，常用的或是可能用到的有用的站点收藏。\n暂定划分为：\n\n 投稿相关\n 文献相关\n 中英文论文写作相关\n 科研绘图相关\n 一些软件记录\n\n\n投稿相关\n\n\n整理论文/专利投稿相关的资源：\n\n会议｜期刊查询\n投稿系统\n专利检索\n投过的期刊整理\n时间换算（投稿的deadline通常为太平洋时间，需要换算）*\n\n\n会议｜期刊查询\n\nLetpub 查询会议、期刊、自然科学基金等\nGuide2Research 计算机学科TOP会议期刊排名\n\n投稿系统\n\nConference Management Tookit (Microsoft CMT) 主流会议投稿，如CVPR、ICCV、ECCV、AAAI、IJCAI等\n\n专利检索\n\n专利查询网站 （新版）\n\n投过的期刊整理\n\nIntertional Journal of Intelligent Systems SCI 中科院/JCR一区 IF:8.7\nTOMM-ACM Transactions on Multimedia Computing, Communications and Application CCF-B期刊 多媒体领域顶刊\nMultimedia Systems SCI 四区\nIEEE Transactions on Circuits and Systems for Video Technology\n\nSubmission\nTracking\n\n\n\n时间换算工具\n\n太平洋时间与北京时间换算\n\n文献相关\n\n\n论文阅读整理相关的资源，包括：\n\n论文查找下载\n论文整理/阅读\n\n\n论文查找下载\n\nGoogle Scholar 谷歌学术 彩蛋 彩蛋\ndblp 某个会议批量论文查询\nArxiv 各种发表未发表的论文\nGIthub 论文仓库\n\nCVPR历年论文 极市团队整理\nICCV历年论文极市团队整理\n更新…\n\n\nSCI-Hub 论文下载\n\n论文整理阅读软件\n\nZotero 整理论文 按文件夹分类挺好用的\nPDF Expert Mac下PDF阅读\n\n中英文写作相关\n\n\n中文写作主要为毕业论文，英文为期刊或会议论文，具体相关内容包括：\n\n英文写作常用\n写作工具问题\n相关文章/视频整理\n\n\n英文写作常用\n\nDeepL 翻译软件 可以换词汇换句式\nGramer 检查语法错误\n其他\n\n科研绘图\n\n论文绘图工具及相关用法资源\n\n常用软件\n\nPPT\n\n在线画图工具\n\n在线画图工具ProcessOn\n在线画图工具Draw.io\n"},"D_Paper_Related/英文论文写作查重":{"slug":"D_Paper_Related/英文论文写作查重","filePath":"D_Paper_Related/英文论文写作查重.md","title":"英文论文写作查重","links":[],"tags":["writting"],"content":"一、latex生成的pdf文件，先在该网站查询字数\nDocument Manager - PlagScan\n二、到淘宝根据字数买账号\n\n亲亲购买的是20000字Turnitin单次查重（卡密2小时有效，限查一次，超过时间禁止登入否则会收录），请发文章字数给客服核对~超出字数需补差价，避免被收录（每1500字/8元）。 请到官网登入，别用搜的，很多假官网。 网址：www.turnitin.com/login_page.asp\nTurnitin查重教程 m.weibo.cn/status/4354235263129260\n赠送Grammarly检查语法2小时 网址：www.grammarly.com [右上角login，超时禁止登录]\n提示：提交后显示“处理中”等待几分钟刷新网页\n查重价格：\n查重2000字以内 拍 8 元 2001～4000字拍 16 元 4001～6000字拍 24 6001～8000字拍 32 元 8001～10000字拍 40 元 10001～12000字拍 48 元 12001～14000字拍 56 元 14001～16000字拍 64 元 16001～18000字拍 72 元 18001～20000字拍 80 元 依此类推\n请根据对应字数拍对应价格/补差价\n不收录不收录不收录 不留痕不留痕不留痕 提交学校不影响不影响 2-5分钟出结果"},"D_Paper_Related/论文创新点观察":{"slug":"D_Paper_Related/论文创新点观察","filePath":"D_Paper_Related/论文创新点观察.md","title":"论文创新点观察","links":[],"tags":["writting"],"content":"此处罗列多篇cvpr2021论文的创新点以观察规律。\n优秀论文创新点整理\n\n\nCVPR (2021) - Human Pose Estimation\n\nBottom-Up Human Pose Estimation Via Disentangled Keypoint Regression\n\nWe argue that the representations for regressing the positions of the keypoints accurately need to focus on the keypoint regions. (A New Observation)\nThe proposed DEKR approach is able to learn disentangled representations through two simple schemes, adaptive convolutions and multi-branch structure, so that each representation focuses on one keypoint region and the prediction of the corresponding keypoint position from such representation is accurate. （Advantages, Proposed Network）\nThe proposed direct regression approach outperformskeypoint detection and grouping schemes and achieves new state-of-the-art bottom-up pose estimation results on the benchmark datasets, COCO and CrowdPose. （Experimental Results - SOTA）\n\nLite-HRNet: A Lightweight High-Resolution Network\n\nWe simply apply the shuffle blocks to HRNet, leading a lightweight network naive Lite-HRNet. We empirically show superior performance over MobileNet, ShuffleNet, and Small HRNet. （Proposed Network）\nWe present an improved efficient network, Lite-HRNet. The key point is that we introduce an efficient conditional channel weighting unit to replace the costly 1 × 1 convolution in shuffle blocks, and the weights are computed across channels and resolutions. （Proposed Network）\nLite-HRNet is the state-of-the-art in terms of complexity and accuracy trade-off on COCO and MPII human pose estimation and easily generalized to semantic segmentation task. （Experimental Results - SOTA）\n\nPose Recognition with Cascade Transformers *\n\nWe propose a regression-based human pose recognition method by building cascade Transformers, based on a general-purpose object detector, end-to-end object detection Transformer (DETR). Our method, named pose recognition Transformer (PRTR), enjoys the tokenized representation in Transformers with layers of selfattention to capture the joint spatial and appearance modeling for the keypoints. (Proposed Network, Summarize)\nTwo types of cascade Transformers have been developed: 1). a two-stage one with the second Transformer taking image patches detected from the first Transformer, as shown in Figure 2; and 2). a sequential one using spatial Transformer network (STN) to create an end-to-end framework, shown in Figure 3. (Proposed Network, Respective, Detail)\nWe visualize the distribution of keypoint queries in various aspects to unfold the internal process of the Transformer for the gradual refinement of the detection. (Explainability)\n结果没达到 SOTA 故没体现在创新中\n\nRethinking the Heatmap Regression for Bottom-up Human Pose Estimation *\n\nTo the best of our knowledge, this is the first paper that focuses on the problems in heatmap regression when tackling large variance of human scales and labeling ambiguities. We attempt to alleviate these problems by scale and uncertainty prediction. （New Problem）\nWe propose a scale-adaptive heatmap regression (SAHR), which can adaptively adjust the standard deviation of the Gaussian kernel for each keypoint, enabling the model to be more tolerant of various human scales and labeling ambiguities. (Proposed Network, Main)\nWe propose a weight-adaptive heatmap regression (WAHR) to alleviate the severe imbalance between foreground and background samples. It could automatically focus more on relatively harder examples and fully exploit the superiority of SAHR. (Another Work)\nOur model outperforms the state-of-the-art model by 1.5 AP and achieves 72.0 AP on COCO test-dev2017, which is comparable with the performances of most top-down methods. （Experimental Results - SOTA）\n\nUnsupervised Human Pose Estimation through Transforming Shape Templates *\n\nWe introduce a conceptually simple but effective method to learn 2D human-interpretable keypoints based on transforming a single manually defined 2D template. (Proposed Method)\nOur proposed approach is capable of performing 2D human pose estimation without any additional need for labeled data, either paired or unpaired. (Advantages)\nWe demonstrate the high adaptability of our approach by evaluating it on benchmark data and in the wild on a challenging infant pose estimation dataset. （Experimental Results - SOTA）\n\nWhen Human Pose Estimation Meets Robustness: Adversarial Algorithms and Benchmarks\n\nWe propose three robust benchmarks COCO-C, MPIIC, and OCHuman-C, and demonstrate that both topdown and bottom-up pose estimators suffer severe performance drop on corrupted images, drawing the community’s attention to this problem. (New Benchmarks)\nWith extensive experiments, we have many interesting conclusions that would help improve the accuracy and robustness of future works. (New Observations)\nWe propose a novel adversarial data augmentation method together with knowledge distillation, termed AdvMix, which is model-agnostic and easy-toimplement. It significantly improves the robustness of pose estimation models while maintaining or slightly improving the performance on the clean data, without extra inference computational overhead. (Proposed Method)\n\n\nConclusion\n\nNew Observation\nNew Network/Method (必要，两个网络 / 一个网络，一个优点)\nAdvantages\nExperimental Results (必要，若没有可用其他代替)\nExplainability\nNew Benchmark\nNew Problem\n"},"E_Technique_Explores/AECC2020中英文切换":{"slug":"E_Technique_Explores/AECC2020中英文切换","filePath":"E_Technique_Explores/AECC2020中英文切换.md","title":"AECC2020中英文切换","links":[],"tags":["软件"],"content":"D:\\Adobe\\Adobe After Effects 2020\\Support Files\\AMT\\application.xml\n方案一：搜索zh_CN 改成 en_US\n&lt;Data key=&quot;AMTConfigPath&quot;&gt;D:\\\\Adobe\\\\Adobe After Effects 2020\\\\Support Files\\\\AMT\\\\application.xml&lt;/Data&gt;\n&lt;Data key=&quot;APPLICATION_CUSTOMIZATION_DATA&quot;&gt;SuppressSerialWF&lt;/Data&gt;&lt;Data key=&quot;BridgeTalkCode&quot;&gt;/adobe/bridgetalk/aftereffects-17.0&lt;/Data&gt;&lt;Data key=&quot;DefaultDriverPayload&quot;&gt;{AEFT-17.0-64-ADBEADBEADBEADBEADBEADB}&lt;/Data&gt;&lt;Data key=&quot;InstallDate&quot;&gt;1606458079&lt;/Data&gt;&lt;Data key=&quot;LicensingCode&quot;&gt;V7{}AfterEffects-170-Win-GM&lt;/Data&gt;&lt;Data key=&quot;ProductVersion&quot;&gt;17.0&lt;/Data&gt;&lt;Data key=&quot;SAPCode&quot;&gt;AEFT&lt;/Data&gt;&lt;Data key=&quot;defaultAdobeCode&quot;&gt;{AEFT-17.0-64-ADBEADBEADBEADBEADBEADB}&lt;/Data&gt;&lt;Data key=&quot;driverAMTConfigPath&quot;&gt;D:\\\\Adobe\\\\Adobe After Effects 2020\\\\Support Files\\\\AMT\\\\application.xml&lt;/Data&gt;&lt;Data key=&quot;driverLEID&quot;&gt;V7{}AfterEffects-170-Win-GM&lt;/Data&gt;&lt;Data key=&quot;installedLanguages&quot;&gt;en_US&lt;/Data&gt;&lt;/Payload&gt;\n方案二：搜索不到，加一句配置代码：\n&lt;Payload&gt;&lt;Data key=&quot;installedLanguages&quot;&gt;en_US&lt;/Data&gt;&lt;/Payload&gt;"},"E_Technique_Explores/DeepFaceLab-操作":{"slug":"E_Technique_Explores/DeepFaceLab-操作","filePath":"E_Technique_Explores/DeepFaceLab 操作.md","title":"DeepFaceLab 操作","links":[],"tags":["换脸","软件"],"content":"一些资料：\n1. 目录结构\n# 文件目录结构\n \ntarget_image_dir = &quot;/workspace/data_dst&quot; # \n \nsource_image_dir = &quot;/workspace/data_src&quot; # 谁被替换就是dst，替换来源是src\n \n# === 图片命名：00001.jpg (%5d) ===\n \n2. 操作流程\n1)初始化文件夹（点击后将清空workspace文件夹）\n\n\nsrc视频转图片\n\n\ndst视频转图片.bat\n\n\na) full face\n(简称F脸，额头部分有些许被裁到)\nb) whole face\n(简称WF脸，范围更大，整个额头都取了，兼容WF和F脸模型)\nc) head\n(不常用，给高玩做avatar用，萌新用不到)\n\n提取src头像.bat\n\n4.1) 查看src头像.bat 删除错误的图片\n\ndst头像提取.bat\n\n5.1) 查看dst头像.bat 删除错误的图片\n\n训练SAEHD模型.bat\n\n一般加载现有模型，然后预训练选择N\n\n\n应用SAEHD模型\n\n\n合成 MP4 视频 merged to mp4\n\n\n3. 总结\n\nsrc有眼镜则支持不好\ndst有眼镜需要手动调整区域\n"},"E_Technique_Explores/JavaWeb简要总结-Mysql-JDBC":{"slug":"E_Technique_Explores/JavaWeb简要总结-Mysql-JDBC","filePath":"E_Technique_Explores/JavaWeb简要总结-Mysql-JDBC.md","title":"JavaWeb简要总结--Mysql,JDBC","links":[],"tags":["Website"],"content":"Summarize \t总结给自己看，泛泛学过去很多东西记不住，这里记录一下最常用的东西，最核心的。\nMysql 极简操作\n\nMysql 首次安装登录出现问题，\nERROR 1045 (28000): Access denied for user ‘ODBC’@‘localhost’ (using password: YES)\n解决问题见：blog.csdn.net/tiankongbubian/article/details/77119751\n适用于 windows 环境\n\n\n常用SQL命令\n分类：DML,DQL,DCL,DDL\n\nData Definition Language (DDL数据定义语言) 如：建库，建表\nData Manipulation Language(DML数据操纵语言)，如：对表中的记录操作增删改\nData Query Language(DQL 数据查询语言)，如：对表中的查询操作\nData Control Language(DCL 数据控制语言)，如：对用户权限的设置\n\nmysql -uroot -proot //登录mysql\nshow databases; -- 显示数据库\n \nuse test;\n \nshow tables; -- 显示表\n \ncreate database 库名; -- 建库\n \ndrop database 库名; -- 删库\n \ndrop table if exists `create`;\n/*\n创建表实例\n*/\nCREATE TABLE student3 ( \n    id INT, \n\tNAME VARCHAR(20), \n\tage INT, \n\tsex VARCHAR(5), \n\taddress VARCHAR(100), \n\tmath INT,  \n\tenglish INT\n);  \n \n-- 修改表相关\nalter table 表名 rename to 新的表名; -- 1. 修改表名\n\t\t\nalter table 表名 character set 字符集名称; -- 2. 修改表的字符集\n\t\t\nalter table 表名 add 列名 数据类型; -- 3. 添加一列\n\t\t\nalter table 表名 change 列名 新列别 新数据类型; -- 4. 修改列名称 类型\nalter table 表名 modify 列名 新数据类型;\n\t\nalter table 表名 drop 列名; -- 5. 删除列\n \n增删改表数据：\n-- 添加记录\ninsert into 表名(列名1,列名2,...列名n) values(值1,值2,...值n); -- 指定列\ninsert into 表名 values(值1,值2,...值n); -- 所有列\n-- 删除记录\ndelete from 表名 [where 条件]\n TRUNCATE TABLE 表名; -- 删除表建议操作\n -- 修改记录\n update 表名 set 列名1 = 值1, 列名2 = 值2,... [where 条件];\n --查询记录\n select * from 表名;\n select 字段名1，字段名2... from 表名；\n \n-- &#039;_&#039;表示一个占位符，&#039;%&#039;表示不确定占位符\n -- 查询姓马的有哪些？ like\nSELECT * FROM student WHERE NAME LIKE &#039;马%&#039;;\n-- 查询姓名第二个字是化的人\t\t\t\nSELECT * FROM student WHERE NAME LIKE &quot;_化%&quot;;\n-- 查询姓名是3个字的人\nSELECT * FROM student WHERE NAME LIKE &#039;___&#039;;\n以上就是最常写的sql命令了。Mysql极简入门，可以查询基本语法。其他高级部分用到再面向百度编程。\n\n\nJDBC—Java连接数据库\n记录一下JDBC最常用的 一套流程，其他冗余的就不多做记录了。\n\n原生 JDBC 连接操作流程\n\npublic class DemoJDBC {\n    public static void main(String[] args) throws Exception {\n \n        //  注册驱动 ，内部自动执行静态代码块\n        Class.forName(&quot;com.mysql.jdbc.Driver&quot;);\n \n        // 获取数据库连接对象\n        //本机可以简写  jdbc:mysql:///test_multi\n        Connection conn = DriverManager.getConnection(&quot;jdbc:mysql://localhost:3306/test_multi&quot;, &quot;root&quot;, &quot;123&quot;);\n \n        // 定义 sql 语句\n        String sql = &quot;update user set password = &#039;lol&#039;&quot;;\n        String sql2 = &quot;select * from user&quot;;\n \n        // 1. 获取sql执行对象\n        // 2. 事务管理\n        Statement statement = conn.createStatement();\n \n        // 执行sql对象\n        // executeUpdate (执行DML语句, DDL语句\n       int count = statement.executeUpdate(sql);\n        ResultSet results = statement.executeQuery(sql2);\n\t\t\n        //遍历结果\n        while (results.next()){\n            int id = results.getInt(&quot;uid&quot;);\n            String userName = results.getString(&quot;username&quot;);\n            String passWord = results.getString(&quot;password&quot;);\n            Date birth = results.getDate(&quot;birthday&quot;);\n \n            System.out.println(&quot;id: &quot; + id +\n                               &quot;userName: &quot; + userName +\n                               &quot;passWord: &quot; + passWord +\n                               &quot;birthDay: &quot; + birth);\n        }\n\t\t\n        // 释放资源\n        statement.close();\n        conn.close();\n    }\n \n}\n \n\n\nJDBCTemplate 流程 — 最常用\n\nSpring框架对JDBC的简单封装。提供了一个JDBCTemplate对象简化JDBC的开发\n\n\n\n// 定义工具类，采用阿里的Druid连接池\npublic class JDBCPoolUtils {\n \n    private static DataSource ds;\n \n    // 初始化数据源\n    static {\n        try {\n            Properties pro = new Properties();\n            pro.load(JDBCPoolUtils.class.getClassLoader().getResourceAsStream(&quot;druid.properties&quot;));\n            ds = DruidDataSourceFactory.createDataSource(pro);\n        } catch (IOException e) {\n            e.printStackTrace();\n        } catch (Exception e) {\n            e.printStackTrace();\n        }\n    }\n\t\n    // 获取连接对象\n    public static Connection getConnection() throws SQLException {\n        return ds.getConnection();\n    }\n\t\n    //释放资源\n    public static void close(PreparedStatement statement, Connection con) {\n        if (statement != null) {\n            try {\n                statement.close();\n            } catch (SQLException throwables) {\n                throwables.printStackTrace();\n            }\n \n        }\n        if (con != null) {\n            try {\n                statement.close();\n            } catch (SQLException throwables) {\n                throwables.printStackTrace();\n            }\n        }\n \n    }\n\t\n    //释放资源重载\n    public static void close(ResultSet res, PreparedStatement statement, Connection con) {\n        if(res!=null){\n            try {\n                res.close();\n            } catch (SQLException throwables) {\n                throwables.printStackTrace();\n            }\n \n        }\n        if (statement != null) {\n            try {\n                statement.close();\n            } catch (SQLException throwables) {\n                throwables.printStackTrace();\n            }\n \n        }\n        if (con != null) {\n            try {\n                statement.close();\n            } catch (SQLException throwables) {\n                throwables.printStackTrace();\n            }\n        }\n \n    }\n \n    public static DataSource getDataSource(){\n        return ds;\n    }\n \n \n}\n \n//JDBCTemplate流程\npublic class DemoJDBCTemplate {\n    public static void main(String[] args) {\n        //创建JdbcTemplate对象\n        JdbcTemplate template = new JdbcTemplate(JDBCPoolUtils.getDataSource());\n        // 定义sql语句\n        String sql = &quot;select * from user&quot;;\n        String sql2 = &quot;select count(id) from user&quot;;        \n        // 执行查询返回javabean对象装载到列表\n        //User 对象 与 数据库列一一对应\n        List&lt;User&gt; zyl = template.query(sql, new BeanPropertyRowMapper&lt;User&gt;(User.class)); // 返回 JavaBean\n        Long count = template.queryForObject(sql2,Long.class); // 执行聚合函数\n        System.out.println(count);\n        for(User map : zyl){\n            System.out.println(map);\n        }\n    }\n}\n \n\n2020-9-1日夜更新。"},"E_Technique_Explores/Linux操作整理":{"slug":"E_Technique_Explores/Linux操作整理","filePath":"E_Technique_Explores/Linux操作整理.md","title":"Linux操作整理","links":[],"tags":["linux"],"content":"根据uid定位user\nps aux grep [your_PID]\n目录创建\nmkdir(make directory) 命令可用来创建子目录。\n\nmkdir app 在当前目录下创建app目录\nmkdir –p app2/test 级联创建aap2以及test目\n\nrmdir(remove directory) 命令可用来删除“空”的子目录：\nrmdir app  删除app目录\n\n浏览文件（cat、more、less）\ncat 用于显示文件的内容。格式：\ncat [参数]&lt;文件名&gt;\ncat yum.conf\n`\nmore 一般用于要显示的内容会超过一个画面长度的情况。按空格键显示下一个画面。\n回车显示下一行内容。\n按 q 键退出查看。\nmore yum.conf\n\n空格显示下一页数据 回车显示下一行的数据\n\nless 用法和more类似，不同的是less可以通过PgUp、PgDn键来控制。\nless yum.conf\n\nPgUp 和 PgDn 进行上下翻页.\n\ntail 命令是在实际使用过程中使用非常多的一个命令，它的功能是：用于显示文件后几行的内容。\n用法:\ntail -10 /etc/passwd    查看后10行数据\n \ntail -f catalina.log    动态查看日志(*****)\n \n**ctrl+c** 结束查看\n\n压缩命令\ntar\ntar命令位于/bin目录下，它能够将用户所指定的文件或目录打包成一个文件，但不做压缩。一般Linux上常用的压缩方式是选用tar将许多文件打包成一个文件，再以gzip压缩命令压缩成xxx.tar.gz(或称为xxx.tgz)的文件。常用参数：\n\nc：创建一个新tar文件\nv：显示运行过程的信息\nf：指定文件名\nz：调用gzip压缩命令进行压缩\nt：查看压缩文件的内容\nx：解开tar文件\n\ntar –cvf xxx.tar ./*    // 打包\ntar –zcvf xxx.tar.gz ./*   // 打包并且压缩：\n \n// 解压 \ntar –xvf xxx.tar\ntar -zxvf xxx.tar.gz -C /usr/aaa\nzip\nzip -r mydata.zip mydata\n\n查找搜索\nfind\nfind指令用于查找符合条件的文件\n示例：\nfind / -name “ins*” 查找文件名称是以ins开头的文件\nfind / -name “ins*” –ls \nfind / –user itcast –ls 查找用户itcast的文件\nfind / –user itcast –type d –ls 查找用户itcast的目录\nfind /-perm -777 –type d-ls 查找权限是777的文件\ngrep\n查找文件里符合条件的字符串。\n用法: grep [选项]… PATTERN [FILE]…示例：\ngrep lang anaconda-ks.cfg  在文件中查找lang\ngrep lang anaconda-ks.cfg –color 高亮显示`\n\n系统管理命令\nps 正在运行的某个进程的状态\n \nps –ef  查看所有进程\n \nps –ef | grep ssh 查找某一进程\n \nkill 2868  杀掉2868编号的进程\n \nkill -9 2868  强制杀死进程\n\n重定向&amp;管道\n\n重定向输出，覆盖原有内容；&gt;&gt; 重定向输出，又追加功能；示例：\n\ncat /etc/passwd &gt; a.txt  将输出定向到a.txt中\ncat /etc/passwd &gt;&gt; a.txt  输出并且追加\nifconfig &gt; ifconfig.txt\n管道是Linux命令中重要的一个概念，其作用是将一个命令的输出用作另一个命令的输入。示例\nls --help | more  分页查询帮助信息\nps –ef | grep java  查询名称中包含java的进程\n \nifconfig | more\ncat index.html | more\nps –ef | grep aio\n\n权限管理\n\nr-4\nw-2\nx-1\n文件权限管理：\nchmod 变更文件或目录的权限。\nchmod 755 a.txt chmod u=rwx,g=rx,o=rx a.txt\n\n**查看硬盘大小\ndf -h  \nsudo fdisk -l  \nlsblk\n\n远程复制文件：\nscp fry@10.21.237.216:/media/T/chenhaoming/Code/DcPose/weights/detector/YOLOv3/yolov3.weights /Users/runyang/Desktop\n\nScreen窗口：\nscreen -dmS name # creatre 窗口\n \nscreen -r id/name # 进入窗口\n \nscreen -D id/name # Detach窗口\n \nscreen -X -S session_id quit # kill 窗口\n \nnohup python -u train.py &amp;&gt;nohup.out&amp; # 命令调到后台执行，并且输出信息写到nohup.out文件中，这个很有用，操作服务器时本地窗口可以关掉不影响服务器运行\n统计文件数量\nls -lR | grep &quot;^-&quot; | wc -l\n显卡占用看不到PiD，但是程序卡死\nhtop # 直接查找卡死的程序杀掉即可，百度的做法比较繁琐\nNet 网络相关\n\nifconfig -a # 查看ip地址、端口等网络相关信息\n \n**ssh jion@1.tcp.cpolar.cn -p 20279 # ssh远程连接 ssh username@ip -p(端口)"},"E_Technique_Explores/Mac-pycharm-任意版本永久激活":{"slug":"E_Technique_Explores/Mac-pycharm-任意版本永久激活","filePath":"E_Technique_Explores/Mac pycharm 任意版本永久激活.md","title":"Mac pycharm 任意版本永久激活","links":[],"tags":["软件"],"content":"下载地址\n一键激活-Mac系统 · 语雀"},"E_Technique_Explores/MyBlog-Hexo快速搭建":{"slug":"E_Technique_Explores/MyBlog-Hexo快速搭建","filePath":"E_Technique_Explores/MyBlog-Hexo快速搭建.md","title":"Hexo_blog搭建部署笔记","links":["E_Technique_Explores/拥有自己优雅的图床","E_Technique_Explores/MyBlog-Hexo快速搭建","E_Technique_Explores/hexo-blog搭建2-主题相关","E_Technique_Explores/hugo_blog搭建部署笔记"],"tags":["blog"],"content":"*博客系列教程\n拥有自己优雅的图床\nMyBlog-Hexo快速搭建\nhexo-blog搭建2-主题相关\nhugo_blog搭建部署笔记\n安装环境\nNodeJS + hexo + git, 这三个安装教程可以自行百度到，很多博客里也有，这里便不再赘述\n搭建博客\n初始化\nhexo init\n写一篇新博客\nhexo new &quot;helloworld&quot; #此时在source/_posts下会生成文章\n运行测试\nhexo s \n此时基本搭建完成\n部署到github\n安装deploy插件\nnpm install --save hexo-deployer-git\n新建github仓库\n这步比较简单，可以自己查查如何新建\n修改全局配置文件  _config.yml：\ndeploy:\n  type: git\n  repo: github.com/fryddup/fryddup.github.io.git #自己的仓库地址\n  branch: master\n推到远程服务器\nhexo -d\n此时可以通过 huaqi.blue 来访问我的博客\n\n注意\n每次写完博客 hexo clean  hexo g 两步必不可少，然后在 hexo d\n"},"E_Technique_Explores/Obsidian笔记和-Zotero-联动":{"slug":"E_Technique_Explores/Obsidian笔记和-Zotero-联动","filePath":"E_Technique_Explores/Obsidian笔记和 Zotero 联动.md","title":"Obsidian笔记和 Zotero 联动","links":["D_Paper_Related/template"],"tags":["软件"],"content":"\n今天装了 zotero 插件，很好用，一键生成模板，填充内容，省很多事，可以增加记笔记积极性。记录一下配置过程。\n\n鱼与熊掌兼得：Zotero 和 Obsidian 联动\n1.安装插件\n\nObsidian --- Zotero Integration \nZotero --- Better BibTex for Zotero\n\n2. ZOtero 设置\nZotero 和 Obsidian 引用风格保持一致 比如 IEEE\n3. Obsidian 设置\n我截图重点记录一下：\n*所用到的模板文件 template\n\n\n\n4. 使用\n使用上述自定义的命令直接使用，具体参考开篇连接\npapercite\ncite"},"E_Technique_Explores/PyTorch冻结部分参数训练":{"slug":"E_Technique_Explores/PyTorch冻结部分参数训练","filePath":"E_Technique_Explores/PyTorch冻结部分参数训练.md","title":"PyTorch参数的初始化与冻结部分参数训练","links":[],"tags":["pytorch编程","Code"],"content":"\n在平时训练中，经常会有冻结某些层的参数，单独训练其他参数以及载入预训练模型来初始化参数的需求，最近也是研究了一下如何实现，记录一下相关的知识。\n\nPyTorch中参数的冻结\n​\t模型的冻结即参数不再更新即可，可以通过梯度来控制，Pytorch中有几种方法，这里先来总结一下：\n\n\nTensor.requires_grad\n这个属性非常重要，在模型训练时，requires_grad_()函数会改变Tensor的requires_grad属性并返回Tensor，其默认参数为requires_grad=True。requires_grad=True时，自动求导会记录对Tensor的操作，requires_grad_()的主要用途是告诉自动求导开始记录对Tensor的操作。故只要把该参数设置成False即可冻结。\nif self.freeze_weights:\n    m.weight.requires_grad = False\n    m.bias.requires_grad = False\n\n\nwith torch.no_grad()\n在该语句块内的代码都会禁止梯度的计算，加快计算速度，一般用于Inference阶段，可以减少计算内存的使用量。\nmodel = HRNet()\nwith torch.no_grad():\n    ootput = model(input) # 不会改变模型参数\n\n\ndetach()\ndetach()函数会函数会返回一个新的Tensor对象b，并且新Tensor是与当前的计算图分离的，其requires_grad属性为False，反向传播时不会计算其梯度。b与a共享数据的存储空间，二者指向同一块内存。\n该函数一般用于中间需要打印张量结果，或者是保存图片,使用时应如下：\nimg = images[i].cpu().detach().numpy()*255\n\n\nmodel.eval()\n训练完train_datasets之后，model要来测试样本了。在model(test_datasets)之前，需要加上model.eval(). 否则的话，有输入数据，即使不训练，它也会改变权值。这是model中含有batch normalization层所带来的的性质。该参数冻结需要依靠eval()函数，即在Inference时，\nmodel = HRNet()\nmodel.eval() \nwith torch.no_grad:\n    output = model(input) # 如果有BN层，此时不会改变模型参数\n保险起见，在推理时都需要加上model.eval()\n\n\ngrad_fn\n表示积分的方法名\n\n\n\n第二次更新 只冻结 Backbone\n在 Backbone 层含有 BatchNorm 时，只冻结参数是不可行的，还需要把 BatchNorm 层进行冻结。BN 层不靠梯度更新，而是根据当前的动量更新，所以冻结 BN 需要在每一次前向传播用到 backbone 时，把对应的层设置成 eval().\n \ndef _freeze_bn(self,m):  \n    classname = m.__class__.__name__  \n    if classname.find(&#039;BatchNorm&#039;) != -1:  \n        m.eval()\n \ndef freeze_weight(self):  \n    for module in self.modules():  \n        parameters = module.parameters()  \n        for parameter in parameters:  \n            parameter.requires_grad = False\n \nif self.freeze_hrnet_weight:  \n    self.hrnet.freeze_weight()  # 进行参数冻结\n \ndef forward(self, kf_x, sup_x, **kwargs):\n\tx = torch.cat([kf_x, sup_x], dim=0)  \n\tself.hrnet.apply(self._freeze_bn)  # 进行 BN 冻结 \n\tx_bb_hm, x_bb_feat = self.hrnet(x, multi_scale=True)\n "},"E_Technique_Explores/Pytorch-Dataloader-用法":{"slug":"E_Technique_Explores/Pytorch-Dataloader-用法","filePath":"E_Technique_Explores/Pytorch-Dataloader-用法.md","title":"Pytorch Dataloader 用法","links":[],"tags":["pytorch编程","Code"],"content":"DataLoder\nPytorch在训练前一步数据读取时，要使用 DataLoader 加载数据, 可以shuffle  、多线程读取等。记录一下如何使用。\n\n自黑一下，之前我写的第一个工程，完全没写DataLoader，直接把原图，标签放在两个大列表里面来循环，😂🤣，所以当时16g内存都直接溢出😆，所以说不写DataLoader也不是不可以，就是有点不可描述\n\n使用步骤\n定义Dataset\ntorch.utils.data.Dataset \n首先自定义dataset类，以上述Dataset为父类，必须重写__getitem__() 方法，即获取数据逻辑；\n可选重写__len()__方法，获取数据长度信息。\n类似如下，是我们最近做的一个研究中Dataset 片段，重点关注它return的结果。\nimport torch.utils.data.Dataset \n \n# 定义 Dataset\nclass JointsDataset(Dataset):\n    def __init__(self, ........):\n        pass\n    def __len__(self, ):\n        return len(self.db)\n    def __getitem__(self, idx):\n        &#039;&#039;&#039;\n        省略代码块\n        &#039;&#039;&#039;\n        return input_data_numpy, input_sup_A_data_numpy, input_sup_B_data_numpy, target_heatmaps, target_weight, meta\n创建 DataLoader\n先看一下Dataloader类定义\ntorch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, sampler=None, batch_sampler=None, num_workers=0, collate_fn=None, pin_memory=False, drop_last=False, timeout=0, worker_init_fn=None, multiprocessing_context=None)\n一般用的几个参数：\n\ndataset 为上述的自定义的类\nbatch_size\nshuffle 打乱数据\nnum_workers 多线程\npin_memory 更快的发送数据到显存 （不太清楚）\n\ntrain_dataset = JointsDataset(\n        cfg, cfg.DATASET.ROOT, cfg.DATASET.TRAIN_SET, True,\n        cfg.DATASET.TRAIN_NPY_DIR,\n        transform=transforms.Compose([\n            transforms.ToTensor(),\n            normalize,\n        ])\n    )\n \ntrain_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=cfg.TRAIN.BATCH_SIZE_PER_GPU * len(cfg.GPUS),\n        shuffle=cfg.TRAIN.SHUFFLE,\n        num_workers=cfg.WORKERS,\n        pin_memory=cfg.PIN_MEMORY\n    )\n迭代获取数据\nfor i, (input, input_sup_A, input_sup_B, target, target_weight, meta) in enumerate(train_loader):\n    pass\n关于获取的数据可以看到和自定义dataset类__getitem()__方法返回的东西是一样的。\n\n总结一下，定义dataset, 创建dataloder, 迭代获取进行训练。"},"E_Technique_Explores/Pytorch中的GPU调用":{"slug":"E_Technique_Explores/Pytorch中的GPU调用","filePath":"E_Technique_Explores/Pytorch中的GPU调用.md","title":"Pytorch / Python 中的GPU调用","links":[],"tags":["pytorch编程","Code"],"content":"一般用Pytorch编程训练模型肯定是要用GPU，多GPU有时候会一直出问题，这里记录一下。\nPytorch用GPU计算\n可以用Python的方法把程序发送到GPU：\nos.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = &quot;2&quot; # 用第三个显卡来计算\n \nos.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = &quot;0，1，2&quot; # 用第三个显卡来计算   \n注意这种方法只能用到一张卡根据我的测试，不能并行计算，要并行计算的话需要一下代码：\n# 用三张显卡并行的跑数据\nmodel_cuda = torch.nn.DataParallel(model.cuda(), device_ids=(0,1,2))\n# batch_size的设置\nbatch_size = origin_batch_size * len(cfg.GPUS)\n\n注意，一般情况下不用os.environ的话，所有的初始张量、模型都会加载在显卡0，故此时如果0显卡被占满会一直提示CUDA OUT OF MEMORY 错误。\n\n第二次更新\nPytorch一般情况主GPU是第一块显卡，可以用以下命令来指定主显卡设备：\ntorch.cuda.set_device(2)  # 指定GPU2伟主要计算GPU\n建议用这个命令代替python命令。\n\n第三次更新\n1.直接终端中设定：\nCUDA_VISIBLE_DEVICES=1 python my_script.py\n2. python代码中设定：\nimport os\nos.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = &quot;2&quot;\n3. 使用函数 set_device\nimport torch\ntorch.cuda.set_device(id)\n官方建议使用 CUDA_VISIBLE_DEVICES"},"E_Technique_Explores/Ubuntu16-18可道云云盘搭建":{"slug":"E_Technique_Explores/Ubuntu16-18可道云云盘搭建","filePath":"E_Technique_Explores/Ubuntu16-18可道云云盘搭建.md","title":"Ubuntu16/18可道云云盘搭建","links":["E_Technique_Explores/从0开始搭建私人云盘"],"tags":["Cloud"],"content":"\n从0开始搭建私人云盘\n\n实验室有自己有一台主机闲置，想着闲置太浪费，但是一直不会用。所以就把这个主机当成一个web服务器😝，这个主机系统是Ubuntu18.04， Ubuntu16.04也已经测试。\n安装 apache2\n// 下载安装\nsudo apt-get update\nsudo apt-get install apache2\n// 防火墙\nsudo ufw allow &#039;Apache Full&#039;\n//启动\nsudo ufw enable\nApache2服务启动后，可以输入本机ip看到对应的页面表示安装成功。\n参考链接：www.cnblogs.com/lfri/p/10522392.html\n安装mysql和php\n sudo apt-get install php-mysql\n sudo apt-get install phpmyadmin\n安装可道云\n下载可道云源码，解压到：\n/var/www/html/cloud\n就可以通过ip来访问：10.21.7.216/cloud\n"},"E_Technique_Explores/X-Particles-4-0-粒子":{"slug":"E_Technique_Explores/X-Particles-4-0-粒子","filePath":"E_Technique_Explores/X-Particles-4-0-粒子.md","title":"X-Particles 4.0 粒子","links":[],"tags":["软件"],"content":"最近学习到了X-Particles4.0粒子的破解流程，虽然我目前已经不做CG了，但是还是值得记录一下，因为这个插件是我当时交智商税交的最多的一个东西,我记得是花了我380还是390大洋!\n这里只是记录一下流程，至于具体的破解文件、工具之类的就不放出来了。\n\n下载插件\n首先下载好X-Particles 7.3.2,放入C4D的插件目录\n破解\n输入用户名、邮箱、序列号， 关于序列号这里照常不写出来。\n定时\n修改C4D时间，定时到某一个点，即可破解成功\nTips\n打开C4D 一定要断网打开，然后再联网即可正常使用。\n"},"E_Technique_Explores/git推送文件":{"slug":"E_Technique_Explores/git推送文件","filePath":"E_Technique_Explores/git推送文件.md","title":"Git常用操作整合","links":["E_Technique_Explores/git推送文件","E_Technique_Explores/git错误与代理相关"],"tags":["git"],"content":"*git相关\ngit推送文件\ngit错误与代理相关\ngithub远程托管文件 非常的方便。git操作种比较多的就是本地文件推送到远程，仓库等。而我一直也是没有搞明白，github本地文件远程推送是如何操作的，以至于今天卡了很久，来记录一下。\n\ngit推送本地文件\n一般流程\n**Note **:  默认远程仓库是空的\n# 1.新建远程仓库 ，这个可以自己去查阅，很容易\n \n# 2.把本地需要推送的文件夹设置为git仓库\ngit init\n \n# 3.添加文件\ngit add .\n \n# 4.提交文件\ngit commit -m &#039;first_commit&#039;\n \n# 5.添加镜像源\ngit remote add origin github.com/fryddup/fryddup.github.io.git\n \n# 6.推送文件\ngit push -u origin master\n同步问题\n以上代码即可实现本地文件推送到远程，但是注意当仓库不是空的，仓库有改动，应当如下操作：\ngit add .\n \ngit commit -m &#039;first_commit&#039;\n \ngit pull  #注意，此命令即是远程文件同步到本地。\n \ngit push -u origin master\ngit 代理相关\n设置代理\ngit config --global http.proxy &#039;socks5://127.0.0.1:1080&#039; \ngit config --global https.proxy &#039;socks5://127.0.0.1:1080&#039;\n查看代理\ngit config --global --get http.proxy\ngit config --global --get https.proxy\n取消代理\ngit config --global --unset http.proxy\ngit config --global --unset https.proxy\n踩坑\n新建仓库，新建README.md，本地 add，commit 之后，push遇到了如下错误：\n ! [rejected]        master -&gt; master (non-fast-forward)\nerror: failed to push some refs to &#039;github.com/tzwx/DeepLearning.git&#039;\n\ngit pull origin master --allow-unrelated-histories //把远程仓库和本地同步，消除差异\ngit push -u origin master\n\ngit其他操作、问题以后遇到了再补充**"},"E_Technique_Explores/git错误与代理相关":{"slug":"E_Technique_Explores/git错误与代理相关","filePath":"E_Technique_Explores/git错误与代理相关.md","title":"git错误与代理相关","links":["E_Technique_Explores/git推送文件","E_Technique_Explores/git错误与代理相关"],"tags":["git"],"content":"*git相关\ngit推送文件\ngit错误与代理相关\n前一篇文章，我写到了git代理设置相关的东西,实际上应该根据自己的vpn选择开哪一种代理。ssr用sock5代理是可以的，但是clash就行不通，需要把git代理取消，然后设置成http代理。\n代理的端口号要根据vpn来设置，并不是通用的。\n我自己的git突然不能用，错误大致如下：\nOpenSSL......Error443\n解决如下：\ngit config --global --unset http.proxy\ngit config --global --unset https.proxy\n \n#设置 http代理\ngit config --global https.proxy http://127.0.0.1:根据vpn选择端口号\n \ngit config --global https.proxy https://127.0.0.1:根据vpn选择端口号\n\nError 403  request denail\n该错误表示登录错了账号，拒绝推送，我自己也是记混了git账号，所以出现了这个问题。\n\n另外，安利一款梯子：Clash\n\n自动选择节点\n界面舒适\n"},"E_Technique_Explores/hexo-blog搭建2-主题相关":{"slug":"E_Technique_Explores/hexo-blog搭建2-主题相关","filePath":"E_Technique_Explores/hexo-blog搭建2-主题相关.md","title":"hexo_blog搭建2-主题相关","links":["E_Technique_Explores/拥有自己优雅的图床","E_Technique_Explores/MyBlog-Hexo快速搭建","E_Technique_Explores/hexo-blog搭建2-主题相关","E_Technique_Explores/hugo_blog搭建部署笔记"],"tags":["blog"],"content":"*博客系列教程\n拥有自己优雅的图床\nMyBlog-Hexo快速搭建\nhexo-blog搭建2-主题相关\nhugo_blog搭建部署笔记\n我的这个版本博客是基于Hexo搭建的，主题采用的是 Butterfly，在这个网址有详细的安装配置教程。 这个主题我非非常喜欢，看了很多的主题就相中了这款。然而配置也是花了我整整一天，可能还算是比较顺利把！配置过程中有的地方安装文档讲的不是很详细，我自己也踩坑了，来记录一下！\n\n添加 Gallery（相册）\n新建页面\nhexo new page gallery #此时会在source目录下产生gallery/index.md\n打开来编辑这个md文件，有两点要注意的：\n\n开头的 type: “gallery”   不能出错\n内容格式 按照  {% gallery %}  img1  img2  img3 {% endgallery %}    来写\n\n\n添加导航栏\n在 butterfly.yml 文件的menu下照着之前的添加相册，如下图\n\n至此，相册添加结束\n添加本地搜索\n安装hexo本地搜索的插件\nnpm install hexo-generator-searchdb --save\n配置全局的config文件\n\n配置butterfly文件\n这里的配置和官方教程一样，不再赘述\n踩坑\n我自己在这里一直配置不好，刚开始发现是找不到 search.xml 文件，后来我在博客插件目录找到了，然后改上图的path，这样在本地启动blog是可以搜索的，但是推送到远程就不行了，后面我重装了插件，path改成默认路径,然后 hexo clean  hexo g hexo d  就很神奇的在github仓库生成了search.xml文件，也就随之可以用了，具体原理我还没搞明白，以后明白了再更新。\n添加评论\n我用的是 Valine 评论系统，这个很容易搭建起来，大致描述一下流程：\n配置 valine\n\n\n注册 www.leancloud.cn/ 在这个网站注册账号，然后进行实名认证\n\n\n创建一个应用\n\n\n进入设置页面，在应用keys中可以看到自己的 AppID AppKey   等会儿会用到\n\n\n在安全中心页面 web安全域名写入自己的博客域名（这一步很重要，我就是一开始在这里栽了）\n至此，valine配置结束\n\n\n配置主题文件\n这里的流程和官方教程一样，很容易，自己直接会明白\n我在这里卡住是因为写 APPID APPKey出了问题，卡了很久才发现\n然后重新更新生成就有了评论功能！"},"E_Technique_Explores/hugo_blog搭建部署笔记":{"slug":"E_Technique_Explores/hugo_blog搭建部署笔记","filePath":"E_Technique_Explores/hugo_blog搭建部署笔记.md","title":"Hugo_blog搭建部署笔记","links":["E_Technique_Explores/拥有自己优雅的图床","E_Technique_Explores/MyBlog-Hexo快速搭建","E_Technique_Explores/hexo-blog搭建2-主题相关","E_Technique_Explores/hugo_blog搭建部署笔记"],"tags":["blog"],"content":"*博客系列教程\n拥有自己优雅的图床\nMyBlog-Hexo快速搭建\nhexo-blog搭建2-主题相关\nhugo_blog搭建部署笔记\n之前一直用的  Hexo Blog ,奈何写出来太丑了，不习惯，部署起来也比较麻烦，就找到了新的好看的，还真的是好看是第一生产力！hugo部署过程如下。\n\n描述版：\n1.安装 hugo （有很多方法，可以到官方网站看一下，我是采用chocolate来安装的）\nchoco install hugo\n2.检测go语言是否安装成功\nhugo version\n3.开始建站,建立一个 myblog 站点（博客的根目录）\nhugo new site myblog\n4.安装hugo博客主题 官网 themes.gohugo.io/,每个主题有对应的方式，我以我自己的为例,此时目录在博客的根目录\ngit clone github.com/flysnow-org/maupassant-hugo themes/maupassant\n5.写一篇新博客，位于content/post/\nhugo new post/Hello,world.md\n6.启动本地调试，此时位于根目录\nhugo server --buildDrafts\n7.新建github仓库，这个比较容易，自己解决\n8.关联到GitHub,此步骤会生成public文件夹\nhugo --themes=maupassant --baseURL=&quot;huaqi19.github.io/&quot;\n9.将 public 文件夹内容推送到空的github仓库中\n此处步骤省略，如何推送可以参考我的上一篇博客  git推送文件\n10.更新博客\nhugo -D\n至此博客搭建完成，主题配置请自行研究\n代码版：\n# 1.安装 hugo （有很多方法，可以到官方网站看一下，我是采用chocolate来安装的）\nchoco install hugo\n \n# 2.检测go语言是否安装成功\nhugo version\n \n# 3.开始建站,建立一个 myblog 站点（博客的根目录）\nhugo new site myblog\n \n# 4.安装hugo博客主题 官网 themes.gohugo.io/\n# 每个主题有对应的方式，我以我自己的为例\n# 此时目录在博客的根目录\ngit clone github.com/flysnow-org/maupassant-hugo themes/maupassant\n \n# 5.写一篇新博客，位于content/post/\nhugo new post/Hello,world.md\n \n# 6.启动本地调试，此时位于根目录\nhugo server --buildDrafts\n \n# 7.新建github仓库，这个比较容易，自己解决\n \n# 8.关联到GitHub,此步骤会生成public文件夹\nhugo --themes=maupassant --baseURL=&quot;huaqi19.github.io/&quot;\n \n# 9.将 public 文件夹内容推送到空的github仓库中\n# 此处步骤省略，如何推送可以参考我的上一篇博客  git推送文件\n \n# 10.更新博客\nhugo -D\n \n# 至此博客搭建完成，主题配置请自行研究\n "},"E_Technique_Explores/index":{"slug":"E_Technique_Explores/index","filePath":"E_Technique_Explores/index.md","title":"index","links":["E_Technique_Explores/服务器初始环境安装与配置","E_Technique_Explores/记录MMDetection-ConvNext配置过程","E_Technique_Explores/内网穿透工具","E_Technique_Explores/一套用于人体姿态估计的关键点标注流程","E_Technique_Explores/Linux操作整理","E_Technique_Explores/Mac-pycharm-任意版本永久激活","E_Technique_Explores/pycharm常见问题合集","E_Technique_Explores/pycocotools-和-numpy-的兼容性问题","E_Technique_Explores/win10环境配置maskrcnn-benchmark","E_Technique_Explores/win10环境下detectron2配置","E_Technique_Explores/PyTorch冻结部分参数训练","E_Technique_Explores/Pytorch-Dataloader-用法","E_Technique_Explores/pytorch-numpy中的shape和size","E_Technique_Explores/Pytorch中的GPU调用","E_Technique_Explores/从0开始搭建私人云盘","E_Technique_Explores/爬虫入门","E_Technique_Explores/拥有自己优雅的图床","E_Technique_Explores/自己的域名与github绑定","E_Technique_Explores/MyBlog-Hexo快速搭建","E_Technique_Explores/hexo-blog搭建2-主题相关","E_Technique_Explores/hugo_blog搭建部署笔记","E_Technique_Explores/DeepFaceLab-操作","E_Technique_Explores/git错误与代理相关","E_Technique_Explores/git推送文件","E_Technique_Explores/python-库-you-get下载视频","E_Technique_Explores/Ubuntu16-18可道云云盘搭建","E_Technique_Explores/windwos下几款实用又美观的软件推荐","E_Technique_Explores/斐波那契额数列几种实现方式","E_Technique_Explores/个人信息管理系统-Web项目总结分析","E_Technique_Explores/计算机网络-IP地址-端口","E_Technique_Explores/牛客网刷题笔记-剑指offer系","E_Technique_Explores/JavaWeb简要总结-Mysql-JDBC","E_Technique_Explores/AECC2020中英文切换","E_Technique_Explores/X-Particles-4-0-粒子"],"tags":[],"content":"\n实验配置（踩坑）\n\n服务器初始环境安装与配置\n记录MMDetection-ConvNext配置过程\n内网穿透工具\n一套用于人体姿态估计的关键点标注流程\nLinux操作整理\nMac pycharm 任意版本永久激活\npycharm常见问题合集\npycocotools 和 numpy 的兼容性问题\nwin10环境配置maskrcnn-benchmark\nwin10环境下detectron2配置\n\n\nPyTorch相关\n\nPyTorch冻结部分参数训练\nPytorch-Dataloader-用法\npytorch-numpy中的shape和size\nPytorch中的GPU调用\n\n\n其他技术 (CS方向)\n\n从0开始搭建私人云盘\n爬虫入门\n拥有自己优雅的图床\n自己的域名与github绑定\nMyBlog-Hexo快速搭建\nhexo-blog搭建2-主题相关\nhugo_blog搭建部署笔记\nDeepFaceLab 操作\ngit错误与代理相关\ngit推送文件\npython-库-you-get下载视频\nUbuntu16-18可道云云盘搭建\nwindwos下几款实用又美观的软件推荐\n\n\n编程刷题\n\n斐波那契额数列几种实现方式\n个人信息管理系统-Web项目总结分析\n计算机网络-IP地址-端口\n牛客网刷题笔记-剑指offer系\nJavaWeb简要总结-Mysql-JDBC\n\n\n视觉设计\n\nAECC2020中英文切换\nX-Particles-4-0-粒子\n"},"E_Technique_Explores/pycharm常见问题合集":{"slug":"E_Technique_Explores/pycharm常见问题合集","filePath":"E_Technique_Explores/pycharm常见问题合集.md","title":"pycharm常见问题合集","links":["E_Technique_Explores/Mac-pycharm-任意版本永久激活"],"tags":["软件"],"content":"\nMac pycharm 任意版本永久激活\n\nPycharm DeBug进不去\nforward函数运行能进去，但是debug进不去，很抓狂\n网上说的对pycharm本身进行设置基本解决不了该问题。\n解决过程：\n\n初始化时， init 内部打断点，看是否在正确的文件内部打了断点；\n终极操作：\n\n\n改成单gpu，单线程 运行，bug消除。\n\nPycharm DeBug卡死\n\nPycharm 修改缓存位置\n配置文件位置（win）：\nD:\\Program Files\\JetBrains\\PyCharm 2019.2.5\\bin\\idea.properties\n\nPycharm 代码模版\n在pycharm菜单栏找File → settings → Editor → File and Code Templates → Python Script，找到后编辑：\n1 #!/usr/bin/env python\n2 # _*_ coding: utf-8 _*_\n3 # @Time : ${DATE} ${TIME} \n4 # @Author : Runyang \n5 # @Version：V 0.1\n6 # @File : ${NAME}.py\n7 # @desc :\n \n&#039;&#039;&#039;\n# @Lab : CVer\n# @Time :  2020/12/3 21:00\n# @Author : Runyang\n# @Email : runyang.feng@qq.com\n# @File : dcpose_backbone.py\n&#039;&#039;&#039;"},"E_Technique_Explores/pycocotools-和-numpy-的兼容性问题":{"slug":"E_Technique_Explores/pycocotools-和-numpy-的兼容性问题","filePath":"E_Technique_Explores/pycocotools 和 numpy 的兼容性问题.md","title":"pycocotools 和 numpy 的兼容性问题","links":[],"tags":["环境"],"content":"numpy.ndarray size changed, may indicate binary incompatibil\n二进制巴拉巴拉一堆问题\n问题在于numpy版本太低。有两种解决方案：\n\n直接更新numpy版本\n\npip install --upgrade numpy\n\n卸载pycocotools库，先安装numpy，在安装pycocotools\n\n推荐使用这种方案，这种方案可以保持旧版本numpy，新版本会有很多的warning"},"E_Technique_Explores/python-库-you-get下载视频":{"slug":"E_Technique_Explores/python-库-you-get下载视频","filePath":"E_Technique_Explores/python-库-you-get下载视频.md","title":"python-库-you-get下载视频","links":[],"tags":["python库"],"content":"You-get 是一个可以下载很多在线视频的使用的Python库，我最近主要利用这个库来下载bilibili的视频，下面做一些使用的简单记录。\n基本使用\n安装\n和其他python库安装相同，直接使用pip即可。\nactivate deep  # 激活当前环境\npip install you-get  # 安装you-get库\n使用\n\n分析视频信息\n\nyou-get -i url # url 为视频链接地址\n \n# for example\nyou-get -i you-get -i www.bilibili.com/video/BV1ST4y1E7FW\n&#039;&#039;&#039;\n输出信息：\nsite:                Bilibili\ntitle:               【4k60帧】真正的奥利给，“后浪风”演讲——看见的力量\nstreams:             # Available quality and codecs\n    [ DASH ] ____________________________________\n    - format:        dash-flv\n      container:     mp4\n      quality:       高清 1080P\n      size:          46.7 MiB (48993131 bytes)\n    # download-with: you-get --format=dash-flv [URL]\n \n    - format:        dash-flv720\n      container:     mp4\n      quality:       高清 720P\n      size:          32.3 MiB (33909997 bytes)\n    # download-with: you-get --format=dash-flv720 [URL]\n \n    - format:        dash-flv480\n      container:     mp4\n      quality:       清晰 480P\n      size:          16.4 MiB (17216475 bytes)\n    # download-with: you-get --format=dash-flv480 [URL]\n \n    - format:        dash-flv360\n      container:     mp4\n      quality:       流畅 360P\n      size:          11.4 MiB (11943322 bytes)\n    # download-with: you-get --format=dash-flv360 [URL]\n \n    [ DEFAULT ] _________________________________\n    - format:        flv\n      container:     flv\n      quality:       高清 1080P\n      size:          75.9 MiB (79590049 bytes)\n    # download-with: you-get --format=flv [URL]\n \n    - format:        flv720\n      container:     flv\n      quality:       高清 720P\n      size:          51.8 MiB (54269960 bytes)\n    # download-with: you-get --format=flv720 [URL]\n \n    - format:        flv480\n      container:     flv\n      quality:       清晰 480P\n      size:          25.2 MiB (26397350 bytes)\n    # download-with: you-get --format=flv480 [URL]\n \n    - format:        flv360\n      container:     flv\n      quality:       流畅 360P\n      size:          11.5 MiB (12080563 bytes)\n    # download-with: you-get --format=flv360 [URL]\n \n&#039;seid&#039; 不是内部或外部命令，也不是可运行的程序\n或批处理文件。\n \n&#039;&#039;&#039;\n    \n\n下载视频\n\n# download\nyou-get www.bilibili.com/video/BV1ST4y1E7FW\\from=search&amp;seid=13389194302262225891\n \n# 指定路径 -o\nyou-get  -o z:/video www.bilibili.com/video/BV1ST4y1E7FW\\from=search&amp;seid=13389194302262225891\n \n# 指定分辨率 -format\n \nyou-get -format==flv720 www.bilibili.com/video/BV1ST4y1E7FW\\from=search&amp;seid=13389194302262225891\n注意的问题\n我是win10 Home版系统，会遇到问题：\n‘seid’ 不是内部或外部命令，也不是可运行的程序或批处理文件。\n解决方案：\n将 &amp;seid= 替换为3D 即可\n一条完整的下载视频命令：\nyou-get  -o z:/video -format==flv720 www.bilibili.com/video/BV1ST4y1E7FW\\from=search&amp;seid=13389194302262225891"},"E_Technique_Explores/pytorch-numpy中的shape和size":{"slug":"E_Technique_Explores/pytorch-numpy中的shape和size","filePath":"E_Technique_Explores/pytorch-numpy中的shape和size.md","title":"pytorch、numpy中的shape和size","links":[],"tags":["pytorch编程","Code"],"content":"每次在获得图片尺寸时我都会写错，因此来记录一下shape、size。\nPytorch\n.size() 是方法(function),可以传参数，例如：\n c = torch.randn(2,3)\n# c\n#tensor([[ 1.4753, -0.5479, -0.4448],\n#        [ 0.1452,  0.9948,  0.1481]])\nc.size(0)\n#2\nc.size(1)\n#3\nc.size[0]\nTraceback (most recent call last):\n  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;\nTypeError: &#039;builtin_function_or_method&#039; object is not subscriptable\nc.size[1]\nTraceback (most recent call last):\n  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;\nTypeError: &#039;builtin_function_or_method&#039; object is not subscriptable\n.shape则是属性\nc.shape\n# torch.Size([2, 3])\nc.shape(0)\nTraceback (most recent call last):\n  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;\nTypeError: &#039;torch.Size&#039; object is not callable\nc.shape[0]\n# 2\nc.shape[1]\n# 3\ntype(c.shape)\n&lt;class &#039;torch.Size&#039;&gt;\n故如果是torch的张量，在需要调用图像尺寸时，应该写：\nc.size(0)\nc.size(1)\n#或者\nc.shape[0]\nc.shape[1]\nNumpy\nnumpy中，.size()是属性，用来输出元素个数\na = np.array([2,3])\na\n#array([2, 3])\na.size\n#2\na.size()\nTraceback (most recent call last):\n  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;\nTypeError: &#039;int&#039; object is not callable\n&gt;&gt;&gt; a.size(0)\nTraceback (most recent call last):\n  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;\nTypeError: &#039;int&#039; object is not callable\n.shape也是属性\na.shape(0)\nTraceback (most recent call last):\n  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;\nTypeError: &#039;tuple&#039; object is not callable\n&gt;&gt;&gt; a.shape\n(2,)\n&gt;&gt;&gt; a.shape[0]\n2\n&gt;&gt;&gt; a.shape[1]\nTraceback (most recent call last):\n  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;\nIndexError: tuple index out of range\n故如果是numpy类型的数组，获取图片尺寸应用：\na.shape[0]\na.shape[1]\n小结\n\n\ntorch的Tensor形状可以用size，也可以用shape；\n\n\nnumpy则只用shape\n\n"},"E_Technique_Explores/win10环境下detectron2配置":{"slug":"E_Technique_Explores/win10环境下detectron2配置","filePath":"E_Technique_Explores/win10环境下detectron2配置.md","title":"win10环境下detectron2配置","links":[],"tags":["环境"],"content":"最强目标检测平台Detectron2 ，基于PyTorch完全重构，windows上很不友好，很难配置，配置好就算装好这个库依然不能使用nms,回头再想办法解决。安装过程曲折，记录一哈。\n\nwww.luyixian.cn/news_show_240401.aspx我主要参考的这个链接\ngithub.com/conansherry/detectron2 这个链接在我踩坑也不可或缺\n我自己电脑是cuda9.0,cudnn7,上面第二个链接要墙置换成cuda10，很难搞，所以我就按第一个来\n安装依赖\n依赖的库：pytorch1.3 opencv  pycocotools  fvcore，其中最后两个安装不同于以往，这里提一下。\npip install git+github.com/facebookresearch/fvcore\npip install &#039;git+github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI&#039;\n其中，pycocotools安装很是麻烦，这个命令可能不会成功，所以需要自己探索一下，装不好可以评论留言找我要。\n确认gcc&gt;=4.9\ngcc --version\n修改lib文件\nfile1: \n  {your evn path}\\Lib\\site-packages\\torch\\include\\torch\\csrc\\jit\\argument_spec.h\n  example:\n  {C:\\Miniconda3\\envs\\py36}\\Lib\\site-packages\\torch\\include\\torch\\csrc\\jit\\argument_spec.h(190)\n    static constexpr size_t DEPTH_LIMIT = 128;\n      change to --&gt;\n    static const size_t DEPTH_LIMIT = 128;\nfile2: \n  {your evn path}\\Lib\\site-packages\\torch\\include\\pybind11\\cast.h\n  example:\n  {C:\\Miniconda3\\envs\\py36}\\Lib\\site-packages\\torch\\include\\pybind11\\cast.h(1449)\n    explicit operator type&amp;() { return *(this-&gt;value); }\n      change to --&gt;\n    explicit operator type&amp;() { return *((type*)this-&gt;value); }\n克隆检测器并安装\ngit clone github.com/facebookresearch/detectron2.git\ncd detectron2\npython setup.py build develop\n这期间提示缺少什么 就补什么就行。\n至此安装完成，pip list 可以看到 detectron2 这个包\n踩坑\n按着第一个链接教程搞，编译一直出错，各种莫名其妙的错误，编译不成功；然后我按照第二个教程修改了文件，就成功安装了。\n不同电脑环境不同，可能是环境问题导致。这个可能也只适用于我自己的电脑。"},"E_Technique_Explores/win10环境配置maskrcnn-benchmark":{"slug":"E_Technique_Explores/win10环境配置maskrcnn-benchmark","filePath":"E_Technique_Explores/win10环境配置maskrcnn-benchmark.md","title":"win10环境配置maskrcnn_benchmark","links":[],"tags":["环境"],"content":"最近在找实例分割代码，我在win10环境下配置，配置了很久很久，很多坑，记录一下\n项目简介\nMaskrcnn_Benchmark 是 MaskRCNN 的 Pytorch 实现版本，MaskRCNN是本身是Tensorflow实现,用于实例分割。\n项目地址：github.com/facebookresearch/maskrcnn-benchmark\n环境准备\nwin10 Home + Anaconda3  + CUDA 9.0 + CUDNN 7.1 + visualcppbuildtools_full\n最后一个软件看可以代替 Visual Studio\n下载地址：pan.baidu.com/s/1J0CAz_d9semPyiEWu8nIbg \t提取码：k490\n安装步骤\n\n按照官方教程以及一些改动即可。\n\n参照这个博客： www.jianshu.com/p/e9680d0bfa5c\n下述是官方安装教程搬运，具体参照上述博客更好。\nopen a cmd and change to desired installation directory\nfrom now on will be refered as INSTALL_DIR\nconda create --name maskrcnn_benchmark\nconda activate maskrcnn_benchmark\n \n# this installs the right pip and dependencies for the fresh python\nconda install ipython\n \n# maskrcnn_benchmark and coco api dependencies\npip install ninja yacs cython matplotlib tqdm opencv-python\n \n# follow PyTorch installation in pytorch.org/get-started/locally/\n# we give the instructions for CUDA 9.0\n## Important : check the cuda version installed on your computer by running the command in the cmd :\nnvcc -- version\nconda install -c pytorch pytorch-nightly torchvision cudatoolkit=9.0\n \ngit clone github.com/cocodataset/cocoapi.git\n \n    #To prevent installation error do the following after commiting cocooapi :\n    #using file explorer  naviagate to cocoapi\\PythonAPI\\setup.py and change line 14 from:\n    #extra_compile_args=[&#039;-Wno-cpp&#039;, &#039;-Wno-unused-function&#039;, &#039;-std=c99&#039;],\n    #to\n    #extra_compile_args={&#039;gcc&#039;: [&#039;/Qstd=c99&#039;]},\n    #Based on  github.com/cocodataset/cocoapi/issues/51\n \ncd cocoapi/PythonAPI\npython setup.py build_ext install\n \n# navigate back to INSTALL_DIR\ncd ..\ncd ..\n# install apex\n \ngit clone github.com/NVIDIA/apex.git\ncd apex\npython setup.py install --cuda_ext --cpp_ext\n# navigate back to INSTALL_DIR\ncd ..\n# install PyTorch Detection\n \ngit clone github.com/Idolized22/maskrcnn-benchmark.git\ncd maskrcnn-benchmark\n \n# the following will install the lib with\n# symbolic links, so that you can modify\n# the files if you want and won&#039;t need to\n# re-build it\npython setup.py build develop\n踩坑\n我卡了三天，各种安装卸载Visual Studio,总结以下问题：\n\n\n可以不需要Visual Studio！！！\n\n\npytorch1.0中，torchvision安装错误\n\n出问题先import torch import torchvision 看是否出错，我的torchvision是pillow库出错，换了个版本的pillow库解决的。\n\n\n\n编译时找不到文件\n\n上述提到到编译软件需要添加环境变量\n\n\n\n\n\nCUDA编译错误\n\nCUDA文件需要按照上述安装步骤中做相应改动，我文件改错了所以导致cuda一直出错\n\n\n\nLink Error 链接错误\n\n直接百度错误，复制两个文件到一个目录即可\n\n\n\nDemo运行\nJupyter Notebook 程序我这边没跑起来，但是摄像头demo、自己的本地demo跑起来了，Jupyter Notebook  demo跑不起来可以用以下的代码代替：运行直接 python demo.py .\n#!--*-- coding:utf-8 --*--\n \nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\n \n#import requests\nfrom io import BytesIO\nfrom PIL import Image\nimport numpy as np\n \npylab.rcParams[&#039;figure.figsize&#039;] = 20, 12\n \nfrom maskrcnn_benchmark.config import cfg\nfrom predictor import COCODemo\n \n \n# 参数配置文件\nconfig_file = &quot;../configs/caffe2/e2e_mask_rcnn_R_50_FPN_1x_caffe2.yaml&quot;\n \ncfg.merge_from_file(config_file)\ncfg.merge_from_list([&quot;MODEL.DEVICE&quot;, &quot;cpu&quot;])\n# cfg.MODEL.WEIGHT = &#039;../pretrained/e2e_mask_rcnn_R_50_FPN_1x.pth&#039;\n \ncoco_demo = COCODemo(cfg, min_image_size=800, confidence_threshold=0.7, )\n \n \nimgfile = &#039;../images/1.jpg&#039;\npil_image = Image.open(imgfile).convert(&quot;RGB&quot;)\n \nimage = np.array(pil_image)[:, :, [2, 1, 0]]\n \n# forward predict\npredictions = coco_demo.run_on_opencv_image(image)\n \n# vis\nplt.subplot(1, 2, 1)\nplt.imshow(image[:,:,::-1])\nplt.axis(&#039;off&#039;)\n \nplt.subplot(1, 2, 2)\nplt.imshow(predictions[:,:,::-1])\nplt.axis(&#039;off&#039;)\nplt.show()\n效果图\n\n"},"E_Technique_Explores/windwos下几款实用又美观的软件推荐":{"slug":"E_Technique_Explores/windwos下几款实用又美观的软件推荐","filePath":"E_Technique_Explores/windwos下几款实用又美观的软件推荐.md","title":"Windwos下几款实用又美观的软件记录","links":[],"tags":["软件"],"content":"本人是windows重症患者，经常在搜集乱七八糟（好看）的软件，因此来记录几款实用的软件。\n1. Typora\nmarkdown笔记软件，美观又实用！\n官网：typora.io/\n\n2. Choaolatey\n这是一款windows下的包管理器，可以像 linux 一样安装各种包，很是方便！\n官网：chocolatey.org/\n\n3. hyper\nhyper是 windows下面很炫酷的一个终端嘛，有各种皮肤、插件！\n官网：hyper.is/\n\n4. Listary\n这是一款很方便搜索打开本地文件的软件，有很多功能值得探索\n官网：www.listary.com/\n\n5. IobitUninstaller\nwindows上卸载软件用的一款软件，超好用\n\n后续再有好的继续补充…"},"E_Technique_Explores/一套用于人体姿态估计的关键点标注流程":{"slug":"E_Technique_Explores/一套用于人体姿态估计的关键点标注流程","filePath":"E_Technique_Explores/一套用于人体姿态估计的关键点标注流程.md","title":"一套针对人体姿态估计的在视频上的关键点标注流程","links":[],"tags":["数据标注","软件"],"content":"最近我们项目需要标注数据集的人体关键点，所以我们开发了一套标注的简单流程，这里来记录一下。\n因为关键点太多，不可能人为的取标注很多东西，所以我们要借助现有的算法先计算在矫正。\n一、标注工具 LabelMe\n项目地址： github.com/wkentaro/labelme\n安装：\n建议在python3环境下运行，python2我自己未测试过，具体的相关的可以到项目地址看\n1.创建虚拟环境\nconda create -n labelme python=3.6\n2.激活环境\nsource activate labelme\n3.安装labelme\npip install labelme\n使用：\n激活虚拟环境后，输入 labelme 打开软件\n二、数据标注流程\n\n用写好的脚本把视频拆分成单帧；\n用OpenPose跑出所有的图片的帧（或者OpenPose直接在视频上跑）,会得到每张图片的关键点的一个json文件；\n用我们写好的脚本把json转换成labelme格式的json;\n用labelme打开文件夹进行标注\n标注完成后在执行一次脚本，补充删除得点\n\nlabelme之后的标注流程可以参考下面的gif动图：\n\n\n目前这套流程已经开始测试使用。\n关于上面提到的脚本，我回头都会集成到实用工具类下面，或者会单独一个labelme的项目上传到github。\n三、标注指南\n一、关键点位置\nBody\n\nHand\n\n手的坐标用原来的21个太多了，人也看不清楚，所以选取了其中11个点(手指尖、手心)，需要矫正\n\n\n\n二、标注相关\n1.遮挡问题\n如果遇到手部遮挡，这样人也是标不出来的，所以直接删除被遮挡的手关节，(身体关节被遮挡、出画后如果检测不出来是(0,0)坐标，这时候就不用管)一个个点的删除比较慢，如果需要批量删除，可以如下操作：（动图pdf无法播放，会在群里再发一份）\n注意： 所有的左上角(0,0)的点都可以不用去考虑\n\n其中，如果没有polygon Labels这个控制框，可以再view菜单下调出来\n2.运动模糊问题\n碰到人的手很模糊，人眼都很难分辨出来，手的点又比较大，这时候标注会很困难，碰到这种情况，直接保留手的关键点，可以跳过手的标注。例如下图：\n\n附加：服务器与本地同步\n关于服务器与本地同步很慢的问题，可以在服务器使用百度云即可。"},"E_Technique_Explores/个人信息管理系统-Web项目总结分析":{"slug":"E_Technique_Explores/个人信息管理系统-Web项目总结分析","filePath":"E_Technique_Explores/个人信息管理系统-Web项目总结分析.md","title":"个人信息管理系统-JavaWeb项目总结分析","links":[],"tags":["Website","Code"],"content":"\n最近学习 javaweb 阶段 ，基本学习了原始的 jsp 开发，完成了一个比较综合的案例，记录一下各种操作的逻辑，以及具体的代码实现细节。\n\n\n技术概览：\n前端 页面采用 原生   html + css + javascript ;\n后端 采用  Java， Jsp开发，具体包含原生 servlet，request，response；\n该系统主要包含功能：\n\n登录\n显示用户信息\n\n分页显示\n\n\n添加联系人\n删除联系人：\n\n单条删除\n多选删除\n全选全部选\n\n\n修改联系人；\n根据条件查询联系人；\n\n\n项目架构设计\n\n如图，首先按web三层结构（ **浏览器 ⇒ 服务器 ⇒ 数据库 **）来分\n\nweb 表示层  ---  包括  控制器 Servlet  Jsp页面\n业务逻辑层 --- 业务实现逻辑\n数据访问层 --- 具体操作数据库\n\n首先拿到前端页面，修改一下逻辑\n\n功能实现\n登录 Login\n\nlogin.jsp 表单提交到  loginServlet ;\n\n&lt;form action=&quot;${pageContext.request.contextPath}/loginServlet&quot; method=&quot;post&quot;&gt;\n\n loginServlet  逻辑\n\n验证码校验：\n\n\n\n// 获取填写的验证码值\nString verifycode = request.getParameter(&quot;verifycode&quot;);\n// 获取真实验证码值\nHttpSession session = request.getSession();\nString checkcode_server = (String) session.getAttribute(&quot;CHECKCODE_SERVER&quot;);\nsession.removeAttribute(&quot;CHECKCODE_SERVER&quot;);\n//判断 验证码错误转发到该页面\n// 转发仅在当前页面\nif(!checkcode_server.equalsIgnoreCase(verifycode)){\n    request.setAttribute(&quot;login_msg&quot;,&quot;验证码错误&quot;);\n    request.getRequestDispatcher(&quot;/login.jsp&quot;).forward(request,response);\n    return;\n}\n\n\n\n获取参数，封装User对象，\n传入 service.login(user)\n结果不为空，返回  User  对象，重定向到首页\n\nresponse.sendRedirect(request.getContextPath()+&quot;/index.jsp&quot;);\n\n\n结果为空，设置错误信息，转发到当前页面\n\nrequest.getRequestDispatcher(&quot;/login.jsp&quot;).forward(request,response);\n\n\nservlet.login   数据库实现\n\n\nString sql = &quot;select * from user where username=? and password=?&quot;;\nUser loginUser = template.queryForObject(sql, new BeanPropertyRowMapper&lt;User&gt;(User.class), user.getUsername(), user.getPassword());\nreturn user;\n添加联系人\n\n添加联系人的页面  add.jsp  ，填写表单信息后提交到  addInfoServlet\n\n&lt;form action=&quot;${pageContext.request.contextPath}/addInfoServlet&quot; method=&quot;post&quot; id=&quot;form&quot;&gt;\n\n填写信息之后要进行表单校验，采用 js 实现：\nwindow.onload = function () {\n\tfunction nameChecked() {\n                var name = document.getElementById(&quot;name&quot;).value;\n                var reg_name = /^[\\u4e00-\\u9fa5]{1,8}$/;\n                var flag = reg_name.test(name);\n                var block = document.getElementById(&quot;n_c&quot;);\n                if (!flag) {\n                    block.innerHTML = &quot;名字只能输入汉字哦！&quot;;\n                }else {\n                    block.innerHTML = &quot; &quot;;\n \n                }\n \n                return flag;\n            }\n    document.getElementById(&quot;name&quot;).onblur = nameChecked;\n    document.getElementById(&quot;form&quot;).onsubmit = function () {\n                return nameChecked();\n            }\n}\n\naddInfoServlet   逻辑\n\n\n获取参数信息，封装对象\nMap&lt;String, String[]&gt; infoMap = request.getParameterMap();\n\n\n调用service.addInfo(addUser)  添加信息\n\n\n重定向到展示信息页面\nresponse.sendRedirect(request.getContextPath()+&quot;/findUserByPageServlet&quot;);\n\n\nservice.addInfo(addUser)`  数据库操作\nString sql = &quot;insert into user (name,gender,age,address,qq,email)  values &quot; +\n                &quot;(?,?,?,?,?,?)&quot;;\ntemplate.update(sql, user.getName(), user.getGender(), user.getAge(), user.getAddress(), user.getQq(), user.getEmail());        \n删除联系人\n\n单条信息删除，点击删除后跳转至 deleteInfoServlet ，并且通过后缀 ?id=${user.id} 传入参数\n\n&lt;a href=&quot;${pageContext.request.contextPath}/deleteInfoServlet?id=${user.id}&quot;&gt;删除&lt;/a&gt;\n\n根据当前id 删除该条信息\n\nString sql = &quot;delete from user where id=?&quot;;\ntemplate.update(sql, id);\n\n重定向到信息展示页面\n\n修改联系人信息\n\n点击修改按钮，传入当前id，跳转至  FindUserServlet\n\n&lt;a class=&quot;btn btn-default btn-sm&quot; href=&quot;${pageContext.request.contextPath}/FindUserServlet?id=${user.id}&quot;&gt; 修改 &lt;/a&gt;\n\n获取当前联系人信息记录，将User对象设置到域对象，转发到 update.jsp 文件中，在该文件中信息输入框设置 value属性，将设置的Usre信息回显\n将  update.jsp  表单信息提交到  updateInfoServlet  ，在该servlet中，与上述逻辑类似，更新信息。\n\n分页显示信息\n封装page对象返回给浏览器，具体包含：\nprivate int totalCount; // 总记录数\nprivate int totalPage;  // 总页数\nprivate List&lt;T&gt; list;   // 每页数据\nprivate int currentPage; // 当前页数\nprivate int rows; // 每页的数目\n需要浏览器传给服务器 currentPage  和 rows  参数\n\n\n@WebServlet(&quot;/findUserByPageServlet&quot;)\npublic class FindUserByPageServlet extends HttpServlet {\n    protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {\n \n        request.setCharacterEncoding(&quot;utf-8&quot;);\n \n \n        String currentPage = request.getParameter(&quot;currentPage&quot;);\n        String rows = request.getParameter(&quot;rows&quot;);\n \n        if(currentPage == null || &quot;&quot;.equals(currentPage) ){\n            currentPage = &quot;1&quot;;\n        }\n \n        if (rows==null || &quot;&quot;.equalsIgnoreCase(rows)){\n            rows = &quot;7&quot;;\n        }\n \n        Map&lt;String, String[]&gt; conditions = request.getParameterMap();\n \n \n        UserService service = new UserServiceImpl();\n        PageBean&lt;User&gt; pageUsers =  service.findUserByPage(currentPage, rows, conditions);\n//        System.out.println(pageUsers);\n        request.setAttribute(&quot;pageUsers&quot;,pageUsers);\n        request.setAttribute(&quot;condition&quot;,conditions);\n        request.getRequestDispatcher(&quot;/list.jsp&quot;).forward(request,response);\n \n \n    }\n \n    protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {\n        this.doPost(request, response);\n    }\n}\n \n \n public PageBean&lt;User&gt; findUserByPage(String _currentPage, String _rows, Map&lt;String,String[]&gt; condition) {\n \n        int currentPage = Integer.parseInt(_currentPage);\n        int rows = Integer.parseInt(_rows);\n \n        if(currentPage&lt;=0){\n            currentPage = 1;\n        }\n \n        PageBean&lt;User&gt; page = new PageBean&lt;&gt;();\n \n        page.setRows(rows);\n \n        page.setCurrentPage(currentPage);\n \n \n        int totalCount = dao.findTotalCount(condition);\n        page.setTotalCount(totalCount);\n \n        int start = (currentPage - 1) * rows;\n        List&lt;User&gt; userList = dao.findUserByPage(start, rows, condition);\n        page.setList(userList);\n \n        int totalPage = totalCount % rows == 0 ? totalCount / rows : totalCount / rows + 1;\n        page.setTotalPage(totalPage);\n \n        return page;\n    }\n}\n public int findTotalCount(Map&lt;String, String[]&gt; condition) {\n \n        String sql = &quot;select count(*) from user where 1=1 &quot;;\n        StringBuilder sb = new StringBuilder();\n        sb.append(sql);\n \n        List&lt;Object&gt; params = new ArrayList&lt;Object&gt;();\n \n        for (String key : condition.keySet()) {\n \n            if (&quot;currentPage&quot;.equals(key) || &quot;rows&quot;.equals(key)) {\n                continue;\n            }\n \n            String value = condition.get(key)[0];\n            if (value != null &amp;&amp; !&quot;&quot;.equals(value)) {\n                sb.append(&quot; and &quot; + key + &quot; like ? &quot;);\n                params.add(&quot;%&quot; + value.replaceAll(&quot; &quot;,&quot;&quot;) + &quot;%&quot;);\n            }\n        }\n        return template.queryForObject(sb.toString(), Integer.class, params.toArray());\n    }\npublic List&lt;User&gt; findUserByPage(int start, int rows, Map&lt;String, String[]&gt; condition) {\n    String sql = &quot;select * from user where 1 = 1 &quot;;\n    StringBuilder sb = new StringBuilder();\n    sb.append(sql);\n \n    List&lt;Object&gt; params = new ArrayList&lt;Object&gt;();\n \n    for (String key : condition.keySet()) {\n \n        if (&quot;currentPage&quot;.equals(key) || &quot;rows&quot;.equals(key)) {\n            continue;\n        }\n \n        String value = condition.get(key)[0];\n        if (value != null &amp;&amp; !&quot;&quot;.equals(value)) {\n            sb.append(&quot; and &quot; + key + &quot; like ? &quot;);\n            params.add(&quot;%&quot; + value.replaceAll(&quot; &quot;,&quot;&quot;)  + &quot;%&quot;);\n        }\n    }\n    params.add(start);\n    params.add(rows);\n    sb.append(&quot; limit ?, ? &quot;);\n    List&lt;User&gt; userList = template.query(sb.toString(), new BeanPropertyRowMapper&lt;User&gt;(User.class), params.toArray());\n    return userList;\n}"},"E_Technique_Explores/从0开始搭建私人云盘":{"slug":"E_Technique_Explores/从0开始搭建私人云盘","filePath":"E_Technique_Explores/从0开始搭建私人云盘.md","title":"从0开始搭建私人云盘","links":["E_Technique_Explores/Ubuntu16-18可道云云盘搭建"],"tags":["Cloud","Website"],"content":"Ubuntu16-18可道云云盘搭建\n现在的云盘有很多，百度云、蓝奏云、腾讯微云等等，看起来似乎是没有搭建云盘的必要，但是百度云限速让我觉得恶心，不开超级会员就非常非常非常慢，开了又觉得亏，所以就想到了能不能自己搭建一个云盘。自己搭建的私有云还有一个特点就是比较安全，其实只是自己比较喜欢乱鼓捣新东西。现在的下载速度并不快，我还没研究，但是上传速度经过修改可以达到2M/s以上，其实我的云服务器比较垃圾，毕竟是1块钱白嫖了一个月—。\n先上链接：\nhttp://111.229.74.50/kodexplorer/index.php\n\n购买一个云服务器\n云服务器性能越好，与上传下载速度都有很大关系，结合自己个人需求、经济状况适当购买。当然也可以先白嫖一个用用体验一下。购买云服务器国内比较火的也就几个：**阿里云，腾讯云，京东云，华为云。**我用的腾讯云的服务器。↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓\n\n开始配置服务器\nNote：服务器以windows操作系统为例；本私有云基于可道云kodexplorer\n远程登陆到服务器\n以腾讯云为例，上图中主机后面就有登录，可以直接登录，登录之后界面是这样的：(每个人的服务器界面会有点不一样，win比较方便，但是 linux更适合作为服务器系统)\n\n配置PHP环境\n下载安装Xampp (XAMPP 是一个把 Apache 网页服务器与 PHP、Perl 及 MariaDB 集合在一起的安装包，允许用户可以在自己的电脑上轻易的建立网页服务器环境。)\n中文官网下载地址 ： www.apachefriends.org/zh_cn/download.html。\n安装下一步下一步就可以，安装完之后开启Apache、 MySQL两项，如下图\n\n点击Admin进入相应页面（如下图）即说明已经配置好。就是这么容易 。\n\n下载和安装可道云kodexplorer\n下载地址：kodcloud.com/download.html\n下载完成后解压，将可道云文件夹复制到xmapp安装文件夹下的htdocs文件夹，此时已经完成\nNote: 可道云文件夹命名应为kodexplorer(有点常识都会理解)\n访问自己的私有云\n112.xxx.xx.xx/kodexplorer/index.php\n可道云配置优化\n操作完2已经可以访问上传下载添加用户了。但是此时上传速度很慢，此时就需要配置一下了，以下操作我亲自测试有用，但是下载还是比较慢，我再研究一下。\n修改php.ini上传限制\n在xmapp安装目录直接搜索该文件即可\nmax_execution_time = 3600\nmax_input_time = 3600\npost_max_size = 150M\nupload_max_filesize = 150M\n修改可道云配置\n在config/下新建 setting_user.php文件;粘贴如下内容；(已存在则略过)\n&lt;?php\n \n//分片上传: 每个切片5M,需要php.ini 中upload_max_filesize大于此值\n$GLOBALS[&#039;config&#039;][&#039;settings&#039;][&#039;updloadChunkSize&#039;] = 1024*1024*5;   \n \n//上传并发数量; 推荐15个并发;\n$GLOBALS[&#039;config&#039;][&#039;settings&#039;][&#039;updloadThreads&#039;] = 15;\n \n\n完结，撒花，下载速度慢研究出来原因再更新\n"},"E_Technique_Explores/内网穿透工具":{"slug":"E_Technique_Explores/内网穿透工具","filePath":"E_Technique_Explores/内网穿透工具.md","title":"内网穿透工具","links":[],"tags":["软件"],"content":"目前我用cpolar 作为内网穿透工具使用\n\n官网配置完后使用如下命令进行连接tcp：\n./cpolar tcp 22 -remote-addr=&lt;1.tcp.cpolar.io:10356&gt;\nTips\n\n可以根据使用的地点来创建穿透链接，比如老婆在美国，创建时就选择美国速度会快很多\n\n"},"E_Technique_Explores/拥有自己优雅的图床":{"slug":"E_Technique_Explores/拥有自己优雅的图床","filePath":"E_Technique_Explores/拥有自己优雅的图床.md","title":"拥有自己优雅的图床","links":[],"tags":["Cloud","Website"],"content":"图床，就是专门用来存放图片的空间，不过与本地不同，图床是存储在网络上的。这样图片会有个地址，可以访问到图片，也可以引用。如果写个人博客等等肯定用得上。github这个天然的图床不用就太可惜了！ 所以选择github来作为自己图床。\n\n仓库准备\n可以新建一个仓库，也可以在现有的仓库下。具体怎么建仓库请自己寻找。\n下面以我自己的仓库为例\n\n进入issues新建问题\n点击issues，然后点击右上角NewIssues，进入如下界面\n\n关键部分红色框已经框出来，可以在这里上传自己的图片，然后点击提交即可，图下图\n\n此时点击图片，即可打开一个新的链接，此时图片的网址就可以被引用了。\nEnjoy！"},"E_Technique_Explores/斐波那契额数列几种实现方式":{"slug":"E_Technique_Explores/斐波那契额数列几种实现方式","filePath":"E_Technique_Explores/斐波那契额数列几种实现方式.md","title":"斐波那契数列几种实现方式","links":[],"tags":["Code","刷题"],"content":"今天在刷题时碰到了斐波那契额数列，然后发现有点忘记了，来记录一下几种实现方式。\n递归法\ndef Fibonacci(i):\n    if i &lt;= 1:\n        return i\n    else:\n        return Fibonacci(i-1) + Fibonacci(i-2)\n \nif __name__ == &quot;__main__&quot;:\n    print(Fibonacci(100))\n这种方法非常慢，有大量重复计算，复杂度是指数级\n递推式\ndef Fibonacci(i):\n   a, b = 0, 1\n   for i in range(i):\n       a, b = b, a+b\n   return a\nif __name__ == &quot;__main__&quot;:\n    for i in range(39):\n        print(Fibonacci(i))\n该方法复杂度是线性的\n生成器\ndef Fibonacci(n):\n   a, b = 0, 1\n   while(n&gt;0):\n       a, b = b, a+b\n       yield a\n       n = n - 1\nif __name__ == &quot;__main__&quot;:\n    for i in Fibonacci(10):\n        print(i)\n暂时记录三种方法。"},"E_Technique_Explores/服务器初始环境安装与配置":{"slug":"E_Technique_Explores/服务器初始环境安装与配置","filePath":"E_Technique_Explores/服务器初始环境安装与配置.md","title":"服务器初始环境安装与配置","links":[],"tags":["linux","环境"],"content":"\n记录服务器初期配环境的一些过程。\n\n\n显卡驱动\nAnaconda\npip\ntorch\n环境集成\n\n\n1. 显卡驱动\nUbuntu20.04安装cuda10.1的步骤(图文教程)\n英伟达 NVCC\n\n2. Anaconda安装以及镜像设置\n[Ubuntu18.04安装Anaconda(最新最全亲测图文并茂)\nanaconda | 镜像站使用帮助 | 清华大学开源软件镜像站 | Tsinghua Open Source Mirror\nUbuntu Anaconda环境变量配置_Tender_Li的博客-CSDN博客_ubuntuanaconda添加环境变量\n配置完无法创建环境，一直卡着，无需刷清华镜像\n\n3. pip刷清华镜像\npip config set global.index-url &lt;pypi.tuna.tsinghua.edu.cn/simple&gt;\npip config set global.trusted-host pypi.tuna.tsinghua.edu.cn\n\n4. torch / torchvision 安装包\ntorch 下载\nLinks for torchvision\n【torch与torchvision对应版本安装】\ndownload.pytorch.org/whl/cu100/torch_stable.html\n\n\n5. pytorch相关的万能环境\npip install -r pose_env.txt\npose_env.txt\npose_env.txt"},"E_Technique_Explores/爬虫入门":{"slug":"E_Technique_Explores/爬虫入门","filePath":"E_Technique_Explores/爬虫入门.md","title":"爬虫入门","links":[],"tags":["Code","爬虫"],"content":"近期出于某些原因突然对爬虫来了兴趣，于是花了几个小时研究了一下爬虫，重新捡起来了，理清了爬虫的一个流程，以前学了很久一直不是很懂，不会自己去爬需要的东西。理清楚之后很简单，很多东西一目了然。在此记录一下简单的爬虫流程。\n爬虫的大致流程\n\n抓取到整个url的html代码\n解析代码中需要的部分\n数据保存\n高级爬虫（进阶）\n\n上述流程中，我自己只学了前两个部分，对我自己来说也是够用了，第三部分对于不以爬虫为工作的，我认为就不用专门去学习了，可以写入txt文件或者直接下载图片。。至于最后一部分就涉及到爬虫的框架，分布式爬虫等等，这个对于个人来说我觉得也是不必要的。\n具体知识点\n抓取url\n我用的是方便、简单的requests库，操作如下：\nimport requests\nurl = &#039;www.baidu.com&#039;\nheaders = {&#039;User-Agent&#039;: &quot;Mozilla/5.0 (Windows; U; Windows NT 6.1; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50&quot;}\nresponse = requests.get(url=url, headers=headers)\nreponse.status_code # 200表示正常访问\n#抓取到的html的所有代码\nhtml_text = response.text\n第一步比较简单，几行代码，当然其中也有高级操作，比如代理之类的，后面在讲。\n解析html 重点\nxpath比较全的一个博客：www.jianshu.com/p/85a3004b5c06\n拿到html内容后，就要分析一下，找出自己需要的内容，html解析有xpath、BeautifulSoup 等库，由于Bs4我并不会（虽然学过），所以这里就用xpath~\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n表达式描述nodename选取此节点的所有子节点/从当前节点选区直接子节点//全局查找某个节点.选取当前节点..选取当前节点的父节点@选取属性and多个属性同时匹配\n我认为会这些基本就够了，可以抓取下来任何自己想要的了，至少解析不会有问题。\nimport lxml.etree\nparser = etree.HTMLParser(encoding=&#039;utf-8&#039;) # 创建解释器\nhtml = etree.parse(&#039;baidu.html&#039;, parser=parser) # 解析html\n# xpath返回的是list\n# 全局搜索id为pins的ul标签下面所有的li标签下面的a标签中的href(链接)\n# @为精确匹配\nlink = html.xpath(&quot;//ul[@id=&#039;pins&#039;]//li/a/@href&quot;)\n \n# 全局搜索class为&quot;item-0&quot;的li标签下面的第一个a标签中的文本 用/text()获取\nresult = html.xpath(&#039;//li[@class=&quot;item-0&quot;]/a/text()&#039;)\n \n# and表示同时满足多个属性\nresult = html.xpath(&#039;//li[contains(@class, &quot;li&quot;) and @name=&quot;item&quot;]/a/text()&#039;)\n \n# 模糊匹配 全局搜索class名称包含&#039;li&#039;的li标签下面的第一个a标签中的文本  \nresult = html.xpath(&#039;//li[contains(@class, &quot;li&quot;)]/a/text()&#039;)\n \n至此就解析完成，拿到了自己想要的，爬虫基本也随之结束了。\n拿到的是一堆字符串，要下载要提取就用python的方法处理即可，关于下载图片可以用requests直接保存：\nh = open(img_name,&#039;wb&#039;) # img_name 为要保存的图片名称\nh.write(requests.get(img_src,headers=header).content) #img_src为字符串\nh.close()\n问题、解决：\n\n抓取url\n\n在抓取url时(get)，有时候非常慢，或者是下载多了ip会直接被封。有以下几个解决方案:\n\n\n爬虫时例如下载图片时，一定要有时间间隔\ntime.sleep(1)\n\n\n动态获取ip：\n# ------------------------------------------------------------------------------\n# # @Time    : 2020/6/7 下午 8:09\n# # @Author  : fry\n# @FileName: ip.py\n# ------------------------------------------------------------------------------\nimport requests\nimport time\nfrom lxml import etree\ndef pnw():  # 定义一个循环的函数\n    headers = {\n        &#039;User-Agent&#039;: &quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.1 &quot;\n                      &quot;(KHTML, like Gecko) Chrome/14.0.835.163 Safari/535.1&quot;}\n    # 依次遍历生成2-99\n    ip_address = []\n    for i in range(2, 100):\n        url = &quot;www.kuaidaili.com/free/inha/&quot; + str(i) + &quot;/&quot;  # 爬取的免费ip\n        response = requests.get(url, headers=headers).text  # 获得网页文本数据\n        response_xpath = etree.HTML(response)  # 转换为xpath可用结构\n        ips = response_xpath.xpath(&#039;//[@id=&quot;list&quot;]/table/tbody/tr/td[1]/text()&#039;)  # ip的信息\n        dks = response_xpath.xpath(&#039;//*[@id=&quot;list&quot;]/table/tbody/tr/td[2]/text()&#039;)  # 端口的信息\n        https = response_xpath.xpath(&#039;//*[@id=&quot;list&quot;]/table/tbody/tr[2]/td[4]/text()&#039;)  # http信息\n        for ip, dk, http in zip(ips, dks, https):\n            proxy = &quot;http://&quot; + ip + &quot;:&quot; + dk  # 拼接ip\n            proxies = {&quot;http&quot;: proxy}\n            ip_address.append(proxies)\n    return ip_address\nif __name__ == &#039;__main__&#039;:\n    ips = pnw()\n    print(ips)\n以上代码可以获取ip，使用时:\nip = pnw()\npic_page = requests.get(url=url_pic, headers=header,proxies=ip[0]) #使用ip代理\n\n\n基本如此，掌握之后可以随心所欲爬取自己所需。\n\n\n\n如果想要爬一个网站所有的东西，需要自己去分析一下url规律，比如哪个标签里面有所有的链接等等\n\n\n关于反爬机制，用好代理\n\n\n最后，关于爬虫的合法性：\nurl拼接上/robots.txt  可以看到网站的爬虫协议：\nUser-agent:  Baiduspider # 允许百度爬虫引擎\nAllow:  /article # 允许访问/article.htm，/article/12345.com\nAllow:  /oshtml      \nAllow:  /wenzhang\nDisallow:  /product/ # 禁止访问/product/12345.com\nDisallow:  /         # 禁止了访问除Allow规定页面的其他所有页面\n \nUser-Agent:  Googlebot  # 谷歌爬虫引擎\nAllow:  /article\nAllow:  /oshtml\nAllow:  /product # 允许访问/product.htm，/product/12345.com\nAllow:  /spu\nAllow:  /dianpu\nAllow:  /wenzhang\nAllow:  /oversea\nDisallow:  /\n \n但是这只是网站的协议，具体的：\n从目前的情况来看，如果抓取的数据属于个人使用或科研范畴，基本不存在问题; 而如果数据属于商业盈利范畴，就要就事而论，有可能属于违法行为，也有可能不违法。"},"E_Technique_Explores/牛客网刷题笔记-剑指offer系":{"slug":"E_Technique_Explores/牛客网刷题笔记-剑指offer系","filePath":"E_Technique_Explores/牛客网刷题笔记-剑指offer系.md","title":"二维数组中的查找","links":[],"tags":["刷题","Code"],"content":"自己最近正好有空，因此开始了刷题系列，牛客网上的剑指offer题目。今天记录一个比较有思想的题目。因为这些都是算法题目，如果去用穷举蒙混过关就没意思了，因此尽量找比较优化的算法。我这里用python来解答。\n最近正好也是有空，因此开始了刷题系列，牛客网上的剑指offer题目。今天记录一个比较有思想的题目。因为这些都是算法题目，如果去用穷举蒙混过关就没意思了，因此尽量找比较优化的算法。我这里用python来解答。\n题目：\n在一个二维数组中（每个一维数组的长度相同），每一行都按照从左到右递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个函数，输入这样的一个二维数组和一个整数，判断数组中是否含有该整数。\n思路：\n思路一：\n因为数组的有序性，从左到右从上到下递增，利用这一个规律，选取左下角的数为基点，如果目标数大于该数，那么目标肯定在基点的右边，所以 **column + 1 **; 如果目标小于基点数，那么 **row - 1 **。\n逻辑是这样，每移动一次，都会以目前所在的点为基点，因此会排除一行或者一列，这种复杂度就会比较低了。‘\n代码\n# -*- coding:utf-8 -*-\nclass Solution:\n    # array 二维列表\n    def Find(self, target, array):\n        hang = len(array)\n        lie = len(array[0])\n        i = hang - 1\n        j = 0\n        result = False\n        while(i&gt;=0 and j&lt;lie):\n            if target &lt; array[i][j]:\n                i = i - 1\n            elif target &gt; array[i][j]:\n                j = j + 1\n            elif target == array[i][j]:\n                result = True\n                return result\n        return result\n思路二：\n遍历每一行用二分查找法，这个代码还没写，所以明天补上，这种方法比较通俗易懂。"},"E_Technique_Explores/自己的域名与github绑定":{"slug":"E_Technique_Explores/自己的域名与github绑定","filePath":"E_Technique_Explores/自己的域名与github绑定.md","title":"自己的域名与github绑定","links":["E_Technique_Explores/拥有自己优雅的图床","E_Technique_Explores/MyBlog-Hexo快速搭建","E_Technique_Explores/hexo-blog搭建2-主题相关","E_Technique_Explores/hugo_blog搭建部署笔记"],"tags":["Website","blog"],"content":"*博客系列教程\n拥有自己优雅的图床\nMyBlog-Hexo快速搭建\nhexo-blog搭建2-主题相关\nhugo_blog搭建部署笔记\n一般情况下用github托管的博客很方便，但是域名访问只能用 xxxx.github.io来访问，这时候就有与自己的域名绑定的问题，即自己的域名指向github地址。（以腾讯云为例）\n\n准备工作\n开始操作前，确保自己已经有：\n\n可用的域名：注意不需要备案（我的是这样），但是大多数需要实名认证\n有自己的github项目地址，确保可以访问\n\n域名解析\n进入腾讯云控制台，在自己的域名解析处设置如下：\n\n这里要注意两点：\n\n记录类型选择 CNAME\n记录值为自己的github项目地址（github域名）\n\n此时如果解析成功，ping一下测试：\n\ngithub仓库设置\n进入github项目仓库设置，往下翻在 Github Page 中自定义地址写自己的域名 然后保存，此时主页会自动生成 CNAME 文件，如下图：\n\n注意\n等待时间\n等待10-15min，就可以通过自己的域名来访问了。注意能ping通说明已经成功，此时就不要频繁修改设置了，影响同步 时间\n普适性\n以上流程在hugo博客上毫无问题，原因是hugo博客推送到服务器是推送public中的内容，而CNAME在根目录并不会影响；\n但是在hexo博客上就有问题了，我查了一下发现如果直接在gtihub直接指定，会在github仓库上直接生成CNAME文件，而这时候有个问题就是在写文章同步的话，本地并没有CNAME文件，会直接覆盖掉已经生成的CNAME 文件，此时指定的域名就不生效了。因此有了以下的章节。\nHexo博客绑定域名\n首先在本地 博客根目录中source文件夹下新建一个CNAME文件，内容就是自己的域名，然后再按照上面去github指定。"},"E_Technique_Explores/计算机网络-IP地址-端口":{"slug":"E_Technique_Explores/计算机网络-IP地址-端口","filePath":"E_Technique_Explores/计算机网络-IP地址-端口.md","title":"计算机网络-IP地址&端口","links":[],"tags":["计算机网络"],"content":"IP 地址\nip地址作为计算机的唯一标识符，每个计算机有唯一的IP地址。\nIPV4 用 32位(bit)表示ip地址，也就是4字节（byte）.\n为了便于表示，用 a.b.c.d 表示。每个二进制都为8bit，十进制为0~255.\n2^8=255\n 127.0.0.1 //本机ip\n localhost//本机\n根据最高位地址划分为ABCDE类。\nIPV6 用  8 组，每组16位（bit）表示ip地址，一共128位\n端口号\n端口号是计算机之间通信时，每个通信软件有固定的端口号。例如qq，不同计算机上qq的端口号相同，消息才得以互通。\n长度16位，0~65535\n1000以下已经分配。\n80 为www服务端口\n3306 mysql服务端口\n8080 www代理服务\n127.0.0.1 //本机ip\nlocalhost//本机"},"E_Technique_Explores/记录MMDetection-ConvNext配置过程":{"slug":"E_Technique_Explores/记录MMDetection-ConvNext配置过程","filePath":"E_Technique_Explores/记录MMDetection-ConvNext配置过程.md","title":"记录MMDetection-ConvNext配置过程","links":[],"tags":["CNN","环境"],"content":"大致按照官方文档来，此处主要是针对ConvNext的个性化：\nConvNeXt/object_detection at main · FRunyang/ConvNeXt\nGitHub - SwinTransformer/Swin-Transformer-Object-Detection at 6a979e2164e3fb0de0ca2546545013a4d71b2f7d\nGitHub - open-mmlab/mmdetection: OpenMMLab Detection Toolbox and Benchmark\nConvNext模型的检测部分使用的SwinTransformer的Detection代码，SwinTransformer的 Detection又是机遇MMDetction的，所以环境配置主要是配置MMDetction。\n\nmmdetection/get_started.md at master · open-mmlab/mmdetection\n这个过程中碰到的几个坑：\n\nmmcv版本要求在1.3.1-1.7.0之间，所以可以直接安装 mmcvfull = 1.3.1\n\nInstallation - mmcv 1.6.1 documentation\n\n安装mmdetection用pip install mmdet，安装完之后在{dir/requiremets}里面的一些依赖也要安装一下，然后在根目录执行pip install -v -e .，不然会出现注册器xxxx一堆报错，命令行无法运行。\nConvNext模型输出的结果是列表，[0]看起来是80类的box，[1]是分割的边界。因为是80类，所以推测是COCO数据集的检测类别，按照COCO的逻辑第0类即[0][0]为人物的box数据.\n"},"F_Others/index":{"slug":"F_Others/index","filePath":"F_Others/index.md","title":"index","links":["F_Others/软件购买信息记录","F_Others/过去，现在，将来？-致自己","F_Others/将夜人物群像","F_Others/《盗墓笔记》里的终极到底是什么？-—-Ailyf-的回答---知乎","F_Others/为何对于《将夜》，许多人都宁可选择莫山山，而不选择桑桑？---知乎","F_Others/我的楼兰","F_Others/我记得"],"tags":[],"content":"\n一些反思、思考，记录某些时候突然想写的\n\n\n信息保存\n软件购买信息记录\n随性\n过去，现在，将来？-致自己\n将夜人物群像\n《盗墓笔记》里的终极到底是什么？ — Ailyf 的回答 - 知乎\n为何对于《将夜》，许多人都宁可选择莫山山，而不选择桑桑？ - 知乎\n我的楼兰\n我记得"},"F_Others/《盗墓笔记》里的终极到底是什么？-—-Ailyf-的回答---知乎":{"slug":"F_Others/《盗墓笔记》里的终极到底是什么？-—-Ailyf-的回答---知乎","filePath":"F_Others/《盗墓笔记》里的终极到底是什么？ — Ailyf 的回答 - 知乎.md","title":"《盗墓笔记》里的终极到底是什么？ — Ailyf 的回答 - 知乎","links":[],"tags":[],"content":"\nwww.zhihu.com/question/303391300/answer/613301021\n\n正文较长，有三万多字，提前预警。\n并非本人所写，原作者是暖和狐狸，摘自百度论坛。\n以下正文\n《盗墓笔记》之谜，就是“一个终极三条迷线！”\n一个终极：\n长白山地底大铜门——中华文明的源头，伏羲古氏族文明\n三条贯穿始终的迷线：\n第一条：\n史前青铜文明——蚩尤——广西——闷油瓶——失忆的一条迷之线！\n第二条：\n盗墓笔记的大主线：古代长生不老的秘密的发展！！\n第三条：\n盗墓笔记的大主线：现代各种力量关于长生不老秘密的博弈！！\n中华文明的起点，就在长白山！这就是终极！\n（三叔谓之：永远的云顶天宫）\n——————————————————————————————————————\n中华文明的起点，是“三皇之首”，也就是伏羲！伏羲是东夷部落的首领！现在的史前考古，把东夷部落的起点，指向红山文化，而东北，可能正是一个最古老的起点！所以，云顶天宫，是一切的终极起源！\n中华文明后来有三个大的分支：\n炎黄、蚩尤和西王母！他们，其实全部是伏羲的后裔和分支！\n炎黄这一支，经过战争而融合，变成了最“正宗”的脉络，成为华夏的祖先\n西迁的一支，母系氏族社会，就是在西域的西王母一支\n而蚩尤原来在河北一带，与黄帝的战争失败后，一部分后裔并入了炎黄部落、一部分南迁，进入湖南，成为苗族、瑶族，后来又南迁到广西、云南等。\n但他们共同的起源和祖先，都是伏羲！\n长生不老的秘密，本来也起源于伏羲，西王母只是发展了长生不老之术！\n伏羲时代，似乎人的寿命是很长的，记载伏羲本人就享寿1100岁！\n可见伏羲部落有长生之术！\n再来看伏羲部落的标志图腾：龙、蛇、鸟、虫！\n炎黄部落继承了龙的图腾\n蚩尤部落继承了蛇的图腾\n西王母应该是最完整的传承了伏羲的图腾：蛇、鸟、虫（尸蹩），或者说，西王母才是伏羲比较正宗的后裔！\n同时，各部落又各有自己的发展\n西王母的玉文化！\n蚩尤部落的青铜文化！\n而一切的起点，还是在长白山！\n第一条迷线：\n史前青铜文明——蚩尤——铁面生——广西——闷油瓶——失忆的一条迷之线！\n按正规历史的说法，中国青铜器时代始于夏朝，鼎盛于商周，但现在的史前考古，显然推翻了这点，远在远古时期，中华就出现过高度的青铜文明，四川的三星堆遗址就说明了这一点！\n有一种说法，蚩尤是史前青铜文化的创始者，而三星堆，正是蚩尤的后裔建造的。老三写盗墓的青铜树、大铜门显然都受到了三星堆之谜的影响（三星堆就出土了一颗巨大的青铜树）\n那么，我们由此可以推测：\n1、蚩尤是史前巨大青铜器的开创者，秦岭的青铜巨树就是蚩尤部落修建的，目的是为了祭祀！那时人们相信，这种巨树，可以直通上天。\n2、这种青铜，不是普通的青铜，而是有很强烈致幻功能的，人类从远古时代，巫师祭祀时就要服用致幻剂，这在各民族都有记载，巫师通过致幻才能与神灵沟通。在神树篇，我们可以看出这种树的祭祀功能，而且已经有了巫蛊的痕迹，而苗族的巫蛊正是从蚩尤传下来的！\n3、这种特殊的青铜，对人的精神有强烈的致幻和集体致幻功能，所以才用来造祭祀树\n或者说，这是一种“金属罂粟”，同时还能使人失忆！在蚩尤时代，巫师正是通过这种青铜树使部落中人在祭祀时产生集体幻觉！\n4、盗墓笔记中所有神秘的青铜都是这种青铜！\n鲁王宫中尸蹩尾巴上的和老痒耳朵上戴的青铜铃铛，就是能使人轻微致幻。\n海底墓中迷倒闷油瓶等的秘道中，挂满铃铛的青铜珊瑚树也是这种东西。\n5、掌握这种青铜秘密的可能只有蚩尤的后裔，它能引导人的意识和洗去人的记忆。\n在最近的这一部阴山古楼中，20年前考古队发现的“铁块”显然就是这种青铜。\n那么，蚩尤的后裔苗人和瑶人肯定是把这种古老的青铜藏在了广西，那个沉没于湖\n底的寨子，一定是这些蚩尤后裔的聚居地。\n6、那么，闷油瓶的身世就有另外一种可能，我还是认定张盐城、盘马、苗族首领，和闷是一个族裔的，都以麒麟纹身为标志！都是蚩尤的后裔！是一些知道和守护古老青铜秘密的人！\n7、盘马保留的“铁块”，闷住的竹楼床下的“危险品”，都是这种东西，但是闷应该知道这东西的危险啊，为什么还会一次次失忆呢？可能是闷为了某种使命或者获得某种能力，不得不一次次接触这种东西。\n8、秦岭神树这一部，其实就是在讲青铜的这种神秘致幻能力！它能创造人的意识！可以操纵别人的思维和意识！但副作用就是后来的失忆！\n9、对于永生中的作用而言，似乎就是洗去一个人的记忆，把另一个人的记忆植入后，把前一个人变为后一个人！\n以蚩尤后裔的视角看闷油瓶在鲁王宫和云顶天宫的表现！！\n————————————————————————————————————\n1、铁面生是书中隐藏的一个主线级重要角色，他是除汪藏海外第二个完全了解这些秘密的人！铁面生一定不会就让闷掐死那么简单。\n2、长沙血尸墓到底是谁的墓？因为是战国墓，从时代上看，必然是鲁王宫应该有的三具血尸中的一具！鲁殇王、铁面生或者周穆王，究竟谁更有可能呢？大家知道，吴邪爷爷从长沙血尸墓中盗出的战国帛书就是铁面生的手记！一个人把自己的手记放在自己的墓中不是更合理？长沙血尸墓是铁面生的墓可能性最大！\n3、那么。留在鲁王宫中的血尸还是周穆王和鲁殇王！周穆王是铁棺中起尸被闷砍下脑袋那具。鲁殇王是被掐死那具。\n4、闷对吴邪他们显然说了谎话，在三具血尸的问题上，他为什么要撒谎？说明三个人中必然有一个跟他有重大关系，他在替其中的一个人隐瞒什么\n5、还有，闷是怎么知道鲁殇王和铁面生这些事情的，铁面生的帛书被吴家盗走，又被裘德考骗到美国，小闷应该是无法看到的，小闷说他是在一个宋墓中找到过四卷战国帛书知道的，这显然是撒谎，要是这种帛书到处都是，也就不会被裘德考宝贝成那样了\n那么，小闷必是鲁殇王或者铁面生的后人，才有可能知道！相比于鲁殇王，铁面生显然要对整个秘密知道的多的多，小闷可能是铁面生的后裔！\n6、铁面生一定也是蚩尤的后裔！山东本来就是东夷部落的一个住址，铁面生本来就知道蚩尤时代的一些古老秘密，而且，他也是一个很有本事的奇人，精通易术、风水，算是倒斗行业的“大家”，否则就无法解释几千年只有他和汪藏海两人接近过那个巨大秘密！\n7、铁面生把自己的墓安在长沙，也说明他和蚩尤苗人的关系，因为那时的九黎三苗聚集在长沙！长沙那时属于荒蛮之地！\n8、那么，铁面生为什么不自己放在玉佣中永生呢？他应该完全有这个能力！答案是，因为他知道那个玉佣根本已经不可能永生了，吴邪他们进鲁王宫时，已经两千多年了，鲁殇王还在那里“脱皮”，虽然没死，但是两千年还不能复活，这种“永生”一定有问题了！一定出了差错！铁面生应该早就知道，那玉佣被脱下一次后就失效了。再进去只能变成活死人，还不如血尸！\n9、铁面生骗殇王相信这个，也许是要借机进穆王墓观察一下，并且取出什么东西（可能就是尸蹩王），然后再找地方实现自己的永生！可怜的殇王却不明就里，进了废了的玉佣成了活死人！\n10、铁面生一是选择了长沙风水宝地血地，二是从西王母处取回了那种黑色陨玉（他一定去过鬼城），大家记得对长沙血尸墓的描述，底下的密道里全部是黑色的玉石！就是说铁面生为自己做了个更高级更大的“玉佣”，写到这不得不佩服铁面生的才智。然后把自己写的帛书放进去。另外。他一定还保留了另一份帛书，传给了他的后人，也就是闷这一支！\n11、小闷的形象，一方面是战神的形象，而蚩尤正是中华的战神。铁面生，顾名思义，是一个冷面、不苟言笑的形象，像不像小闷？而且，铁面生的倒斗大家风范，正可以解释小闷出神入化的墓中表现！\n12、小闷进鲁王宫，一定有铁面生帛书的详细介绍，或者他之前也来过，所以分外熟悉\n关于克尸蹩的血液，大家记不记得，那些神秘铁块（古青铜）能使蚊虫不近，小闷\n作为蚩尤后裔和经常与青铜打交道的人，应该具有这种血液，而克制粽子，让古代女尸磕头，源自这种血的对魂魄施加影响的巫蛊能量。\n13、对铁棺中周穆王血尸磕头和对话，这里，作为蚩尤后裔，周穆王的身份就变为前辈\n和仇敌了。前辈是因为他们都是伏羲的后裔，仇人是因为周天子是黄帝的后裔，黄\n帝杀死了蚩尤！闷的磕头是源于此，对话必是铁面生一支祖传的克制粽子的办法。\n而且，从蚩尤到苗族，都是巫蛊的族群（比如赶尸），向来能够跟鬼神对话的，这\n和倒斗南派、北派都不一样！\n14、小闷自己走开，是为了争取时间，调换殇王帛书，拿走殇王的鬼玺。\n15、小闷杀死穆王血尸，是为报仇和保护自己和吴邪们的安全，掐死殇王血尸，是厌恶\n鄙视\n16、小闷撒谎，是为保护铁面生的秘密！\n云顶天宫：\n17、小闷对长白山磕头，是拜祭始祖伏羲\n18、小闷和阴兵一起进大铜门解谜：\n小闷进大铜门，是因为有调动阴兵的鬼玺！\n在墓道里，小闷在吴邪面前几秒钟的消失，正是他身上带着鬼玺的缘故。\n（1）、什么是阴兵？\n阴兵就是西王母殿中那些王俑，陨玉有保证千年不腐的功能。阴兵就是用这种方法保存的“千年僵尸”。所谓僵尸，根据传说和记载，更类似于一种动物或生物而不是人，他们没有高级思维，只凭一些简单反应和本能，比如说，僵尸不会说话，膝盖不能弯曲等都说明他们不具高级思维。只是千年不腐保证了他们的神经还能运动。\n（2）、做成阴兵的马脸巨人是什么？\n马脸巨人应该是古老西域的一个种族，因高大而战斗力很强，应该是西王母的“雇佣兵”，是西王母专门作为军队的一种人。他们死后，被西王母用陨玉做成了“阴兵”\n用以保护王族的墓地及秘密。\n那么，长白山天宫地下的大铜门里的阴兵，显然就是西王母殿里王俑脱去俑后的样子，\n东夏是伏羲的后裔，是西王母的祖先，大铜门的阴兵应该是西王母送给东夏保卫伏羲氏族的秘密的。\n（3）、瓶子为什么能混进阴兵队伍？\n答案非常简单：瓶子手里有鲁殇王的鬼玺！闷在鲁王宫拿到了鬼玺。\n鬼玺本来就是调动阴兵用的，更别说混在阴兵群里了\n或许这些阴兵就是闷油瓶用鬼玺调出来的，用以进入大铜门！\n（4）、鬼玺是什么？\n在西王母殿触动王俑。发动阴兵的是什么?是星盘！\n鬼玺应该就是西王母小型的星盘！\n用以在外的时候调动阴兵！西王母把鬼玺送给了她的情人周穆王，因为周穆王最爱征伐，阴兵是给他最合适的礼物。\n后来，鲁殇王盗掘了穆王陵墓\n鬼玺，落在了鲁殇王的手中。\n（5）、兼回答闷油瓶在天宫几秒钟在无邪面前消失之谜\n无邪没有看错！消失的那几秒，应该就是瓶子拿着鬼玺！\n想想鲁殇王如何调阴兵吧？阴兵在西王母处，鲁王的战场在中原\n总不至于阴兵是一步步从西域走到山东，那样，几个月后仗早打完了！\n鬼玺一定和陨玉也有关，有时间黑洞的作用\n可以瞬间把人搬运到几千公里以外\n就像科学之谜中，有些人失踪，却突然出现在几千公里以外\n那就是黑洞的作用。鬼玺正是这种东西。\n而闷的消失，和他拿着鬼玺有关\n瓶子的身世之谜！\n——————————————————————————————————————\n很多人都觉得瓶子是书中秘密最多的人。\n我看却恰恰相反，瓶子应该是书中最“正常”的人。\n因为，越往后写，作者对瓶子的偏爱越明显，当然也是摄于瓶子众多粉丝的巨大力量，\n老三写瓶子时也越发小心翼翼，放不开手脚。\n基于此判断：一、瓶子一定不是反面人物，二、瓶子一定不是妖魔鬼怪（什么鲁殇王、汪藏海什么的）他最大的可能性是一个正常的悲剧人物：\n1、铁面生——张盐城——闷油瓶这一支蚩尤后裔和超级倒斗世家！\n从闷超长的手指和超凡的倒斗技巧可以看出，出自家传\n正是铁面生——张盐城这一支有着高超的古代传下的倒斗秘技，既懂得风水易术又有着蚩尤巫蛊中克制粽子及控制鬼神魂魄技术的族裔！\n2、麒麟纹身是蚩尤族裔和张盐城势力的标志\n像张盐城这样的人，一定不会是一个人，就像吴三省一样，他会有自己的一拨人\n那么，苗族首领、盘马应该都是张盐城的人，从他们的身手也能看出来。\n也可能，张盐城自己就是苗族的首领之一。\n我们看书，会感觉瓶子似乎背负着什么使命，也许就是家族或一个族群的使命。\n阿贵也有问题！绝不是普通村民那么简单\n我倾向于阿贵也是这个族裔的人，他那个塌肩膀的儿子十分神秘。\n包括那些神秘的村民，其实全部是蚩尤的后裔，这一支是专门保护湖底古青铜的秘密！\n还有，大家是否觉得云彩和闷油瓶以前就认识和熟悉？\n苗族首领为什么要刺瞎陈皮阿四的眼睛？是否他看了什么自己不该看的东西？是不就是那枚蛇眉铜鱼？\n3、阴山古楼\n阴山古楼这一集老三是要写闷油瓶的身世之谜\n我隐隐感觉这是一个悲伤的故事，闷油瓶的执着，是在为一个族群。\n盘马杀考古队，可能根本不为什么粮食，而是为保护古青铜的秘密。\n闷油瓶对文锦的感情，可能根本不是什么男女之情，而是当年盘马杀了某些队员后，瓶子对文锦讲明了真相，取得了文锦的同情和谅解，瓶子同时加入了考古队，这是一种感恩之情。\n苗寨人火烧瓶子的竹楼、盘马对吴邪撒谎，可能正是为保护瓶子。\n4、二十多年，瓶子几度“轮回”失忆！\n瓶子在广西参加了考古队，身负家族和族群使命。\n在海底的经历，可能就如瓶子自己对吴邪说的，他也迷晕了，中了招，身体不老，但是这些墓，张盐城可能早就来过，或者因为比别人知道的多他自己有些防范，所以半途逃脱，没有被囚在格尔木。\n之后二十年，他又在家族和族群使命之外，又加上考古队的秘密\n他根据线索，反复在几个墓之间进出，寻找终极秘密\n海底、鲁王、天宫到蛇沼，他都进出了数次，这从他的记号可以看出。\n他的每次失忆，可能都与接触那个陨玉物质有关\n他每次都能更深入的发现一些线索，但悲哀的是，结局都是失忆\n待记忆恢复些后，又重头开始踏上征程\n这是多么悲哀的一种“死循环”啊。\n但是他又何其执着，这就是高尚的悲剧！\n当然，这只是一种可能，为了探索秘密，瓶子的失忆也可能是伪装的。\n5、上一次“失忆”！\n上次失忆的真假目前还难以判断\n应该是五六年前，就是瓶子在这个竹楼的那段。\n为什么他失忆时会回这里？因为这是他出发的地方，是他的“家”\n是一个安全的地方！\n至于他被当做僵尸诱饵那段，以闷的身手，那其实毫不危险\n是不是他和他的族人商量好的，要借机探索陈皮阿四的秘密呢？（这个陈皮阿四，其实是真三叔，后面会写到）\n6、这一次失忆\n至于现在正在写的阿贵和盘马的表现\n是不是在暗中保护“再一次失忆”的瓶子呢\n竹楼被烧，盘马谎言，都是因为还不相信吴邪和胖子两个陌生人\n在试探他们和闷到底什么关系。\n盘马那样说，是不是为了吓退吴邪（包括说，一个人会害死另一个这种话）\n如果瓶子身上真有这种奇怪味道\n为什么吴邪和瓶子接触这么久一点闻不出来？为什么胖子、潘子等和瓶子紧密接触的都闻不出呢？\n而云彩，给瓶子唱歌那段，最能说明他们以前很熟识\n她，是在倾诉什么，还是在唤起瓶子的记忆？\n第二条迷线：\n盗墓笔记的大主线：古代长生不老的秘密的发展！！\n（按历史时间前后排序）\n\n一、 远古中华文明的源头，伏羲女娲时代！\n那时的人有两个特点，一是能活千岁，二是人和兽合体！伏羲女娲的形象就是人头蛇身，而且他们是兄妹通婚！这里隐藏着他们最初长生的办法！因为很多动物是比人活的更久远，伏羲时代是个人和兽共生的时代！\n首先，他们能用古青铜控制动物的行为，鲁王宫尸蹩尾巴上的青铜铃铛等就是这种，\n他们为了更长生，应该是把某种动物放在身体内部，也就是共生关系\n比如伏羲就应该是有一条蛇在肚子里，这样他就可以获得蛇的寿命，也还能保持人的思维！但缺点是会变成人蛇的样子（人和蛇基因的结合），会变成“妖怪”的形象！\n伏羲女娲虽然寿命很长，但是还是会死，当他们体内的蛇死去的时候，他们也就死去了。伏羲活了1100年，应该就是他体内共生的蛇的寿命！\n这就是最古老的长生，也是一切其他长生办法的起点！\n前文已经说了，云顶天宫地下的古老遗迹和大铜门就是伏羲留下的。\n东夷部落有一些古老的动物，但都被人用青铜驯化，为人所用或与人共生！\n大蚰蜒——龙的原型\n大鸟——人面鸟身\n蛇——伏羲人面蛇身\n虫——伏羲部落虫图腾\n二、 历史在向前发展，伏羲死后，东夷部落开始分裂：\n黄帝的一支，来到中原，开始发展农牧业\n蚩尤的一支，在河北，极大的发展了伏羲的青铜文明和巫蛊祭祀\n西王母的一支，来到昆仑山，她带走了大鸟、蛇和虫！\n三支部落开始向不同的方向发展：简单说\n黄帝爱政 治，蚩尤爱征战和祭祀，西王母却独爱研究和发展长生不老术。\n但在那时候，他们全部依然延续着伏羲人和兽共生的形态：\n在《山海经》的描述中，黄帝、蚩尤、西王母都是半人半兽的怪物，黄帝是人和龙，蚩尤是大鸟和人的合一，西王母更是多种动物和人混合的怪物，这就说明，他们依然延续着伏羲的“人和兽同身”长生法，他们的寿命，都在千余岁左右！\n三、 后来，开始了战争时代。\n先是炎帝部落和黄帝部落征战，炎帝是农耕文明的代表，后来，炎黄两个部落融合，形成华夏的祖先。再后来，是黄帝和蚩尤的战争！蚩尤是战神形象，又会巫蛊之术，开始黄帝打不过蚩尤，这是，西王母站在黄帝这一边，她派九天玄女（鸟身人面）助黄帝破了蚩尤巫蛊，蚩尤战败被杀，从此蚩尤部落流散，一部分并入炎黄部落。另一部分流散到长沙、再到广西、云南，成为苗族、瑶族。\n此后进入“五帝”时代：\n蚩尤部落流散，应该逐渐失落了这种“人与兽同身”长生术，只剩一些后人还保留着神秘的古青铜文明，发展了巫蛊文化。\n黄帝一支，发展、壮大，成为中华文明的主流，随着农耕的深入，他们逐渐遗忘和放弃了“半人半兽”那种妖怪形象，慢慢变成了正常的人，当然，也就失去了“长生”。\n只有西王母，继承并发展了伏羲的长生！\n四、 黄帝时代到西周中期数千年之间：\n中原在进行三皇五帝到夏商周的朝代更替。\n而，昆仑西王母族在这数千年间远离中原的征伐，自成一体。\n西王母可能是伏羲女娲最近最直接的后裔，因为早期伏羲部落文化在这里得到最好的保存\n而，西王母也数千年如一日的研究长生之术。\n西王母为什么要移居昆仑墟？答案是：玉！\n是的，玉！在中国古代文献中，西王母的昆仑以产玉著称！一种黑色能让人“长生不老”的玉！有记载，王母曾把这种玉制成“玉英”送给黄帝吃，吃了可以容颜永驻，长生不老！\n大家觉得熟悉吗？是的，这就是文锦进入的那个陨玉！就是鲁殇王玉佣的材质，就是这种玉！\n有了伏羲的“人与兽共生”之术、控制蛇鸟之术，再加上让人容颜永驻之术。\n西王母开始了延续数千年的长生不老实验，精彩大戏上演了！\n西王母的三种长生办法！！\n第一个办法;： 人和兽共生\n这依然是伏羲流传下来的老办法，有人蛇、人鸟等，这种办法的长生之道是：\n如果人一切正常，就可以达到体内共生动物一样的寿命，千年左右。如果人出了意外，只要体内动物还活着，因为这动物和人长期共生，有了人的记忆和意识，就可以继续和另一个被洗去记忆的人共生，实现“转世”。\n缺点是，到了体内共生动物的寿命周期时，人还是得死。其次全是半人半兽的妖怪摸样。\n早期西王母一定是用这种办法的，所以早期的西王母是妖怪形象。\n从蛇沼遗迹中，壁画上那些人面蛇身的女人身上以及那些行为神秘古怪的鸡冠蛇身上（鸡冠蛇就是早期被西王母驯养的用于和人共生的有灵性的蛇类），就可以证明这点！\n第二个办法： 丹药 = 玉英+尸蹩王\n人和兽共生长生法的弱点，就是西王母改良的重点！\n体内共生动物死了人也跟着死去怎么办？——\n能不能设法保留人头脑中的记忆和意识，同时让肉体不腐和重生？\n能不能不再是半人半兽的妖怪形象？——\n体内的共生动物尽量小，不像鸟或者大蛇这种。\n西王母发现了三种东西： 尸蹩 陨玉 鸡冠蛇王！\n尸蹩——某种通过在人脑中产卵保存人的记忆的动物，体型也较小，可以成为新共生物。\n陨玉——人活着时候吃了可以容颜不老，死了后可以保持人尸身不腐\n鸡冠蛇王——一定是一种死后通过蜕皮还能复活的神蛇！\n于是西王母开始了大规模的实验：\n魔鬼城中的西王母罐——脑子中有大量尸蹩卵的人头，就是尸蹩保持记忆的实验\n把尸蹩卵喂鸡冠蛇的壁画——西王母要创造一种同时具有尸蹩和鸡冠蛇优点的生物！\n这种厉害无比的东西终于被西王母研制成功了，是的，尸蹩王！\n巫蛊的蛊就是把几种极厉害的毒虫放在一起，让它们互相吞噬厮杀，最后留下的，就是蛊！\n西王母的尸蹩王一定也是这种东西！\n它兼具了尸蹩和鸡冠蛇的最厉害之处！\n能保留记忆，能使人保持人型、能让人脱皮复活！\n当然，尸蹩是剧毒的。\n于是，把这种尸蹩王封进玉英中，一粒长生不老的丹药就制成了！\n西王母临死时，只要服下丹药，躺进陨玉就可以等待复生了！\n尸蹩会在这期间通过在人头脑中产卵保存记忆，同时陨玉可以让人不朽，尸蹩还有蛇脱皮功能。脱过几次皮后，人就可以复归年轻，同时头脑里恢复记忆，就完成复生了！陨玉强大的功能当然不是玉佣可比，西王母这个过程应该短很多，大概几百年左右？\n第三个办法： 年轻时、活着时就吃丹药！\n人，当然都是贪心的，西王母也不例外！\n虽然实现了复活，但还是要“死” 很长一段时间\n年轻时、活着时就吃掉尸蹩丹药，会怎么样呢？\n西王母一定拿活人做过无数实验，结果——要么立即死了，要么变成禁婆等怪物，要么疯狂。\n总之，不成功！\n终于，西王母发现了秘密：\n唯一活着时吃掉尸蹩而又安然无恙，能够长生的办法就是：进入陨玉中，不离开陨玉！\n最后，我们推测，西王母选择了第三种办法！\n那个坐在陨玉外边的，貌似女王的女尸，当然就是西王母的“影子女王“！替西王母传递消息，发号施令的！\n而，陨玉里伸出头那个女人，可能正是西王母本人！\n二更。\n五、 周穆王与西王母，长生术再现中原！\n在中国的神话传说中，周穆王是西王母的情人！\n周穆王西见王母，在瑶池边，饮酒酬唱，两情相悦，缠绵数日。周穆王以贵重的白圭玄璧和绚烂的丝织品为见面礼，同时也作为定情之物。\n分别之际，西王母为天子谣曰：白云在天，丘陵自出。道里悠远，山川间之。将子无死，尚能复来？“——如果你能够不死，还会再来吗？\n以后的岁月，西王母一直痴痴等待情人穆王重来，但穆王再也没有来\n李商隐有诗：“八骏日行三万里，穆王何事不重来？”\n周穆王是西周时代酷爱游历和征伐的天子。\n他驾着八匹马来到西王母昆仑山\n起初，可能是为了战争，但显然像那副壁画描述的，他没法对付那些鸡冠蛇。\n他遇见的那一代西王母，或许正是研究出了尸蹩丹药永生术的那位，\n所以，已经不是半人半兽的妖怪，成为了正常的美丽女王（《穆天子传》这样记载的）\n或许，穆王和西王母在战争中一见钟情，也或许，穆王只是假意投降示好示爱。\n但西王母爱上穆王是真！缱绻温柔后\n离别时刻，西王母许下再见的诺言，并赠送给穆王不死之药\n就是玉英包裹尸蹩的丹药和玉佣（用陨玉做的）\n穆王和西王母一起种下一棵槐树\n西王母还送给穆王两样东西：\n一是调动阴兵的鬼玺，助穆王征伐\n另一样，就是古青铜控制的尸蹩群（鲁王宫中六角铃铛的大尸蹩）\n现在可以解释六角铃铛，就是伏羲控制动物的古青铜。\n穆王回到中原，把他的陵设在山东一座山中（一般只有帝王才有山陵）\n临死时果然按照西王母的吩咐，吃下丹药，穿上玉佣盘坐在一棵树下（西王母的约定？）\n进入到脱皮，等待复活之中。\n他把鬼玺陪葬墓中，而尸蹩，用来守陵！\n六、 战国时代的变故！\n时间，继续向前。\n很快，周王室衰微，到了诸侯混战的春秋战国时代。\n鲁殇王，是一个靠给鲁国公倒斗起家的将军，而，铁面生是他的幕僚，这个精通风水、易经的人，其实是蚩尤的后裔！\n鲁殇王一次倒斗时发现了穆王的山陵，一定是找到了穆王和西王母故事的记录\n他拿到了西王母送给穆王的两件东西：鬼玺和铜铃尸蹩，于是战斗力大增！\n那个金丝帛书上鲁殇王的自传，所谓发现大蛇，得到了两件宝贝，其实暗指他盗周穆王的墓的过程。\n殇王是个粗人，这一切，都应该是铁面生帮他破译的\n于是铁面生也知道了这个秘密！\n他知道鲁殇王最后一定会取代穆王进那个玉佣，但他也看出，那个玉佣一经换人，也许就失效了。\n他开始盘算自己的永生，自己带着一支队伍前往西王母的昆仑山。\n取回了一些黑色陨玉和四颗尸蹩丹药，至于过程如何，那是另外的故事了。\n鲁殇王临死时，把穆王从玉佣中弄了出来，把穆王的血尸封进墓门边的铁棺，变成为他守墓的“看门兽”。自己进了玉佣。并在穆王墓的上边修了自己的墓，放置七星疑棺。\n穆王的“永生”被鲁殇王破坏了，可怜“八骏日行三万里，穆王何事不重来？”，但其实鲁殇王也是白忙一场，因为那个玉佣其实已经失效了。\n鲁殇王死前，铁面生也假死，其实是寻找风水宝地，他在长沙找到了血地，为自己造墓\n同时，作为蚩尤的后人，三苗当时正在长沙一带，也便于铁面生活动。\n铁面生一定是把生平，包括西王母的秘密写在帛书上，一份给了他的后人或族裔\n另一份陪葬在自己墓中。\n他自己设计的墓分两层，上边的是个虚棺。下面的密道，全部使用陨玉铺成，等于是个大玉佣，他雕刻了四目九天娘娘像（西王母），把尸蹩和帛书放在上边\n然后吃掉尸蹩，进入自己的“永生”。\n七、 元朝初年的东夏王国！！\n在伏羲时代之后，东北的历史是一大段的空白。可能因为气候的原因，已经不适合发展强大的文明。\n东夷部落离开后，巨大的遗址——地底峡谷的大铜门，可能还留有少量的族裔看守或留守。\n为什么在宋元时期突然出现了东夏政权？\n答案就是东北女真人的崛起，已经威胁了伏羲遗址的秘密和这个族群后裔的生存\n必须要建立一个政权！这个后裔人数应该是非常少的，那么就征服和利用女真人！\n伏羲后裔征服女真人的方法应该是：\n威胁：巨大的鸟的杀伤力 神迹：利用大铜门的集体致幻功能和神秘的万奴王的威慑力\n利益：许以他们青春永驻（伏羲氏族也有玉文化，食玉驻颜不是难事）\n万奴王是谁呢？\n万奴王就是伏羲氏族留守的后裔！他们几乎原封不动的守护着伏羲氏族的传统！\n1、 大型鸟人，用青铜铃铛控制的“嘴中猴”\n2、 龙的图腾——蚰蜒\n3、 青铜的祭祀和集体致幻的巫术\n4、 “人和动物一体共生”的长生方式！只有他们，还保留着伏羲时代“妖怪”的形象！\n伏羲氏族以往共生的是蛇，但长白山的气候可能已不适合蛇类生存，蛇已被西王母带到昆仑。这里留下的唯一适合共生的就是蚰蜒了！所以，万奴王十二手的妖怪形态不就是人和蚰蜒的结合体吗，只有蚰蜒有这么多的肢体。但是，蚰蜒和人的共生可能远远不如蛇，蚰蜒的寿命很长，但每一代万奴王的寿命并不长。\n5、 兄妹通婚的传统，天宫的万奴王和汪藏海墓的十二手尸明显是兄妹兼配偶。\n万奴王从地下爬出来的秘密？\n每一代万奴王死去，大铜门才开启，这就是伏羲氏族古老的死亡和交替仪式，也就是人死了，而他体内的共生动物还活着的情况。\n万奴王死去，进入铜门，这时他体内的蚰蜒就会把尸体吃掉，剩一张皮，然后出来。\n这时，后继的万奴王（前任万奴王的子孙）已经被古青铜洗去全部记忆，蚰蜒就进入这个万奴王体内，继续共生，蚰蜒和前任万奴王长期共生，已经有了灵性，有了前任的意识和记忆，然后传给后一代王，如此反复传承，知道这个蚰蜒死去。\n而新万奴王刚刚蚰蜒进入体内，当然是像蚰蜒一样从地底爬出来！\n为什么只有第十四代万奴王要修墓？\n前十三代万奴王都没有墓，他们可能都变成了青铜门中的一张人皮\n为什么这一代万奴王要修墓？\n答案是：因为他体内的蚰蜒要死了，这十四代的轮回也要结束了。\n当然，他不甘心，他肯定寻访了西王母，毕竟他们是同一个族裔！\n西王母传授了新的永生方式，是的，就是尸蹩丹药！\n所以，他开始需要墓了，需要不被盗掘和打搅的“永生”之墓\n就和穆王、铁面生一样！\n所以，东夏开始寻找和强迫这方面的大师汪藏海来给他修墓！\n而且，那些阴兵一定也是西王母送的，为了保护大铜门的“终极”，毕竟那是他们共同的始祖！\n大铜门后面是什么？\n答：终极！\n终极有三层意思：\n1、是古青铜的神秘致幻力，你想看到什么就有什么（像罂粟）\n2、是中华文明终极起点伏羲文明的遗迹！\n3、就是古代长生的秘密，伏羲等“人面蛇身、人和动物共生”竟是真的\n可以看到前十三任万奴王轮回的秘密——蚰蜒的秘密，洗去记忆的秘密，永生的秘\n密！\n八、 明初，盗墓大师汪藏海时代！\n笔记故事里，汪藏海的故事貌似是最复杂的，其实，有了上面的解谜铺垫后，\n汪藏海的秘密反而变得简单，他的故事大概脉络如下：\n1、 第十四代万奴王把他抓去修墓，从而使汪进入了大铜门，知道了万奴王和蚰蜒共生的轮回秘密，也知道了青铜致幻和洗去记忆的秘密。\n2、 对于汪藏海这样的人，他当然动起了也让自己长生不老的心思，但是，万奴王那类和动物共生变成妖怪的办法，他显然不能接受。于是他开始借帮万奴王搜集珍宝在各古墓中搜集相关的信息。\n3、 他发现了两个特别的古墓——鲁王宫和陈皮阿四的镜儿宫，鲁王宫关系着血尸丹药——另一种长生办法，镜儿宫显然是关系青铜秘密的，是蚩尤的后人苗人在广西守护某种秘密的。丹药和洗去记忆，显然是和长生有关的两个重点。但是，汪没能得到战国帛书，所以没能马上获知西王母国存在的秘密。\n4、 后来，他第二次回了云顶天宫，这次他看到已服食丹药后的万奴王尸体（就是吴邪他们在天宫看到的那具穿着黑色盔甲的十二手尸），并且找到了西王母昆仑的地址。\n5、 汪前往天宫，但这时西王母应该只能呆在陨玉里了。汪藏海找到了西王母死后复活这种永生的秘密，但他应该不知道活着时吃下丹药并呆在陨玉里这种办法，即使知道了，他也不一定会用的。汪从蛇沼拿走了陨玉材料和尸蹩丹药。\n6、 回去后，汪开始修建自己的海底墓，其实是琢磨和研究自己的长生之道。他和铁面生不一样，铁面生就是遵循，汪藏海还想自己试验和完善。\n7、 万奴王的妹妹和老婆（怀孕的十二手尸体）、脑袋里有尸蹩卵的马脸古尸，都是汪弄回来试验的。\n8、 汪藏海不愿意在墓中脱皮的那个等待过程，他一定也想到了活人吃尸蹩丹药的办法，于是找了人试验，结果和西王母一样，这些人变成了禁婆和海猴子那种怪物。\n9、 汪还是不甘心，于是他想到古青铜，想到了一个妙计，就是临死前吃两个丹药（大概啊，我猜的）复制两份自己的记忆，一方面自己的身体继续脱皮等待，另一面先让自己的意识和记忆移植到另一个人身上，自己等于率先复活了。然后再通过有着自己意识的这个人再去操作，等待自己肉体的复活！汪实在聪明啊！\n10、 但是，自己死后，如何控制这一切的实现呢？汪的所有阴谋主要体现在这里，他想以后能进自己墓的，一定也是盗墓贼，所以汪锁定了这个群体，他本来就是盗墓的大师，所以他的墓全是为这个目的设计的。\n11、 他的那三条蛇眉铜鱼全是为引诱盗墓贼的，鲁王宫、镜儿宫那种诡异的情景再配上蛇眉铜鱼上的秘密，会引诱的任何一个盗墓人欲罢不能，一定要解开秘密。就一定要来到他的海底墓！\n12、 他自己深深了解盗墓贼，所以每一个陷阱都是为引诱他们的，他的所有机关、设置、阴谋并不是为了防盗墓，恰恰相反，他是为了达到自己的目的！\n青铜珊瑚树——一定是为了洗去盗墓人记忆的\n哨子棺——是个阴谋\n最后就是为了让盗墓人活着吃掉汪身体里的一个尸蹩王！移植他的记忆而已！\n当然，具体情节如何，就像老三说的，用古龙笔法写这又是一个极精彩的连环套故事！\n13、 汪布置好这一切，死去了，但是一切仍然在他的掌握，也就是他的导演之下进行着\n究竟，谁会是那个倒霉蛋呢？\n到此，全部古代的谜就差不多了\n时间，流淌着\n远古的神秘、铁面生的记录、汪藏海的阴谋就静静的在时间里\n等待着\n等待着现代的又一批冥冥中注定要接近这一秘密并被它改变命运的人们！\n吴家三代人、考古队、裘德考、文锦、闷油瓶等等\n第三条迷线：\n盗墓笔记的大主线：现代各种力量关于长生不老秘密的博弈！！\n（以事情发生的时间先后为序）\n——————————————————————————————————————\n一、汪藏海死后，一切似乎归于沉寂，命运之轮，在海底等待着。\n直到上世纪四十年代解放前，吴家三代土夫子盗掘长沙血尸墓\n使吴家从此陷入了这个命运之轮。\n当时的情况应该是，他们进了黑玉密道，破坏了铁面生的“脱皮”，使铁面生变成了血尸，年轻气盛、血气方刚的吴老狗的哥哥应该一马当先，拿到了四目九天娘娘口中的帛书和眼睛上的尸蹩丹药。\n这是，血尸惊起，三个人都中了招\n老狗的哥哥误食或者临死时惊慌中把丹药当“救命药”服用\n结果变成了新的血尸。\n50年前的《盗墓笔记》中记载的年幼时的吴邪爷爷见到的全身通红滴血的血人，应该是中招后的他爸爸，那个没有瞳孔的巨脸就是铁面生的血尸。\n那个血尸，跑了？死掉了？还是被老狗的哥哥杀了？不祥。\n墓中的四颗丹药，应该是铁面生当年服了一颗，老狗的哥哥服了一颗，后来被吴三省拿出了一颗，被吴三省摔碎了一颗。\n此后，吴老狗虽然精明能干，但一是年龄小，二是当年受的刺激太大，应该没再进过这个墓，之后，就解放了，他拿到的战果帛书被裘德考骗走，又很快就被裘德考出卖了，吴老狗逃到杭州，应该也没再进过墓。\n那个吴家的血尸就这样在墓里呆了几十年。\n二、大关键，1974年！！\n1974年是考古队事件的起点和关键之年！\n因为，这一年裘德考在美国破解了他从吴老狗那骗去的那卷战国帛书的内容！（至于他一个美国人是怎样破译帛书的，应该背后还有故事）\n那时中美关系已经解冻和建交，这个考古界轰动性的事件传到中国，肯定会吸引高层关注！那时是一个中国考古很火热的时代，而且还有某大人物身体健康的关系。\n这个帛书的秘密高层一定希望掌握。\n那么以国家的力量很容易就应该查到裘德考是从谁手中得到的帛书，吴老狗和吴家！\n这就是《笔记》中一再提到的“惊动中央的老底子”。\n那么出于这种事情的神秘性，高层一定会找到吴老狗，让他重回血尸古墓，看看还有无剩下的战国帛书。\n1974年左右，吴老狗一定重新下过血尸墓，和他一起下去的，是解连环的老爸解九爷。\n他们是亲戚且关系最好，而且都是“老底子”。这个情节老三的网络版原来有，后来修改掉了，实体书没有。\n吴老狗和解九下墓，一定是有官方或者军方的人物在墓外等着，监视着。\n之所以没有惊动血尸或者血尸没攻击他们，可能因为血尸还有微弱的意识知道这是他弟弟。吴老狗自然也不愿声张。\n吴和解果然从墓中拿出了战国帛书其他卷。\n当然，马上就上交了！\n大关键，1974年！！（续）\n吴老狗和解九再次拿出的战国帛书可能很快被破译，内容再次震惊了高层\n这份帛书本来就是铁面生的记录，他一定记述了西王母长生不老的事情，还有，就是蚩尤后裔的古青铜神话（前文讲过，他本人就是蚩尤后裔）\n1974年，还发生了什么事？对！就是陈皮阿四倒斗镜儿宫被苗人刺瞎眼睛，第一条蛇眉铜鱼出土！汪藏海的蛇眉铜鱼记录了同样长生不老的事情和海底墓的位置（铜鱼就是为了引诱盗墓贼进海底墓的）\n这两件事的对“永生”的共同指向，揭开了一个大计划的序幕！\n高层或者军方准备用一种非常规的手段介入和研究这件事！\n这个力量就是”它”\n文锦的考古队就此成立了，但是他们并不知道真相，也不知道幕后的“它”\n为什么他们的档案全部失踪了？是因为从一开始这支考古队就作为“它”表面上的试验品和牺牲品存在的。\n考古队的第一站，就是阴山古楼的发现“铁块”的地方，苗人的蚩尤古青铜秘密！在这里，闷油瓶应该和考古队发生瓜葛，并最终加入了考古队。\n“它”的计划中还有一个布局，就是引诱裘德考的参与\n首先，裘德考手上有第一卷帛书，并且解开了帛书内容，非常有利用价值。吸引裘德考进来后，可以跟在后头坐收渔利，对于一个秘密任务来说，这是一个最好的选择。\n1975年的圣诞节，裘德考收到了新的战国帛书照片！从此，这个人疯狂的投入了这个谜之中！\n是谁寄给裘德考的呢？吴三省或者解连环！（老三的网络版同样有这个情节)\n其实，这就是“它”授意吴、解家族给裘德考抛出的橄榄枝，让裘德考以为这是盗墓贼们的意图。引诱裘德考进来。这也就是文锦说的其实那个“它”的力量借助吴、解家告诉了裘德考新的帛书和海底墓的位置。\n考古队和裘德考两支力量，其实都是“它”的棋子！\n至于“它”为什么要采取非常规的绝密的办法？\n以前已经分析过\n1、这是一件非常骇人神秘的事情——长生的秘密，但这事和发掘兵马俑不同，几乎等于鬼神迷信，和教育人民多年的某某主义简直水火不容。，和无神论意识形态完全相悖，在一个意识形态至上的年代，根本无法大张旗鼓的做。\n2、这事有很多鬼神色彩，正规考古学家还真做不了，要做，得调动盗墓界的力量，这又是一个不能大张旗鼓的原因。\n3、战国帛书的破译在国外影响很大，更不宜打草惊蛇，弄不好会有国际影响\n4、这种事可能还要有人体试验之类，更不能公开。\n国家势力不宜公开参与，但又要最终掌控结果，同时既要利用又要盯着防着盗墓界和裘德考。\n总之，此事只能绝密的做，考古队的命运从第一天就已注定，无论成败，都是“狡兔死，走狗烹”的命运。\n三、1975年——西沙前一年！\n从吴老狗和解九1974年二进血尸墓开始，吴、解两家就开始被迫秘密给“它”做事。\n吴老狗已经老了，吴三省就变成了主力！在“它”安排的剧本中，吴三省名义上是为利益给裘德考做事。\n其实，他是“它”用来钓裘的鱼饵以及牵制和监视裘的。\n所以，当年的吴三省，是“三料间谍”\n第一层是考古队文锦的男友，第二层是装作为利益给裘做事的，第三层，真正的底子其实是“它”安排的同时埋伏在考古队和监视裘德考的人！\n早在1976年左右，文锦的考古队就已经成立，\n那么，吴三省和文锦这种气质完全不般配的人谈恋爱是不是也是“它”的安排呢？\n目的是为了吴三省有个名正言顺的理由接近和加入考古队，而不让考古队的人产生怀疑。\n那时，“它”对吴、解两家的利用，毕竟吴家是盗战国帛书的始作俑者。吴三省主要用于裘德考。至于解连环，文锦说的可能是真的，他的确是考古大学学生正常参加文锦考古队的。但是，解连环依然是“它”安插在考古队的一枚钉子，解连环主要是要利用他盗墓世家的知识和经验，协助那些没有什么倒斗实战经验的学院派（和后来胖子的用途相似）\n从1975年吴三省给裘德考寄出新帛书后，就一直在用帮裘做各种事来钓裘。\n而同时，解连环被“它”安排进考古队，帮助考古队深入“长生不老”的秘密。\n终于，在西沙事件前一年，裘终于来到中国，开始了行动，首先就是再次盗掘长沙血尸墓\n大家是否觉得，吴三省在血尸墓截胡裘德考过于巧合\n包括以后每次三叔（解连环）和裘德考的不约而同。\n其实，根本就是吴三省在奉“它”的命令监视裘德考啊\n在墓中，吴三省杀掉吴家血尸，误用尸蹩把裘的队伍弄崩溃、拿出一粒尸蹩丹药这些应该都是真的。\n这件事最关键的，是让吴三省看到了人变成血尸的悲惨景象以及这个秘密的恐怖之处！\n四、西沙的秘密 （一）\n就像老三写的，命运之轮在西沙海底越转越快！\n长沙血尸墓失败后，裘德考把目光转向了西沙，在考古队之前，裘德考其实已经几次派人下过海底，早已找到了墓室。当然这些秘密也通过吴三省传给了“它”。\n“它”毕竟有着帛书的几乎全卷，还有裘德考对海底的这些探索，已经掌握了足够的秘密。时机基本已经成熟了，关键的行动就要开始了！考古队就要开赴西沙了！\n吴三省的恋爱这时该发挥作用了，他将利用和文锦的关系打入考古队！\n他这次的任务很简单，就是将一无所知的考古队引进古墓，迷晕，并且让他们吃下尸蹩丹药，作为活体实验。\n而且，这个名单中包括也解连环。\n之后，吴三省就可以离开海底，出来后大肆宣扬考古队在海底失踪就可以了。\n行动之前的那一晚，本应该是吴三省趁队员睡熟，到墓里再看看，最后安排一下明天的行动步骤。\n但是，吴三省不甘于这样的命运！\n他已经在长沙血尸墓看到了人变成血尸的悲惨景象！\n或许，他已经真的爱上了文锦，或许，他同情考古队的命运，更不忍解连环的命运，\n当然，他也知道，所有人都是“它”的工具！他自己也不例外，他已经知道了太多的秘密，一旦失去利用价值，命运会无比悲惨。\n吴谢两家的命运，他要反击！\n西沙的秘密 （二）\n那个《笔记》中西沙最扑朔迷离的一夜，那个“吴三省害我”还是“解连环害我”的血字？\n其实，这是吴三省和解连环共同设的一个局，或者说是吴家和解家为自救共同设的一个局！\n那个对于考古队来说最可怕的一天越来越近了\n但，吴三省知道自己救不了文锦、救不了考古队\n考古队的命运是注定的，自己的命运也是注定的，暗中将有人监控着这次行动。\n但是，聪明的吴三省想出了一个“局”\n西沙的前一夜，吴三省按照计划出海下墓，但是，他悄悄叫上了解连环。\n在海上，吴三省告诉了解连环明天将要执行的任务\n这对表兄弟为了自救、为了有机会摆脱吴谢两家的悲剧命运\n共同设了一个绝妙的局。\n那就是其中一个人诈死！\n按原计划，吴三省这次离开海底后，还得为”它”继续诱惑和监视裘德考，所以吴三省不能死！但是解连环的能力和对这个秘密的了解又不如吴三省。\n所以，他们设了这样一个局，利用的是他们本来就是亲戚，长的有七八分相似：\n假装吴和解图谋不轨，二人在墓里发生了冲突，吴害死了解，解连环写血字“吴三省害我”\n第二天考古队将发现解连环的尸体（其实是裘德考队伍里一个撞烂脸的人的尸体）\n那么解连环这个人在世间就不存在了。真的解连环就可以假扮成吴三省继续为“它”做事。而真的吴三省就可以借此逃脱，人间蒸发，摆脱“它”。然后在暗处帮助吴解两家对付“它”以及寻找这件事的终极秘密，解救被当做试验品的文锦和考古队。\n这个局设好后，吴三省当夜就可以逃走，由解连环假扮吴三省回到船上。\n第二天，“解连环”的尸体将被发现。考古队将下墓，解连环将代替吴三省完成它安排的迷晕考古队的任务！\n一切天衣无缝，但，这时事情发生了变化！\n西沙的秘密（三）\n黑暗中的第三个人！！\n就在吴三省和解连环谋划好的时候，黑暗的墓中突然出现了第三个人，突施杀手要杀掉两个人！这个人当然是考古队里的人，他是谁？\n齐羽！\n前边已经分析过，“它”让吴三省和解连环打入考古队\n但是怎么可能对这两个盗墓后人放心而不加防范呢？当然会有“螳螂捕蝉、黄雀在后”的是连环监视。\n齐羽才是真正“它”的人——秘密特工！\n齐羽的任务本来就是主要监视他二人的，他暗中跟踪他二人下海，看到他二人的密谋，当然要下杀招！但是他自己的身份也不能暴露！这就是黑暗中的袭击，一击不中，全身而退。，然后放掉他们潜水的氧气瓶，要将两人困死在墓中。\n这时，吴和解才发现，事情比他们想的更复杂，考古队里也有“它’的特工，但又不知是谁？但是剑在弦上，已不得不发，他们仍然还是决定按原计划，因为他们料定特工也不敢随便暴露身份，那样就没法暗算考古队了。\n解连环冒充吴三省回到船上，吴三省却留在了墓中。\n明天，他们将演出一场”两个吴三省“的大戏，同时借机找出特工！\n西沙的秘密（四）\n后来，在蛇沼，文锦对吴邪的讲述，三叔是解连环冒充的不假，但西沙那天的事情却又有很多漏洞！主要还是为了掩饰吴邪自己身世的秘密！\n三更——\n那天的真相应该是：\n早晨，考古队发现了解连环死在礁石上（裘德考队伍中那具撞烂脸的尸体）。\n后来，考古队为避风暴下了墓，那个假托身体不好睡觉的是解连环扮的吴三省。后来偷偷跟在考古队后面。\n装女人梳头引考古队进入奇门遁甲的是一直留在墓中的真吴三省，到了甬道中，迷倒了考古队，按原计划给队员喂了墓中的尸蹩丹药\n闷油瓶回忆晕过去前看到吴三省蹲着面无表情的看着他，这是真实的，而且，那时真吴三省一定已经开始怀疑闷就是那个黑暗中的特工，为了灭口，所以用墓中的古青铜给闷消了记忆，这可能是闷第一次失忆的原因。（也是只有闷失忆，而其他人没有失忆的原因）\n这“两个吴三省”真是妙计啊！让人真假难辨\n按照”它’的安排，把考古队迷倒后，吴三省就应该尽快离开，回到海面，假装获救\n而考古队会由“它”运走，这个是吴三省不能看也不该看的。\n这时，奇门外假扮吴的解连环应该回海面了\n真的吴三省则可以藏在墓里，看到“它”，甚至可以跟踪它，知道考古队的下落！\n但是，且慢！还有一个人！\n西沙的秘密（五）\n是的，齐羽！\n齐羽没和这些人在一起，齐羽才是特工！当然不能被迷晕，吃下丹药。他必然要躲起来。\n等待下一步配合“它”运走已中招的考古队！\n这样，吴邪在秦岭那个怪梦得到了完美的解释！\n考古队没中招前，有人问齐羽哪去了，答：他贪玩不知跑哪去了，不等他了。\n齐羽当时一定躲在一个棺材附近，观察着这些人的一切\n当然，他被吴三省发现了\n才会有：“原来是你小子跟着我！”的话以及掐住齐羽的脖子的一幕\n其实，那时吴三省即便发现了齐羽的身份，更不敢对他怎么样。如果他杀了齐，他自己也会暴露，和解设下的局就失败了！\n事情应该是这样：\n在和吴三省的打斗中，齐宇自己跌进了棺材中\n这也是吴邪梦的一开始，他在一个棺中的原因！\n恰巧，那个棺材就是汪藏海设的套，齐宇才是那个中招的倒霉蛋！\n但是其中一定出了差错，就是齐宇自己的记忆还没洗去，就吃了尸蹩丹药（汪藏海一定设了巧妙的机关）输进了汪藏海的记忆，直接导致他大脑崩溃了，疯了！\n这时，真吴三省一定吩咐解连环快撤\n他自己则藏了起来，看到它把考古队以及中招的齐羽一起运走后，终于逃脱了\n五、 1984年到1992年左右！\n西沙之后：\n中招的考古队被运到格尔木，被“它”严密的监视起来，每个人都被录像，也就是观察吃了丹药后的反应。那时，他们反应都比较轻微，但都已停止衰老，霍玲有反复梳头的毛病而已，一无所知的考古队被囚禁以后开始研究这件事的来龙去脉！文锦说考古队有几个人不在了，被顺子的爸爸带到天宫的7人当时应该都在疗养院，考古队是11人，那么一减应该是4个人不在了：吴三省、解连环（名义上死了）、齐宇和闷油瓶。\n闷油瓶毕竟是蚩尤苗裔和张盐城的后代，功夫不凡，应该是在到格尔木后逃脱了，但是他也不衰老而且失忆了，之后，他走上了寻找记忆和继续解谜之路，也就是反复进入相关的古墓！\n齐宇是和文锦他们分开的，一方面齐是“它”的人，另一方面齐中招最严重，他已经进入丧失全部意识的疯狂状态，他也成为“它”监控和研究的对象！\n格尔木疗养院的那具黑棺材是汪藏海的陨玉棺材，也是“它”从海底运来研究的，可能就是齐宇中招的那个棺材！\n至于解连环和吴三省。解连环出来后就开始冒充吴三省，到处说考古队和他的恋人文锦都在海底失踪了，当然吴解两家都是知情的，共同维护着这个秘密。这本来就是他们反抗“它”的共同事业！否则，解根本骗不过吴家！\n“它”当然还要继续利用吴三省来对付裘德考，所以支持吴（解扮的）表面上继续发展自己的倒斗事业和势力。吴（解扮的）也继续在表面上为“它”效劳！\n而真的吴三省，变成了一个没有身份的“黑人”。肯定易容漂泊着，隐藏得很深。一方面暗中关注“它”和考古队，同时暗中和解连环、吴家、解家保持联络。另一方面应该在继续探索这个大秘密！\n如此，大概8年！\n六、1992年前后，又一次重大的关键！！\n如此8年之后，到1992年前后，可能因为一些高层或者军方的变化，“它”的力量在慢慢衰退，也就是说，对这件事的重视程度在降低。\n此时，文锦的考古队7人借机逃出了格尔木。\n刚开始，“它”仍然调集了大批人追捕他们。\n文锦他们逃亡期间，隐藏起来的真吴三省一定找到了文锦，告诉了文锦事情的真相\n并开始暗中帮助文锦和考古队。\n在暗中监视“它”的活动的吴三省发现：\n一夕之间，“它”突然神秘消失了。格尔木疗养院被搬空、废弃。“它”似乎完全放弃了这个计划。\n只剩下汪藏海的棺材和被遗弃的疯了的齐羽\n吴三省建议考古队依然回到疗养院，最危险的地方是最安全的地方！\n1992年前后，考古队开始活跃，他们探索秘密的步伐开始加快！\n一可能是因为得到了吴三省暗中的帮助，让他们知道了更多的线索\n二是他们的身体开始出问题了。他们亲眼看到尸化变禁婆的女孩，这个女孩不一定是考古队的人，也许是裘德考队伍中中招的人，一样被“它”弄到疗养院进行研究（疗养院就像一个实验场，可能还有各种尸化的怪物），时间不等人，他们的自救步伐必须加快了！\n1992年前后，又一次重大的关键（二）\n当时，文锦等已经得到了长白山的信息，开始筹备前往云顶天宫\n这时，齐羽成了一个难题。\n吴三省对齐羽一定是矛盾的：\n一方面，虽然齐是“它”的人，但也是奉命行事，本身是无辜的。却因为吴三省落到这个悲惨地步，而且被“它”遗弃，这种命运一定让吴三省同情和不忍。\n另一方面，虽然看来“它”已经完全放弃了这个计划，但是暗中情况不明，齐羽依然让吴三省最不放心，因为这种吃了丹药后身体的变化谁也无法把握。一旦有朝一日回复了齐羽原来的记忆，那么，吴三省和解连环的局就会暴露。殃及吴、解整个家族！\n恰在此时，吴三省的侄子，十几岁的吴邪应该得了绝症，不久人世。\n于是，吴三省有了个大胆的设想\n就是前边重点讲过的，把吴邪的记忆移植到齐羽身上！\n这样，齐羽就将作为吴邪活下去！齐羽得到了收留，吴家人得到了一个拥有吴邪记忆的人，更为关键的是，即便将来齐羽的记忆得到恢复，但他也同时拥有吴邪的记忆和感情，自然不会对吴家不利！\n于是，吴三省应该暗中征得了解连环、吴家、解家的同意，并告知了文锦\n他们共同实施了这个齐羽变吴邪的计划\n那个楚哥给吴邪的塌肩膀的人的照片，就是当时濒死的真吴邪的照片！\n那么，真吴邪的尸体在哪里？会不会就是阿贵那个塌肩膀的所谓“儿子”？\n被吴三省他们藏在了广西？\n真吴邪一定也吃了丹药，保证尸身不腐，在等待着。\n于是，92年前后，有着吴邪记忆的齐羽来到吴家，继续作为吴邪生活着。\n而，考古队，踏上了去云顶天宫的路！\n附——考古队11个人和吴邪录像带的秘密！\n当年的考古队，是11个人！\n假三叔（解连环）从鲁王宫出来后到云顶天宫后，对吴邪讲述考古队时都说是10个人！\n故意忽略掉第11个人——齐羽！\n考古队中的齐羽是吴邪在秦岭自己梦到的。\n这十个人是：\n在云顶天宫死循环中死去的五人，其中包括李四地和一个女人，另三人姓名不详。\n剩下五人是：陈文锦、霍玲、闷油瓶（张起灵）、吴三省、解连环！\n第11人！就是那个照相的人，是谁给考古队照的相呢？\n这个事情不大却关系重大的细节是被吴邪的大学同学发现的。\n南派三叔还特别隆重的写了一章，名字就叫《第十一个人》，显然是核心情节！\n这第11个人，也就是照相的人就是齐羽——吴邪！\n在蛇沼，文锦故意说照片上的吴三省是解连环，而吴三省当时在照相。\n如果真是这样，吴邪就应该在照片上看到一个和自己长得一模一样的人——齐羽！\n但实际并没有！所以文锦说的也是谎言\n但可以视作善意的谎言，主要是为了保护吴邪，不愿吴邪知道自己就是齐羽的秘密！\n录像带里无邪在地上爬，其实关系到一个是否“眼见为实”的问题！\n那么有两种答案：\n1、眼见未必为实！\n这盘录像带是阿宁拿给吴邪看的，阿宁身份神秘，不排除是阿宁他们造假，目的是把吴邪牢牢的牵进这件事。或者就干脆那盘录像带是阿宁当时把吴邪催眠后创造的幻觉。但是，如果吴邪和当年的考古队没有任何关系，他自己也不是什么倒斗高手，阿宁又何必费这么大力气要把吴邪扯进来呢？这不太合逻辑。\n2、眼见为实！\n那盘录像带里就是吴邪本人！\n这是吴邪和20年前那支考古队有关系的最直接的证据！\n随着笔记情节的展开，原来我们认为吴邪完全是无辜的“菜鸟”\n但，慢慢发现，吴邪身上的秘密一点不比其他人少！\n很多情节都暗示着，吴邪和考古队的重大关系。\n而且，故事的整体感觉，作者和读者的心灵互动，都在深深指向——吴邪绝不是局外人\n我想，这是每一个爱笔记，真正用心读笔记的人都不会感觉不到的\n山雨欲来风满楼，而风暴眼，正是吴邪！\n录像带里，那个在地上爬的和无邪一摸一样的人是谁？\n答案：就是无邪！\n世界上除了双胞胎不存在两个一模一样的人，父子也不可能。\n所以，我坚持认为，录像带里，就是吴邪本人！\n齐羽不衰老的身体+已死去的真吴邪的童年和少年的记忆\n试想，如果你忽然失忆了，失去了以往的全部记忆，有一个人把另一个人从小到大的全部经历，植入到你的记忆中，你是不是会认为那就是你的真实经历呢？你会想到你其实是另一个人吗？\n假设那时真吴邪得了绝症，死去了。把吴邪的记忆用尸蹩移植到齐羽（齐羽那时已完全失忆）\n那么齐羽就会认为自己是吴邪。他拥有吴邪十几年的全部记忆。他作为吴邪活下去，这是起点\n后边上大学直到现在，齐羽和吴邪就合一了，就是自己的经历和记忆了（或者说，还是吴邪，因为实际齐羽只是一个身体了。他作为一个人已经不存在了，他认为自己就是吴邪）\n所以，十几岁的真吴邪的记忆和十几岁后吴邪版齐羽的记忆就这样接合起来了。\n在蛇沼，文锦初见吴邪时：\n——她笑着说：“我看到你长这么大了的时候，我也反应不过来，想想已经二十多年了，当时你还尿床，我还给你洗过尿布，你那时候长得好玩，比现在可可爱多了——”\n是不是有些意味？\n当然，这一切有个前提，就是本来齐羽和吴邪相貌、身材就有些相似\n这也是真三叔想出这个绝招的先决条件，\n吴邪版齐羽的可能漏洞有三个：\n1、吴邪的记忆里会留有自己本来的相貌和病的记忆。\n\n-----这个只需在记忆移植后做一些处理就可以。短暂失忆+洗脑，让吴邪忘记自己的死亡（况且，死亡会留下记忆吗）相信自己病好了就可以。吴邪对自己长相有记忆，可以在移植时让吴邪的记忆遗忘，并反复强化齐羽的面容\n\n2、年龄问题\n———齐羽参加考古队时也就二十岁左右，而且他不会老，那么接合吴邪十几岁的 记忆问题不大。\n3、小时候的照片、朋友、邻居、同学等\n——---前边说过，吴家对这个一定是知情的，甚至是整件事主谋。而且吴邪和齐羽的相貌一定恰好有些像（但不可能一摸一样），照片不是大问题。对于其他邻居朋友同学，一场大病之后（邻居朋友都不知道真吴邪的死亡，吴家只说病治好了）人的相貌发生些变化真是很正常的啊！而且十几岁孩子相貌正是变化最快的时候。所以应该不会有问题。吴邪的大学同学描写的很多，而中小学同学，只写了老痒一个，而这个人的自己的问题，实在比吴邪还大！\n七、1993年——1995年间考古队的两次行动！\n93 年左右，考古队的七个人踏上了云顶天宫之路，他们的向导就是顺子的爸爸，他们在天宫的墓道中由于种种原因兵分两路，一路是顺子爸爸带着的五个人，也就是李四地等五个人（其中有一个女人），他们本来就是正常的考古队员，不是倒斗的，所以最后被大头尸胎困死在天宫的死循环墓室里。至于文锦和霍玲为什么要单独行动，这里一定还有一些事情。\n但最后，进入大铜门的绝不是文锦和霍玲两个人！文锦寄给解连环的录像带就是记录了他们在铜门内的片段，有闽南话的男人的声音，有陕西口音的男人的声音。这些人，就是真的吴三省和文锦等一起进入了大铜门！\n在大铜门内，文锦等看到了伏羲氏族终极的秘密，同时知道了昆仑西王母的信息\n于是下一站，他们计划到塔木陀！\n回到格尔木后，文锦开始整理西王母的资料，为前往塔木陀做准备。也就是文锦笔记的内容\n在1995年左右，文锦和霍玲来到蛇沼。\n文锦应该在吴三省的劝阻下暂时没进去。\n但是，霍玲这时估计开始和裘德考合作了\n吴邪他们在蛇沼中发现的很多尸体都是九十年代左右的，既有裘德考队伍的标记又有霍玲队伍的痕迹，这两支队伍很可能合作了。\n但是，他们惨败在蛇沼。\n霍玲回来后，开始尸化！（霍玲，这个人藏着很大的秘密）\n八、1995年至吴邪进鲁王宫前，平静之下暗流涌动！\n我们先假定吴邪进鲁王宫的时间是2004年或2005年\n那么从1995年起的这10年，应该是各方势力相对平静的十年。\n”它’的力量突然消失了。\n裘德考因为八、九十年代的几次惨败，也沉寂下去了\n文锦依然避居格尔木，但这些年她应该一直在探索塔木陀的秘密，为此做准备。\n解连环继续扮演着三叔。经营着表面上所谓的“倒斗事业”，奉命探索着考古队的失踪之谜（装的）\n“它”似乎也放松了对解连环（三叔）的控制，但是暗流依然在涌动，解连环身边的潘子就有问题！\n变成吴邪的齐羽倒是一无所知，快乐安逸的过着吴邪的正常生活。\n这里面，真正没有松懈的人是两个\n吴三省和闷油瓶！\n闷油瓶在海底中了古青铜的招，导致间歇的失忆，他一直在不断地进入这些关联的古墓，在尝试找到秘密！并在墓中留下记号。\n闷的身世本来和其他人不同，是从蚩尤到铁面生到张盐城传下的一支。\n他的信息来源、倒斗家传的经验和技巧都是其他人不能比的\n要不是失忆，以闷的才智和本事早不亚于汪藏海。\n吴三省是真正雄才大略的人\n他应该一直关注着闷油瓶，确定闷不是“它”的人之后，就萌生了和闷合作的想法。（闷在海底的失忆就是吴三省造成的）\n而且，吴三省对已经消失的”它“并没有丝毫掉以轻心，因为，他感觉事情并没有结束。而且，霍玲出现尸化后，文锦的时间也不多了。\n吴三省必须要有一个新的能掩人耳目的进入古墓探索秘密的身份\n陈皮阿四！\n陈皮阿四就是陈文锦的父亲，扮人一定要扮成一个熟悉的人，各种细节才不会露出破绽。文锦当然很熟悉陈皮阿四，可以指点吴三省扮的天衣无缝。\n真的陈皮阿四可能早已经死了，即使没死当然也不会戳穿吴三省。\n而且，即便陈皮阿四和文锦接触也不会让人怀疑，毕竟是父女。\n陈皮阿四这种老盗墓贼的身份也能方便吴三省在古墓活动。而且可以建立自己的力量！\n还有一个关键点：眼睛！\n假扮成一个人，眼睛和眼神是最不好伪装的\n所以要装成眼睛有问题的人才万无一失\n陈皮阿四恰好被苗人割瞎了眼睛\n书中一共有两个眼睛有问题的人：陈皮和黑眼镜！\n都是吴三省假扮的。\n而且，十年之内，吴三省假扮成陈皮在广西活动（广西偏远，掩人耳目）\n一定也是要寻找闷油瓶，因为闷的根在广西。\n大概五年之前，通过”阿坤“的事情，闷应该已和吴三省走到了一起，达成了合作。\n共同探寻秘密，解救考古队还活着的人，对付”它“的力量\n也就是，后来闷一直跟随陈皮阿四的原因。\n阿坤的事情应该是吴和闷两个人演的戏\n给闷一个合理的接触陈皮阿四（真吴三省）的借口\n而且，真吴邪的尸体是不是也是这个时候被吴三省弄到了广西\n假装成阿贵的儿子在瑶寨这种偏远地方藏了起来？闷也是在这个时候知道了齐羽——吴邪的秘密。\n更啦～\n九、吴邪、鲁王宫，风云再起！\n沉寂多年之后，神秘的”它“的力量突然又出现了！又开始全力介入这件事。\n就像当年西沙事件一样，这次幕后的总推手，依然是“它”！\n依然是以前的惯用手法——放出帛书新的内容，也就是新的秘密线索。这次的线索，指向鲁王宫！（全部帛书内容，一直都在”它“的掌握中！）\n于是，又搅起了各方风云又起！\n裘德考又开始新一波的行动了！\n当然，解连环也接到了”它“最新的任务，继续跟进和监视裘德考！\n解和裘的队伍，又”奇迹“般相遇了\n吴三省当然也知道了这一切，但在鲁王宫里，他没有出面\n而是让闷油瓶去配合解连环。\n他们三人，吴三省、解连环、闷油瓶开始形成了三人的团队配合关系，共同暗中对抗“它”。\n至于突然发现闷是二十年前没有老的张起灵云云，就根本是骗吴邪的鬼话。\n但，这次出现了一个新的情况，就是吴邪！\n大金牙把帛书送给吴邪，本身就是个阴谋，“它”一定是命令吴三省（解连环扮的）让吴邪也要参加这些行动。这说明“它”已经对吴邪的身份和吴家产生了怀疑。因为吴邪长的和当年的那个秘密特工齐羽一模一样，而那个特工应该已经死在格尔木疗养院了（吴三省、文锦他们应该找了个假尸体扮作死去的齐羽）但“它”还不敢肯定，于是就让吴邪进古墓来试探！\n这就是解连环对吴邪参与这件事奇怪态度的根源：既不敢不让他参与，又要瞒着他。\n吴邪这件事充分说明，解连环和吴家的身边有“它’的人\n这个人，就是潘子！\n也许，从一开始。潘子就是”它“安排在解身边监视吴三省和吴家的\n也有可能，潘子是”它“这次收买的，背叛了三叔（解）\n这次的风云再起，比西沙那次更为扑朔迷离\n几支势力互相上演谍中谍，你中有我，我中有你！\n形势复杂无比！\n吴邪、鲁王宫，风云再起！(二）\n在鲁王宫，虽然只有三叔（解）、潘子、闷、吴邪等几个主要角色\n但真是各怀心事，互相防范\n解和闷本来都是吴三省团队，他们之间的很多举动都是在表演。\n潘子是”它“的人，在监视解和闷，同时观察吴邪\n至于胖子，那是”它“新雇来的职业倒斗的\n用途和当年的解连环差不多，是利用他的倒斗知识探索古代秘密\n从胖子的出场看，明显他是上一拨倒斗队伍中的一员，也就是裘德考的队伍！\n应该是帮助裘德考队伍进入古墓的（从胖子的入墓地形图可以看出）\n但是，胖子知道的秘密并不多，还是相对单纯的。\n也许是胖子贪图明器，在裘德考的队伍撤出后，胖子找了个什么借口留在墓中\n恰好碰到了吴邪他们的队伍\n这是他认识吴邪和解连环的开始。\n这里面，真正”天真无邪“的就是吴邪，他真的以为自己和这事毫无关系，只是下斗的一次探险呢。其实他置身事情的风暴眼，但自己真的一无所知，不也很幸福吗？\n解连环是第一次进鲁王宫，但小闷以前明显来过，\n解和闷也是演了一场戏：\n主要目的有二：\n一是小闷拿到鬼玺（鬼玺可以比较便利的进入云顶大铜门）\n二是杀死血尸、毁掉鲁王宫( 最后，他们放火烧了鲁王宫) 阻止“它”的力量进一步探索血尸和鲁王宫的秘密。\n其实，蛇眉铜鱼已经不是重点，吴三省团队对整个秘密的了解应该已经超出了铜鱼上的内容\n但，铜鱼还是鬼使神差的落在了吴邪手中。\n至于小闷和铁面生的关系，前边已讲过。\n十 海底！海底！\n鲁王宫出来以后，\n解连环匆忙离开（在吴邪眼里是：三叔的失踪）其实是解连环又接到了“它”的密令！裘德考已经下海，让他抓紧前往！\n其实这时，“它’已经基本不信任解假扮的三叔了，之所以还派任务，主要是试探他，寻找他露出的马脚。\n其实，这时“它”主要力量，已经放在裘德考的队伍里了！\n是的，是阿宁！\n阿宁表面上是裘的人，其实是这一波“它’的行动主力。就像上一波行动当年考古队中齐羽一样！\n“它”把这一波的寻找秘密和长生探索实验的重点放在裘德考的队伍中。\n对于原来三叔的势力，因为他们知道的秘密太多了，又和考古队有说不清的瓜葛，所以更多的只是试探。\n吴邪在鲁王宫中的“天真无邪”并没能打消“它’的顾虑。所以，”它“授意阿宁把吴邪骗到海底墓去。借这个机会让阿宁试探他\n阿宁毕竟不是传统倒斗出身，而且她应该是第一次进海底墓，她需要帮助才能顺利进入，找到自己要的东西。\n她找的是张教授（张秃子）——明地宫的专家\n胖子——“它”雇佣协助倒斗的，就和上次在鲁王宫一样。\n阿宁的任务应该是先利用这几个人把她带到墓里，找到“它’ 需要的东西，\n最后干脆就是在海底杀掉他们三个灭口。还可以嫁祸给裘德考。\n闷油瓶假扮张秃子，显然瞒的是阿宁。\n因为阿宁一离开，他马上就恢复了原型，也就是说，他不防吴邪和胖子。\n那么，闷油瓶这次应该是被安排牵制和监视阿宁的，闷自己带着任务装作张秃子混进来。\n并且保护吴邪\n阿宁找到张秃子（应该有真的张秃子这个人），闷油瓶把张秃子藏起来，自己扮作张秃子。所以他那么夸张的语言和表现，都是为了像张秃子，不被阿宁怀疑。\n海底的这几个人就是这么凑起来的，这就是阿宁在海底的表现的原因\n阿宁在海底暗算他们，又将他们的潜水氧气放掉，就是下了海底后要杀死他们三个。\n当然，最后阿宁发现自己的能力不足以杀这几个人\n而且，这几个人不计前嫌，还救了她，虽然她是“它”的人，但人心都是肉长的，何况女人。\n从这时候起，阿宁应该对这几个人，尤其是吴邪的感情产生了变化。\n也是在这次海底，胖子也发生了转变\n胖子在鲁王宫和海底墓，的确是阿宁雇来的职业倒斗的，虽然，胖子并不了解核心机密。\n但是，海底墓中，阿宁几次显然要杀掉吴邪、闷和胖子三个人。导致了胖子对阿宁一方的憎恨，由是，从天宫开始，就投靠了吴三省团队，受命保护吴邪，但同样，他也不知道吴三省\n这一方的核心机密。\n胖子身上应该没有什么骇人的秘密，所以他一直给人感觉比较轻松轻快。\n在充满阴谋、背叛、沉重的盗墓笔记中，的确需要这么个“开心果”人物。\n这也是故事好看的需要。\n就像老三自己说的：胖子是有一点点小秘密，但是不是主要的（大意如此）\n海底！海底！(二)\n在吴邪的眼里，海底墓一切如此新奇，他刚刚接触了海底墓和考古队的秘密。\n想想，这一切是多么悲哀啊。\n所有人对吴邪讲海底墓那段往事时，都没对吴邪说实话，因为没有办法告诉他实话！\n但是每个人又都说了一部分实话：\n第一次是解连环和他说的，说三叔一直在睡觉，醒来后考古队失踪了\n第二次是闷在海底和吴邪说的，说了他们被迷晕和两个三叔，这对于当年的闷来说，就是说了实话，当然，后来闷知道了吴和解做局秘密！\n第三次就是解连环在医院再次讲西沙往事。\n第四次是文锦讲的三叔和解掉包的秘密。\n这几次，他们其实掩盖的核心问题都是：\n1、 吴和解共同做的这个“局” 2、考古队第十一个人——齐羽、吴邪的秘密！\n这两个核心秘密，也正是“它”现在怀疑吴三省和吴家，要找出来的秘密！\n还有，这次的海底，少了一个人，就是潘子，这进一步说明了潘子的身份。\n因为，有了阿宁，再放一个潘子就没有必要。\n潘子一定暗中监视失踪的三叔（解连环）去了！\n十一、秦岭神树——专门针对吴邪的骗局！\n“它” 现在对三叔和吴家的怀疑，集中到了吴邪身上！\n这个外貌和以前的秘密特工齐羽一摸一样，但是似乎又真是吴邪的人太可疑了。\n吴邪，已经成了吴家、解家这个局的最大的定时炸弹！\n这个有着齐羽身体和吴邪的意识的人，到底是齐羽还是吴邪？\n吴家人对他是矛盾的，不可能把他完全当吴邪，但是他又分明有着吴邪的记忆和对吴家的深厚感情，而且这么多年的相处，吴家和解连环应该对这个真的一无所知的人产生了感情！\n但是，吴三省和解连环更担心的是一旦齐羽的记忆恢复，吴家和解家就全完了！齐羽是唯一知道他们当年那个局的秘密的人！\n秦岭神树，是一个专门针对吴邪的骗局！\n因为吴邪的问题，已经是山雨欲来风满楼。\n首先，齐羽吃了丹药是不会老的，随着时间的推移，吴邪会发现这个身体的秘密。\n而且，现在“它“拼命在让吴邪参与这一秘密，来试探吴邪，那么在未来一个个诡异的古墓里，什么事情都有可能发生！或许，吴邪会恢复齐羽的部分记忆。\n神树，这个骗局，就是给吴邪的一个预防针！\n这个骗局的核心，就是让吴邪相信世界上有物质化这回事！\n这一方面是出于对吴邪的爱护，当未来吴身上出现各种超出常规的东西时，给吴邪一个合理的解释，让吴相信这是物质化。否则吴邪会疯狂的\n另一方面当吴邪出现记忆的问题时，他自己就会归结为这是物质化对记忆的影响。不会去深究这些属于齐羽的记忆！\n这样，既是对吴家的保护，也是对吴邪的保护。\n秦岭神树——专门针对吴邪的骗局！（二）\n这个骗局由谁实施呢？在哪里实施呢？\n秦岭的故事，应该是吴三省和解连环共同策划的。\n要让人相信一件事，当然要找这个人绝对相信的人！\n老痒！老痒是吴邪从小到大的发小，当然是吴邪最信任的人！\n话说很多人都说老痒是解连环的儿子，我同意这种观点！\n解连环的一生也非常不幸。从老痒和吴邪年龄相仿来分析。\n在 70年代末80年代初，解连环应该已经退出家族生意，堂堂正正上了考古大学。这时，他应该也有恋人，准备结婚了。这段时间解家已跟着吴家卷进了“它“的秘密行动，终于波及了解连环，解被指定加入考古队，解连环这时一定有了不祥的预感，不愿连累恋人，离开了那个姑娘，但是那个美丽的姑娘（老痒的妈妈很漂亮的）却对解一往情深，而且那时她已怀孕了，她自己生下了私生子，叫做解子庠！等来的，却是解连环已经死在西沙的消息。这就是老痒很小爸爸就死了的秘密。\n解连环刚假扮吴三省时，为了保密一定不敢和老痒母子相认，近在眼前却不能相认，解连环当真也是个坚韧的男子汉啊！吴邪记得小时候他爸爸经常带他去看老痒母子，这是吴家在替不能出面的解连环照顾老痒母子。\n在“它”神秘消失的比较平稳的十年，解连环和老痒母子应该还是相认了！当然老痒知道了吴、解两家的全部秘密和吴邪的秘密。\n秦岭骗局的地方，应该是吴三省发现的一个蚩尤部落的史前青铜祭祀遗址\n吴三省早发现那种古青铜有着致幻的强大功能，和记忆意识有关。\n于是。老痒在他爸爸解连环的安排和授意下把吴邪带到了这里！\n果然，巨大的青铜力量，让吴邪有了齐羽的记忆片段，那个怪梦就是齐羽中招前最后的记忆，被吴三省发现身份，跌进汪藏海的阴谋棺材等！\n老痒给吴邪进行了一场完美的致幻，让吴邪相信老痒就是物质化人，同时吴邪也具备了物质化的能力。至于秦岭中那些奇幻的细节，多数都是幻觉！就像老三说的，这是吴邪的一个梦！\n而后来老痒的出国和寄来的和他变年轻的妈妈的合影，只是强化了这骗局而已。\n老痒不是物质化的人，当然也没出国，后边的书中，他一定还会再出现的！\n秦岭神树，一场游戏一场梦！\n秦岭神树篇我认为全部人物只有两个——老痒和吴邪\n老痒把吴邪带到了蚩尤后裔的青铜神树边\n这棵青铜树，本就是用于巫蛊祭祀的，有强大的致幻能力。\n老痒就是利用古青铜的这种能力，给吴邪进行催眠，然后讲了一个魔幻的盗墓经历。\n至于凉师爷、王老板这些人物和烛九阴什么的，都是老痒讲的故事\n在古青铜的作用下，让吴邪以为自己真的经历了。\n目的就是为了让吴邪相信物质化这种能力！并且让吴邪认为自己也具备了这种能力。\n老痒为了解释让吴邪掌握物质化能力的动机，以及相信这事，并不再去深究，他妈妈的事是老痒自己编造的：\n1、他妈妈死了以及死而复生，都是老痒自己说的\n2、照片完全可以是PS的（现在的老痒和年轻时妈妈的照片拼一起）让吴邪进一步相信\n3、说他出国了，就是让无邪不再深究他的行踪\n十二、云顶天宫，各种力量的角逐！\n在第一波二十年前的行动中，其实裘德考和“它‘都失败了\n都获知了一些长生不老部分秘密，但都没得到终极答案\n那么这一次，对终极答案的争夺更加白热化，都怕别的势力抢先得到秘密！\n在二十年后第二波行动中，\n“它“已把主要的宝压在裘德考的队伍（阿宁）\n对于吴家和原来的考古队，主要是利用兼防范！\n这种地位，已经和二十年前倒了个。\n裘德考在海底获得了天宫的秘密后（壁画），马上派队伍奔赴云顶天宫！\n当然，解连环还是“它”派给他的老角色——牵制裘德考，依然强制要求吴邪也参加。\n在这种越来越白热化的角逐中，这次吴三省和解连环的布局费了很大的心思！\n首先，解连环以三叔的身份孤身打先锋，率先进入天宫，以抢在裘德考和裘队伍中潜伏的“它”势力阿宁的前边。\n接着以假三叔夹喇嘛的方式安排吴三省团队的其他人员。\n这次，真吴三省第一次露面了：陈皮阿四！\n由于真三叔的出面，这一次格外慎重，楚哥的出mai和遇到警察也都是吴三省的计策\n目的是怕队伍中混入“它”的奸细！\n经过这么一乱，最后出发到天宫的人员是：\n陈皮阿四（吴三省）和自己认为绝对可靠的三个人：华和尚、郎风、叶成、\n吴邪、潘子（从云顶之后，解连环的行动就再也不带潘子了，也许他对潘子产生了怀疑，又或许，潘子的任务改变了，从监视解连环变成了监视吴邪，所以，潘子下墓基本都和吴邪形影不离！）闷油瓶（吴三省团队）、胖子（“它”曾经雇佣的人，但已投靠吴三省团队）\n共八个人！\n云顶天宫，各种力量的角逐！（二）\n在天宫一章的开始，吴邪是在哪里见到陈皮阿四的？\n二叔的茶馆！\n陈皮阿四（吴三省）拿着的第三条铜鱼，也就是真的陈皮阿四在镜儿宫拿出来那条。\n就是吴三省假扮成陈皮在秦叔给吴邪请柬的那个拍mai会上拍下来的。\n这条铜鱼本来是“它”的饵，秦叔把这个信息故意漏给吴邪，就是为了试探吴家。但是聪明绝顶的吴三省将计就计：\n他正好一直假扮陈皮阿四，一个人很多年后高价拍下自己当年倒斗出来的东西很合理。\n同时，又给解连环假扮三叔夹喇嘛夹到陈皮阿四一个合理的理由。\n于是，陈皮阿四就不引人怀疑的出现在了天宫！\n这样，三条铜鱼等于都在吴三省这方力量的手上！\n从陈皮阿四在天宫的表现可以看出真的吴三省的风格\n真是雄才大略，做事干净、有力，有很浓的江湖风格，而且聪明绝顶！\n解连环其实一直都在模仿真正的吴三省的风格。\n但，解连环身上你细品还是有书生味道\n陈皮阿四的风格才是真正三叔的老丶江湖风格。\n从一进雪山开始，其实就是解连环和陈皮阿四（吴三省）、闷油瓶配合的一场局。\n这场局的目的不在于取得秘密，其实吴三省早就和文锦等进过大铜门\n他们的目的是破坏和阻止裘德考的队伍和隐藏着的“它”的队伍进入铜门。\n而且，大家还记得吗。\n鲁王宫的最后，闷杀死血尸，解连环他们烧掉天宫墓室\n吴邪他们进海底墓，最后也是通过把咋要炸毁了整个地宫！\n这些秘密，吴三省他们早都知道了，这一波的对抗，就是吴三省阵营要抢在前边，赶着毁掉这些秘密\n阻止裘和“它”的势力得到！\n云顶天宫，各种力量的角逐！（三）\n陈皮阿四到雪山后，主要的任务一是为了配合打前锋的解连环，延宕阿宁等的进程。\n二是观察和防范自己这边有无“奸细”，从而暴露吴三省团队的秘密\n所以，一路吴三省都装成老态龙钟\n但是，关键时刻他和闷油瓶的几个眼神交换表露了他们之间的默契！\n陈皮阿四突然死去又复生之谜？\n那是他发现了自己队伍里还是有可疑的人，他在用自己的假死试探。果然他试探出来了！\n他复生后，那个掐吴邪脖子的动作，其实就是当年吴三省在海底最后掐齐羽脖子的动作！\n他还是信不过吴邪，如果吴邪还有齐羽的记忆，这一下一定能试出来！\n好在吴邪真的没有齐羽的记忆了。\n最后一更\n后来，就是陈皮阿四发现被骗，挖出磁石乌龟，陈皮又让点燃乌龟！\n其实，吴三省和闷都来过云顶。\n这些，都是演戏！吴三省当然知道焚烧乌龟会引出蚰蜒！\n他就是为引出蚰蜒趁乱除掉他已发现的奸细！\n是谁？郎风！\n现在，大家应该看明白了，顺子就是解连环留下接应吴三省他们的人，甚至就是解连环装扮的！所以趁乱，顺子帮吴三省打晕了郎风！\n闷油瓶这时也失踪了，他是去完成的他的分工任务，抢时间率先进入大铜门，拿走或者毁掉秘密！\n这边，陈皮阿四单独叫走顺子，就是两人交换了意见\n之后，就出现了所谓顺子大叫：“开枪的留下”，放走陈皮阿四几个人。\n留下吴邪、胖子、潘子传达所谓“玄武拒尸”的密语！\n真实的目的是放走陈皮阿四，让他们到地下峡谷中放出巨鸟，阻挡阿宁他们接近大铜门！\n真是层层设防啊！\n那个记号，是人为把吴邪他们引入死循环。那个死循环真正的目的其实困住潘子。浪费他两天的时间。最后即使吴邪发现不了尸胎，顺子也能放他们出来！从死循环出来以后，顺子全部任务顺利完成了，当然，他就消失了！\n然后，吴邪他们发现了和蚰蜒激战的阿宁队伍以及队伍中的“受伤昏迷”的解连环（假三叔），其实那些蚰蜒就是解连环胸口里的蚰蜒尸体引来的，解是故意的，目的还是延宕阿宁他们的进程。\n解连环悄悄递给吴邪闷油瓶留下的那张纸条，正好说明了他们之间是有计划的配合。闷留下那张纸条是告诉解连环或者吴三省（扮作陈皮阿四）他的行动进度。说明他已经进了墓室。\n假设那张纸条是真留给无邪的——“我已进入墓室，下边的一切已不是你们能应付的”（大意），以胖子的性格以及吴邪对闷的关心，会不下去吗？这样，闷等于做了没用的事情，闷会做没用的事情吗？\n解还让吴邪用铜鱼骗取了阿宁们的信任。当阿宁的队伍终于来到万奴王的九龙抬尸棺和地底大铜门前时，巨鸟已被陈皮阿四他们放出来了，阿宁的队伍崩溃了。当然陈皮阿四也完成了任务消失了。而闷也靠鬼玺和阴兵一起进了大铜门。\n裘德考和“它”这一次又彻底失败了，什么秘密也没得到！没能进入大铜门！\n吴三省、解连环、闷油瓶配合打的这一仗真是太完美了！阻止了对手的行动却没露出任何马脚！\n这几个男人，真是优秀！！\n十三、向塔木陀进军！本来应是大结局！\n云顶天宫，以裘德考和“它’ 的完败收场！\n“它“当然也领教了吴家这块骨头的难啃！\n“它“也转变了突破的方向，就是考古队仅存的陈文锦！这些年，假三叔在严密的监控下，当然和文锦没什么联系，。但是“它”已发现了蹊跷，于是把突破口选在了文锦和吴邪身上！\n蛇沼的开头，解连环还在病床上，录像带就紧锣密鼓的寄来了。\n文锦认为，最后的时刻到了！\n因为，虽然文锦这些年一直都防范着“它”，躲藏在塔木陀雨林的边缘，但她还是发现，“它”开始向她步步进逼！另外，她的身体开始了尸化反应，时间已经不多了！\n这一次的雨季，必须进塔木陀！\n定主卓玛一家当然是文锦的人！\n文锦让他们寄出了录像带：\n一盘是给裘德考的，一盘是给解连环（也就等于给吴三省）、一盘给考古队的张起灵！\n此时的文锦，一定是已经完全明白了长生的秘密。\n她想把这几支力量一起召集到塔木陀\n做一个终极的了结！\n20余年，这些人的命运全部被改变，同时他们也全部是“它”手中的棋子和试验品！\n文锦就是想公开这一切，结束这整个邪恶不幸的故事。\n当然，文锦也知道，“它”的力量也会混在其中前来。\n“它”当然掌握了文锦寄录像带的行动\n是谁把齐羽发疯后在地上爬的录像展现给吴邪的？阿宁！\n这几乎就是“它”在直接的试探吴邪了。\n当然，这也直接刺激了吴邪跑到格尔木疗养院探索秘密！\n4、还记得把吴邪吓的半死的那个又像蛇又像人站立在那里的影子吗？吴邪以为是阿宁，其实那就是一只已被造好的人蛇！\n5、蛇攻击三叔（解连环）带去的队伍，也是本能的要制造人蛇！\n6、吴邪的那个怪梦，梦到阿宁变作妖怪，其实就是阿宁变作人蛇的前兆！\n7、阿宁当然没有死，只是变作了人蛇，吴邪他们一定还会再去社沼鬼城的！那时，一定还会遇到人蛇的阿宁，至于结果怎样，也许，吴邪们已经破解了西王母的秘密，解救了阿宁，也许，反而是最后变作人蛇的阿宁窥探到了西王母的秘密，救了吴邪！总之。阿宁的故事，还没有完！\n向塔木陀进军！本来应是大结局！(四)\n但是，“它”绝不是那么简单和无能！否则也不会让吴三省、解连环、闷油瓶这种人中龙凤煞费心机。\n树上挂的阿宁手链、像人又像蛇的怪物，会叫“小三爷”的蛇\n这些的背后，还有玄机，以后会讲到。\n闷油瓶本来已经失踪去保护文锦了，为什么在吴邪到达解连环他们突然撤离的帐篷时又回来了？\n答案是昨夜在暗中闷已经看到有人在组织蛇攻击解连环的队伍了。那么今夜也会照样攻击吴邪等。闷油瓶出现是为保护吴邪他们。让他们在帐篷上抹上泥。\n潘子是“它’ 的顶尖人物，无论身手，还是智慧。\n潘子从文锦和闷的举动应该能发现他们在怀疑自己，所以文锦始终不现身。\n于是潘子也设了一个计策。\n就是伪装被蛇咬成重伤，从树上摔下来。失去一切活动能力。\n其实，他没什么事。\n吴邪发现闷油瓶回来的第一眼，就看见闷蹲在潘子身前看着潘子\n这充分说明闷忌惮的谁。但潘子应该伪装的太好，闷也一时判断不出他是否真的受了极重的伤。\n直到那蛇沼最神秘的一夜！\n那夜毒雾使吴邪等失明，蛇群发动了攻击，\n这一切其实都是为了对闷下手，除去闷，文锦自然能抓到。果然，闷被咬伤了\n而，蛇攻击了所有的帐篷，为何偏偏放过了吴邪那座，答案是潘子在里头！\n那夜，吴邪刚复明时看见的黑影究竟是谁？真的是文锦吗？\n其实是潘子！\n那章有一个很诡异的小细节\n在蛇刚开始攻击时，他去摸潘子，在发烧。但过了不救去摸，竟然自己退烧了\n吴邪很吃惊。\n潘子发烧是伪装的，他翻找背包。当然是在找吃的，他装作重伤昏迷，当然没人给他吃饭\n但他若没事，当然会饿，要找吃的！\n向塔木陀进军！本来应是大结局！(五)\n当第二天吴邪无意中向闷说起昨夜看到的影子时\n闷立即大惊失色，因为他的怀疑证实了，潘子是装作重伤\n那么文锦就有很大的危险！于是他才提议马上用汤钓文锦\n其实这是闷和文锦约定的重大事情的暗号！\n果然，文锦出现了！闷再次和文锦会和，向陨石宫进发！\n这次的目的，就是甩掉潘子。当然，潘子无奈\n因为他假装受了那么重的伤，一旦再出现，就彻底暴露了。\n吴邪经过了一些波折，终于和胖子一起和解连环队伍汇合了。\n黑眼镜（吴三省）就在解的队伍中。\n当然，吴邪也见识到了解连环这支很不纯洁的队伍！\n此时，闷和文锦已经会合并且安全了\n那么，解连环就改为殿后！保证文锦和闷先进入陨石宫！\n于是，吴邪这时才终于见到了文锦！并听文锦讲起了往事\n文锦虽然没对吴邪讲全部的实话，但是她对吴邪还是相当善意的。\n之后他们继续前行，马上就要到达陨石宫了\n可以说这次的行动已经定盘，吴三省这方基本又胜利了！\n而且，文锦也和胖子见面了，并且对胖子基本没有防范\n这也再次说明文锦一路躲避的“它”并不是胖子，胖子没有什么问题！\n这时，文锦和解连环终于见面了，但是解已经被蛇咬伤！\n这因该是解连环和文锦自西沙分开后，二十多年的第一次见面。\n他肯定为当年吴三省和他没能解救考古队，而是给考古队喂下尸蹩丹药，从而造成考古队这么多年的苦难深感内疚！但是解为了这个命运也已经付出了自己的半生！\n所以，见到文锦，解很激动，所以，文锦明白解连环的意思，她说：这不怪你，你归队了！\n当吴邪喊解连环三叔时，解泪如雨下。\n在旁边看着这一切的黑眼镜（吴三省）心中又该是什么惊涛骇浪呢？但这个铁打一般的男人硬是一丝也没有流露出来！\n向塔木陀进军！本来应是大结局！(六)\n吴三省们的这次任务，本来就是护送文锦进陨玉\n并且，和“它”摊牌决战，但是看起来，“它”的终极BOSS并没出现\n事情还远远没有完！\n但是，文锦毕竟安全进入了陨玉，只有进入陨玉，是唯一克制尸化的办法。\n文锦将在陨玉中等待。\n闷油瓶进陨玉是为了护送文锦，查看陨玉中有没有危险\n他七天后出来失忆大概是两个可能：\n一是，文锦在里边确实安全，他放心的出来了，但为了隐藏陨玉内的秘密，他装作彻底失忆。\n二是，可能陨玉里还有陷阱，西王母的、汪藏海的、“它”的都有可能，闷再次中招了，真的失去了记忆！\n从现在的故事看，还不好判断是哪种！\n黑眼镜在陨玉外看着文锦进入，有闷油瓶护送他很放心！\n他身边解连环队伍中的人，很多都是“它”的人\n所以，吴三省绝对不能暴露，同时他在外边保护着文锦，防止“它”的人进陨玉！\n终于，几天之后，黑眼镜带着他的人离开了\n他和解连环，依然要踏上征程，继续寻找解救文锦的办法，并且，准备和“它”的最后一战！\n解连环到了敦煌，在敦煌给吴邪写来了那封信。吴三省是否和他一起在敦煌呢？\n敦煌，是不是格尔木之后，“它”的另一个大本营呢？\n在等到闷油瓶之后，吴邪、胖子和闷一起离开了塔木陀！\n踏上了为闷油瓶寻找记忆之路！\n向塔木陀进军！本来应是大结局！(七)\n我想大家追看盗墓将近三年，曾经都以为蛇沼会是整个故事的大结局！\n无论从时间上，还是故事本身的张力上\n太多的谜都到了非解不可的时候！\n就像一个铺垫的满场华彩的晚会，即将达到高潮同时结束。\n突然又宣布再延长三个小时。\n这种滋味其实并不好受。\n再次感叹，商业的双刃剑力量。\n超级长篇悬疑是很少有人能驾驭和挑战的\n这样无限制的铺排下去，也会给老三的写作造成困难。\n虽然能理解而又很无奈！\n其实《谜海归巢》真的是归巢的时候了。\n应该在陨玉之前安排一次各方力量的总摊牌。\n解开所有的谜题，各人走向他们应该的命运归宿\n最后，毁去这个超级迷人又超级邪恶的“长生之术”和“长生之地“\n有时候，死去比活着更幸福。\n人类正因为有黑夜才珍爱光明\n有死亡才珍爱生命！\n死亡并不能夺走生命中的一切：比如爱与正义！\n离去的人，安心的离去。留在世间的人，充满爱和力量的走向明天。\n应如是！\n至此，盗墓现已写完的这五部的现代部分之谜也全部解完！\n第一阶段、隐藏在国家力量中\n也就是1974年前后——1990年代初“它”神秘消失，格尔木疗养院被废这一个阶段。\n这一个阶段，前文已说过，主导这件事的的确是国家的力量。\n但是，在中国，国家的力量也是要由具体负责人操办的。\n霍玲的父母，可能正是当年负责这件事的军队高干！\n永生这种事情，任何人接触了都难免产生私心，这简直就是人类终极的追求！\n霍玲一家肯定也不例外。\n我想，老三最后发表时，拿考古队做人体实验这种残忍的事情一定不会被写成国家意图。\n而是，霍家的意图。\n让吴三省迷晕考古队，进而给考古队喂食尸蹩丹药，一定也是霍家假借国家的名义安排的。\n那么，当年的霍玲一定也是秘密打入考古队，借以实现家族意图的。\n我们可以看一下霍玲当年在考古队的表现：\n娇滴滴做纯真状当然是高干大小姐的本色，但也不排除她要掩饰什么\n关键还在于，当年吴三省装神弄鬼，装女人梳头引考古队进入奇门时，闷油瓶回忆他本来能看到吴三省的脸，但是霍玲似乎有意在前面挡了一下。这就很说明问题了。\n说明，霍玲在暗中协助吴三省引诱考古队进奇门，但是，吴三省应该不知道。\n吴三省迷晕考古队，喂下尸蹩丹药时，霍玲应该也在其中，她为了不暴露身份，伪装和考古队同样中招。但是，她当然不可能真被迷晕并吃下丹药，在那种情况下，吴三省也不可能观察很仔细！\n在考古队被运走到在格尔木疗养院醒来的一周之间\n我认为，霍玲已经被调了包！也就是说，有“两个霍玲”。\n霍玲的家庭一定找了一个和霍玲长的很像的年轻女孩，再稍作装扮，霍家利用权势胁迫这个女孩假扮霍玲，并在这女孩不知情的情况下给她吃下尸蹩丹药。然后把这个女孩放进格尔木疗养院和文锦等在一起。\n这样，霍玲就既瞒过了考古队，又可以自由在外边活动！\n假扮霍玲的女孩吃了丹药，不再衰老，还出现了反复梳头的死循环症状。录像带里的霍玲，就是这个假霍玲！\n注意：那个文锦所说的第一个出现“尸化”的女孩并不是假霍玲。\n那个可能是原来裘德考队伍里“中招”的人或者是霍家用于实验的其他人。\n大家记得格尔木疗养院有半层是用水泥封死的吗？估计封住的就是各种实验失败的妖怪\n格尔木疗养院就是霍家的人体试验场，就像汪藏海的海底墓一样！\n在这七八年间，“它”一直在监视观察这批试验品的反应。直到九十年代初。\n九十年代初，国家力量应该是正式放弃了这件事，开始时是监控力量大大减弱。因而文锦等七个人逃了出来，这里也包括假霍玲！\n后来，考古队又返回了废弃的疗养院，它的力量已经全部失踪了，就是说国家已经完全放弃了！但是，霍氏家族并没有放弃！而是转入了更秘密的阶段！\n这时候，吴三省应该和文锦等联系上了\n他们开始谋划天宫之行！\n第二阶段、十年前，霍玲的行动！（二）\n从天宫回来后，只剩下文锦和霍玲两个人。\n真霍玲这次和文锦一起住进格尔木疗养院，共同研究塔木陀，筹划塔木陀之行！\n大概在1995年左右，文锦和霍玲一起前往塔木陀\n但是，应该吴三省已经把对霍玲的怀疑告诉了文锦\n而且，文锦这次和霍玲前往塔木陀，自己也发现了霍玲的不对劲，所以，文锦和霍玲分道扬镳，文锦退回格尔木，而霍玲进了塔木陀！\n霍玲的重大秘密，发生在塔木陀！1\n当时，霍家已经无法大张旗鼓的借用国家和军队力量。但是霍家仍有很大的权势。\n因而霍玲这次行动，组织了一支自己的队伍，这支队伍，以女性为主，这也和霍玲的性别有关。同时霍玲还通知了裘德考的势力，和裘合作以增加成功概率！\n这就是吴邪们进入塔木陀时发现十年前的神秘武装力量和裘德考队伍遗迹的原因！\n裘的队伍的标志是钢印的号码，而霍玲队伍的标志是神秘的铜钱手链（对！就是阿宁戴的那个！）\n大家还记得吴邪、闷油瓶他们在死去的金鳞大蟒的蛇骨里发现的女尸吗？\n那个女尸有着裘德考的钢印号码，带着手榴弹（那个年代，只有霍玲的队伍能弄到这个）\n还有，吴邪的怪梦，那个女尸带着铜钱手链，变成阿宁！\n这，就是霍玲的人！甚至我认为可能是阿宁的母亲！\n霍玲这次在塔木陀，发现了重大的秘密——就是西王母人蛇共生的“蛇人”长生之谜！\n霍玲一定是留在了塔木陀开始研究这个秘密！\n为了不引起文锦等的怀疑，霍玲让假霍玲冒充从塔木陀出来的自己回到疗养院！\n恰好，这时，假霍玲开始了尸化。\n文锦看到的回来后尸化并变成紧婆的是假霍玲，吴邪在疗养院见到的那个禁婆也是假霍玲。\n而，真霍玲，留在了塔木陀。\n裘德考团队活下来的人和霍玲自己团队里的“青铜手链”女人们\n应该像考古队一样，再次成了霍玲的试验品\n他们像阿宁那样被鸡冠蛇咬后，一些死去\n很少的，变成了“蛇人”！\n这些蛇人，既是霍家的试验品，又是武器，因为他们变成蛇人后，就能操控鸡冠蛇进攻了！\n霍玲应该在塔木陀呆了不少年，研制蛇人。\n这十年，国家的力量已放弃，霍家的力量留在塔木陀！\n正是前边讲的表面上风平浪静，吴邪安然在吴家生活的十年。\n第三阶段、十年后的“它”——霍玲，全面出击！\n霍玲在十年之后为什么又离开塔木陀，“重出江湖”，搅得风云再起？\n答案是：就像汪藏海一样，人蛇共生的长生方式并不是霍家需要的长生方式！\n因为，那样人就变成了妖怪！\n而活人吃尸蹩，也已经被证明只能产生禁婆那种妖怪！（她还未必知道活人吃尸蹩进玉佣后能永生）\n这几种永生实验几乎都失败了，一切似乎走回了起点！\n霍家和霍玲当然不甘心，接近这秘密的人哪个不像穿上疯狂的停不下来的红舞鞋？\n于是，线索又重回战国帛书。\n鲁王宫的那部分似乎指向了另一种长生方式！\n一切，又要重新开始！\n这次，霍家和霍玲——它中之“它”会如何行动呢？\n现在，它毕竟已经失去了国家的力量，所以只能选择纯粹的“暗战”。\n这次，在表面上，它选择了裘德考！把自己的力量——阿宁放在了裘德考的队伍中！对，阿宁也有铜钱手链！阿宁就是霍玲的人！\n对于吴三省团队和考古队剩下的文锦，霍玲应该是这个态度：\n首先，修正一下前边的解谜，最新这一波行动中，“它”不应该也无法再命令假三叔（解连环）继续像二十年前西沙那样监视和控制裘德考了！\n因为，90年代初国家的力量撤出后，等于假三叔（解连环）也解放了，因为他也不用再为国家力量做事了！\n而霍家的行动也是秘密的，他们不可能继续假借国家力量命令解连环做事！也不可能以霍家的身份命令吴解两家！吴、解两家也不傻！\n所以，前边分析的“它”继续命令解连环应该不合理！\n实际上，变成了纯粹的“暗战”！\n霍玲对于吴三省团队和文锦：\n1、 霍玲在和文锦进天宫时，已经对吴三省的身份产生了怀疑，进而怀疑到了解连环，从而怀疑吴、解两家。所以，此次她把主要力量放在了裘德考队伍中！\n2、 但是，从进天宫的那次起，霍玲隐隐感到吴、解家族对这个秘密有很深的了解，她一方面怀疑他们，一方面又想利用他们得到自己想要的长生之谜！\n3、 霍玲应该先收买了解连环身边的潘子！用以监视吴、解家族。潘子在汇报吴邪这个人的时候（应该是给了霍玲照片），霍玲大惊，发现竟然吴邪长的和当年的秘密特工齐羽一摸一样！（霍玲当然知道齐羽是特工，但当年在海底她应该没看到吴三省和齐羽的打斗以及齐羽跌进棺材中招，所以她一直不知齐羽为什么也中招了，加之西沙之后，她并不在格尔木疗养院，不知道也不会关心齐羽的下落）霍玲由此对吴邪产生了极大的怀疑！\n4、 霍玲当然也怀疑文锦，应该是派人开始监视文锦！\n第三阶段、十年后的“它”——霍玲，全面出击！（之鲁王宫 ）\n十年之后，霍玲最新一波的行动，从战国帛书中记述的鲁王宫开始，这个地方，似乎关系着一种新的霍玲不知道的长生方式！\n霍玲先是把帛书上这部分的信息透露给裘德考，裘德考当然要行动了，于是阿宁就进入了裘德考的团队！阿宁和裘德考的团队有一个共同点，就是缺少“土夫子”的倒斗经验，所以霍玲雇佣了职业倒斗的胖子协助阿宁！胖子手上的鲁王宫地图，一定也是霍玲给的！\n对于吴、解家族这边，霍玲也是派一个老头拿着帛书关于鲁王宫的图去找吴邪\n她一方面要把假三叔卷入其中，为了利用他们\n另一方面把吴邪卷进来，是试探吴邪和齐羽的秘密！\n正因为这个帛书上的鲁王宫地图，裘德考和解连环两支队伍先后进了鲁王宫！\n试想，战国帛书都在国家手上，除了霍玲这种特殊背景的人，谁还能拿得出来？\n帛书片段，就是霍玲钓裘德考和吴、解家的饵！\n裘德考和阿宁这些人终归没有经验，队伍应该在七星疑棺那里就崩溃了(疑棺那留下了外国人的尸体)，本没能进到真正的鲁殇王墓室就退了，只有贪财的胖子借机留在墓中！\n再说吴三省团队这边，\n前边说了，虽然90年代初“它”的力量忽然消失，但是吴三省并没放松警惕\n而且，同进天宫时，吴已经开始怀疑霍玲！\n这次帛书再现，并送到了吴邪的手上，怎能不引起吴三省的极大警惕？\n吴三省这次安排解连环（解带着潘子和大奎）、闷油瓶带着吴邪（如果吴邪不去，那么正好说明吴邪有问题）下墓。在墓中遇到了胖子\n吴三省肯定提醒了解连环一定会有奸细，但他们还判断不出是谁。所以，墓中解和闷一直在演戏！\n解连环应该是第一次到鲁王宫，但是吴三省和闷油瓶肯定早就来过。他们也知道玉佣长生的秘密！\n解和闷下墓后，就判断裘德考的队伍和“它”的队伍根本没进主墓室，也没看到血尸、玉佣的秘密！\n吴三省团队都知道，凡是“它”接触永生的秘密，就必然给很多人带来灾祸和不幸，所以他们要毁去鲁王宫的秘密，防止裘和“它”再次进来。\n为防止内部的奸细，闷就做戏似的杀死两具血尸（其实闷早就能杀掉穆王血尸，为什么要等到那时候？就是避免怀疑）闷拿到鬼玺，蛇眉铜鱼也落到吴邪手里。最后，解和闷装作迫不得已一把火烧了鲁王宫。这样一切秘密都消失了！\n霍玲和阿宁在鲁王宫什么也没得到。奸细潘子应该也没看出什么破绽！\n第三阶段、十年后的“它”——霍玲，全面出击！（之海底墓）\n鲁王宫之后，霍玲一定极为恼火\n急令阿宁通过裘德考团队前往海底墓！霍的团队和吴三省团队开始了抢时间。\n海底墓中，似乎也有另一种长生的痕迹，只是当年霍玲们没有好好研究。\n得知阿宁的行踪，解连环立即去到海底墓（三叔的失踪）\n这时，解应该已经怀疑潘子了，不再带潘子，而是用“失踪”来单独行动！\n在解的干扰下（解对海底墓很熟悉）裘德考和阿宁似乎又一无所获。\n所以才有阿宁抛开裘德考，一人再次下墓。\n这次，阿宁请了张秃子和胖子做帮手，并在霍玲的授意下，骗来了吴邪\n想用海底墓这个对齐羽来说很特殊的地方试探吴邪。\n这三人帮阿宁下墓后，阿宁就杀掉三人灭口！\n吴三省团队这边：\n张秃被闷油瓶掉了包\n阿宁当然杀不了这几个人，他们反而救了阿宁\n阿宁应该除了拍到几张云顶天宫的照片，也没得到什么\n这次海底的结果是：胖子投向了吴三省团队，阿宁对吴邪似乎产生了些感情！\n最后，闷炸掉了海底墓。\n第三阶段、十年后的“它”——霍玲，全面出击！（之云顶天宫）\n海底墓的结尾，\n吴邪搜索“吴三省”名字的时候，跳出了一个寻人启事\n一张20年前考古队的照片和一句话“鱼在我这里！”\n我认为，这也是霍玲所为，同样是一个饵！\n时间应该在送给吴邪鲁王宫地图的战国帛书同时，目的是引诱吴解家族参与。\n“鱼在谁那里？”————霍玲！\n老海给吴邪讲鱼的来历时，应该说过铜鱼被一个姓霍的中年富婆买走过，这个应该是霍玲本人！\n铜鱼，主要描述了云顶天宫的信息和海底墓的信息！\n云顶天宫，也有另一种永生的秘密！\n拍卖铜鱼，同样是霍玲的鱼饵，事情发展到现在，霍玲一定发现假三叔有问题。\n拍卖铜鱼，可能是霍玲在钓真三叔\n结果真三叔将计就计，扮作陈皮阿四拍到了铜鱼。\n霍玲对天宫比较熟悉，所以这次阿宁们动作很快！\n而，吴邪这次又是怎么加入的呢？——潘子传的信息，说吴三省（解连环）让吴邪到天宫！\n这又是霍玲把吴邪卷入的阴谋！\n之后，就是真三叔（陈皮）、解连环和闷油瓶在天宫演的一出好戏！\n让裘德考团队和阿宁没能接近大铜门，无功而返。\n前边已经解析的很详细，这里不赘述了。\n第三阶段、十年后的“它”——霍玲，全面出击！（之蛇沼鬼城）\n鬼城是霍玲力量和吴三省团队斗争的高潮！\n天宫之后，霍玲把重点转到文锦身上，想以文锦为突破口。\n于是加紧了对文锦的监控，文锦，也预感到最后的时刻来了。\n所以寄出了录像带！\n而录像带，被霍玲势力改变了邮寄对象！\n阿宁给吴邪看的齐羽在地上爬的录像，是再次把吴邪卷入！\n数次的失败，霍玲这次，实在是已经对这些人动了杀心。\n而蛇沼是除掉他们的最好地方，因为这里有霍玲的秘密武器——蛇人和蛇人控制的鸡冠蛇！\n霍玲这次的目的是——抓文锦，同时一网打尽吴三省团队。\n为未来扫清障碍。\n吴三省团队此次任务是——保护文锦安全进入陨玉，因为文锦已经出现了尸化反应。只有进入陨玉才能克制尸化。\n其次是找出“它”的终极BOSS ，大家摊牌（吴三省早就怀疑霍玲，但可能还不能肯定）\n裘德考和阿宁团队也来了，但这时他们早已不是霍玲的杀招！只是用来牵制吴三省团队的。\n这次又不例外，裘和阿宁的团队还没进沼泽就崩溃了\n而，黑眼镜（真三叔）协助解连环的队伍一路急进，他们是为文锦前往陨玉扫清道路的。\n而文锦，跟着吴邪、闷油瓶和胖子，当然这队伍里有潘子，闷主要保证文锦的安全\n文锦的逃跑和所谓抓文锦是闷油瓶和文锦设下的计策，保证文锦安全，躲避潘子的。\n对于阿宁，霍玲应该是极度失望和生气，因为阿宁虽然忠心耿耿，却几乎从没完成过任务。\n霍玲想在社沼把阿宁变成“人蛇”，可能还更有用些。\n但阿宁不知真情，应该是接到了霍玲要处死她的密令。\n所以，在瀑布被蛇咬之前，她对吴邪那样灿烂而又凄婉的一笑\n灿烂，是因为他对吴邪的感情，凄婉，是她知道自己要被处死的命运！\n霍玲，给这些人设计的命运是，被蛇咬死！\n对于解连环的队伍，是在他们进蛇沼后，驻扎的第一夜让人蛇指挥鸡冠蛇袭击他们，一些人被咬死了，但是真三叔和解连环带着一些人逃脱了，这就是吴邪他们看到了三叔他们突然撤离的空帐篷。\n对于闷油瓶和文锦这边，霍玲采取的是各个击破战略\n闷油瓶在沼泽边缘就去追文锦，其实是和文锦会合，保护文锦\n而文锦，偏偏又会涂泥防蛇，霍玲一时拿他们没办法。\n于是就先设计除掉吴邪\n那些蛇用长沙口音喊“小三爷”，正是霍玲教给潘子的办法。\n让潘子一路喊小三爷，蛇学会后，就能把吴邪单独引开，咬死。（他们估计看出胖子在保护吴邪）\n吴邪一死，胖子也好对付。\n吴邪在黑夜中看到的那个站立的半人半蛇的怪物，以及树梢上挂的铜钱手链\n让吴邪以为那是阿宁，其实那个就是十年前霍玲“铜钱手链”团队中被制成的蛇人！\n是可以操纵鸡冠蛇的\n好在吴邪命大，没有中招！\n同时潘子看出闷和文锦是因为他才不现身\n所以他就设计策装作受重伤。\n这时，他们正好到了昨夜解连环团队扎营的地方！\n闷油瓶昨夜亲眼看到蛇攻击解的团队，担心今夜吴邪的安全，所以回来了！\n当夜，借助毒雾，蛇人指挥鸡冠蛇群发起了进攻！\n其他所有的帐篷都被攻击，是为了杀掉闷和胖子\n只有吴邪的帐篷没受攻击，是因为潘子在里面！\n结果，闷只是轻度被蛇咬伤，胖子和吴邪无恙。吴邪还发现了黑影——潘子\n闷借助抓文锦甩脱了潘子，再次和文锦汇合\n而，吴邪历尽危险也再见到了闷和文锦！\n他们，几乎和黑眼镜、解连环队伍一起靠近了陨玉！\n最后，文锦在黑眼镜（三叔）和闷的保护下安全的进入了陨玉。\n基本上，吴三省团队的最大目的又实现了！\n但是，他们没能找到终极BOSS 霍玲和霍家！也没能摊牌决战。\n故事还远未结束！\n霍玲和吴三省团队的较量，还远未结束！\n正义终将战胜邪恶！（结尾）\n南派三叔的盗墓笔记还远远没有结束，\n我们不知道还要等待多长时间、再看多少部才能等到结局？\n在霍玲和霍家这边，\n还会继续寻找永生之谜，像被魔鬼抓住了灵魂。\n在最新一季的阴山古楼，裘德考终于出现了，这次，霍玲安排了谁在裘的队伍中？\n裘德考对于自己的二十余年的棋子命运有所感知了吗？\n阿宁不会死，她变成人蛇后会如何呢？\n潘子还会出现，他的结局又是什么呢？\n在吴三省团队这边，\n文锦还在陨玉中等待，等待吴三省们的解救。\n陨玉中还有其他人吗？比如西王母、汪藏海，又会给文锦的命运带来什么变化？\n闷油瓶能找回自己的记忆吗？能解救自己吗？这个迷人的人结局会是什么？\n可爱的胖子呢？他还要继续卷进这个谜？\n吴三省和解连环，这一对为了这个秘密付出了半生的兄弟，会双双死去吗？\n老痒呢？何时再出现，面对他的父亲和童年好友吴邪？\n最后，是最让人揪心的吴邪\n“遗忘，意味着背叛”，这个遗忘和背叛了自己一生的人\n能重回阳光下，过着宁静平凡却幸福的生活吗？\n这些，我们都不知道\n但我们知道：正义必将战胜邪恶！\n光明必将代替黑暗！\n生命会死亡，爱将永存！\n本阶段解谜到此全部结束\n谢谢大家！！\n转载请注明出处哦。（来自百度论坛暖和狐狸）\n尝试为三胖子填坑：七星鲁王宫里所谓的三千年婴儿被张家长老奉为不死的象征，但三千年婴儿其实已经死了，为了保住张家信仰的核心，闷油瓶被抱出来代替了这个三千年婴儿作为张家信仰的核心。汪臧海和汪家作为第二股势力，了解到了长白山下青铜门内的终极秘密，然而作为第一股势力的张家的目的是保守这个秘密。汪家用了很久的时间，在张家的基层一点一点插入渗透自己的势力，从根本上架空了张家，张海杏说“早就没有真正的张家人了”，其意应指她，以及其他的一些“张家人”叛变了张家，或者根本就是汪家安插的间谍。张启山的祖父是张瑞桐，前某任张家族长。张启山一支，是从北方南迁至长沙的，而且应该就是他这一代才到了长沙，因此可知张启山对于张家的终极秘密一无所知。按照《沙海》里所述，汪家不仅渗透了张家，还把眼线布到了全国，为了加速张家的瓦解，他们“甚至改变了社会制度”。张启山早年盗墓，中年从军，晚年从政，甚至似乎还成了开国元勋。某位领袖在晚年时惧怕死亡，因此授意九门提督在四川开展了“史上最大的盗墓活动”，闷油瓶说的“为了一个活着的人”，指的应该就是这位领袖。根据汪家为了对付张家改变社会制度这一条件，可以推论出这位领袖和他的势力其实是受汪家控制的，九门在张启山的领导下，因此也是受汪家控制的。那具被吴五爷和解九爷藏起来的棺材，会不会里面就是那位领袖呢？吴家霍家和解家先后觉醒，决心要对抗“它”，这个逻辑上必然存在但又看不见摸不着的势力，也就是汪家。那么吴邪秀秀还有解雨臣他们所做的一切都是在继续这个他们长辈开始的计划，也就是从根本上离间汪家，最后使他们分崩离析。关于“终极”，恐怕连作者自己都没有想好该怎么设定这个秘密，不过我们大体可以推导出个轮廓来：不管具体是什么，应该都是指这个世界的真实面目，或者是世界即将面临的真实面目，既有可能是真相，也有可能是预言，但一定完全动摇我们对世界的认识和信念，因此张家应该是守护者，汪家是破坏者。吴家祖孙和黎簇能读取蛇毒中的费洛蒙，吴邪在黎簇之前进行了十七次尝试，也就是说能够读取费洛蒙的人并不仅仅是他们；张家族长的宝血应该来自蛇毒，不过这里似乎就有一个悖论：闷油瓶八岁时被带入泗州古城放血，十三岁放野，依靠宝血才能进入古城深处。然后成为张家族长，最根本的原因应该是他拥有可以辟邪护体的宝血，而并非是在成为族长是拥有了宝血。张家的秘技里，发丘指和缩骨功是从小练习，而只有宝血可遇不可求，所以才会成为选拔族长的依据。九门提督里，传到第三代的应该只有三家，而他们和他们的长辈都不可避免的被卷入了张汪两家的博弈，那么，齐羽是什么人，霍玲和文锦又是什么来头，他们又遇到了什么，甚至，吴邪的真实身份到底是什么，这些不是推理能解决的问题，唯一的方法就是等三胖子自己一点一点的构思好再慢慢填坑。。\n张启山在接到为领袖寻找长生不老方法指示的时候，就知道关键是要找到不知所踪的族长。张起灵计划由此开始。这就是阴山古楼里，鬼影所说“他们又找到一个”的意思。闷油瓶生于四十年代中期，根据在张家古楼里的发现，张家人普遍年龄都在一百七十岁以上，考虑到意外死亡与正常死亡的区别，张家人和寿命可能在二百岁以上，霍家二小姐看到闷油瓶下跪，说明当年那个“面无表情的年轻人”，有可能就是闷油瓶，说明张启山最后应该找到了他们的族长，但由于失忆，闷油瓶已经记不太清那时候的事情了。按照张海客所述，闷油瓶前一任族长死于泗州古城之中，还有那个大铜铃。闷油瓶找到了那个青铜铃铛，但是没有得到前任族长口述的张家核心秘密，七十多年来闷油瓶一直专注的事情就是寻找这个秘密——他至少两次进入青铜门；还有寻找他自己的身世。闷油瓶在进入鲁王宫前是处于一种半失忆的状态，直到最后进入云顶天宫。随后，闷油瓶为了完成计划来到了蛇沼鬼城，在陨玉中失忆那么问题来了，陨玉里到底发生了什么，文锦又在哪里。但是这一次，他没有等上很多年才找回记忆，而是与吴邪等人迅速与霍解两家取得联系并进入张家古楼，再次找回了记忆并进入青铜门。五年后，吴邪在墨脱第一次与汪家正面冲突，他又找到了吴五爷的骨灰，最终彻底决绝的展开复仇的计划，汪家分崩离析，虽然还不知道黎簇如何在众目睽睽下突然消失，但为了谨慎起见汪家总部转移，“汪家上层有内鬼”的猜想也迅速充斥在每个知情的汪家人的脑海里。可以设想的是，汪家总部转移之后必然会彻查原因，而这么一个大家族，里面的成员不可能没有矛盾和冲突。当私人的恩怨与公务叠加在一起，内斗必然会发生在汪家的每一个阶层。这一过程应该是在两年内完成的，随后这个“它”也彻底不成气候。张瑞桐，也就是张启山的祖父死于失魂症，在三胖子更新之前，能不能够设想闷油瓶也遇到了同样的事情呢？而且，鉴于这个故事已经涉及到现实，估计三胖子永远也不会填坑了\n在七星鲁王宫里，王胖子就是一个bug，云顶天宫，他带着闷油瓶再次出现，一直到现在：一个完完全全不知道底细的人，闷油瓶阿花秀秀和大邪都是九门提督的后人，瞎子的身世虽然离奇但也有据可查，唯独这个胖子，误打误撞的出现在鲁王宫——真的是巧合吗？\n盗掘具有历史、艺术、科学价值的古文化遗址、古墓葬的，处三年以上十年以下有期徒刑，并处罚金；情节较轻的，处三年以下有期徒刑、拘役或者管制，并处罚金；有下列情形之一的，处十年以上有期徒刑或者无期徒刑，并处罚金或者没收财产：\n（一）盗掘确定为全国重点文物保护单位和省级文物保护单位的古文化遗址、古墓葬的；\n（二）盗掘古文化遗址、古墓葬集团的首要分子；\n（三）多次盗掘古文化遗址、古墓葬的；\n（四）盗掘古文化遗址、古墓葬，并盗窃珍贵文物或者造成珍贵文物严重破坏的。\n所以，青铜门就是监狱门，鬼玺和王玺是刑法，阴兵借道是jc，十年之约是有期徒刑十年。本来应该判吴邪，结果小哥顶了十年雷。文锦之前进去过…\n大概就是这意思"},"F_Others/为何对于《将夜》，许多人都宁可选择莫山山，而不选择桑桑？---知乎":{"slug":"F_Others/为何对于《将夜》，许多人都宁可选择莫山山，而不选择桑桑？---知乎","filePath":"F_Others/为何对于《将夜》，许多人都宁可选择莫山山，而不选择桑桑？ - 知乎.md","title":"为何对于《将夜》，许多人都宁可选择莫山山，而不选择桑桑？ - 知乎","links":[],"tags":[],"content":"\nwww.zhihu.com/question/36612666/answer/536034822\n\n莫山山就是男频文里标准的女主设定，标准到千篇一律。她在书里的每一场戏，作者都是按照女主的套路来的。（相识、相遇、英雄救美、谈心、同生共死、女追男）\n将夜这本书，没有从宁缺刚出生开始写，所以很多读者无法体会宁缺和桑桑十五年的相依为命。\n他们醉心于，红墙白雪，我喜欢你。\n却忘了，三十粒葱花，两个煎蛋。\n他们享受着和盛名天下的山主并肩同游长安城。\n却忘了老笔斋里日日夜夜等待宁缺回家的身影。\n莫山山很好，她真的很好，完美无缺。所以她得到了无数读者的喜欢。\n反观桑桑，不论书里书外，都不讨人喜欢。但是宁缺说，没关系，我喜欢你就够了。\n追将夜这本书最大的感触是，作者本人很喜欢桑桑。他每次写到桑桑，都会在书后面留言，哎呀一写到桑桑我就开心~我就写得飞快~~\n书里宁缺总是一口一个我家桑桑、我家桑桑，书外猫腻也总是桑桑、桑桑、桑桑……这就够了啊。\n宁缺喜欢，作者喜欢，我喜欢。\n就很美。"},"F_Others/将夜人物群像":{"slug":"F_Others/将夜人物群像","filePath":"F_Others/将夜人物群像.md","title":"将夜人物群像","links":[],"tags":[],"content":"\n现在是2021/6/6 夜，刚写完pami论文初稿，明天要去老师那边改论文。\n不知道为什么要写这篇，反正就是想写，那就写吧。\n正如书院那样，追求有意思。\n《将夜》这部小说真的喜欢，带来了过多的惊喜，每天白天日常干活，晚上都会回到将夜世界去看看，看这个宏大的世界，仿佛置身其中。\n剧情过于宏大，不需写，那就画一篇人物群像吧！\n\n夫子—人间最强者\n李慢慢\n君陌\n宁缺-桑桑"},"F_Others/我的楼兰":{"slug":"F_Others/我的楼兰","filePath":"F_Others/我的楼兰.md","title":"我的楼兰","links":[],"tags":[],"content":"你总是随手把银簪插在太阳上面\n万道光芒蓬松着你长发的波澜\n我闻着芬芳跋涉着无限远\n只为看清你的容颜\n你总不小心把倩影靠在月亮上面\n万顷月光舞动着你优美的梦幻\n我闻着芬芳跋涉着无限远\n只为看清你的容颜"},"F_Others/我记得":{"slug":"F_Others/我记得","filePath":"F_Others/我记得.md","title":"我记得","links":[],"tags":[],"content":"我带着比身体重的行李 游入尼罗河底 经过几道闪电 看到一堆光圈 不确定是不是这里 我看到几个人站在一起 他们拿着剪刀摘走我的行李 擦拭我的脑袋 没有机会返回去 直到我听见一个声音 我确定是你 可你怎记得我 我带来了另界的消息 可我怎么告知你 注定失忆着相遇 我记得这里是片树林 后面有个山坡 山坡上的枣树每当秋天到来 我们把枣装满口袋 我记得除了朋友我还 做过你的叔父 你总喜欢跟在我的屁股后面 只是为了那几个铜钱 我记得我们曾是恋人 后来战争爆发 你上战场后就再也没有回来 直到收不到你的信 我们总这样重复分离 却要重新开始 相互送别对方 说着来世再见 再次失忆着相聚 呜 呜 呜 呜… 快来抱抱 快来抱抱我 呜 呜 呜 呜… 快来抱抱 快来抱抱我 在路上我遇到了一位故去多年的人 她是如此年轻 扎着过肩马尾 露出和你一样的笑 她和我讲了很多关于你成长的故事 在星空另一端 思念从未停止 如同墓碑上的名字 不要哭我最亲爱的人 我最好的玩伴 时空是个圆圈 直行或是转弯 我们最终都会相见 在城池的某个拐角处 在夕阳西下时 在万家灯火的某一扇窗纱里 人们失忆着相聚 呜 快来抱抱 快来抱抱我 呜 快来抱抱 快来抱抱我 我终于找到你 呜 快来抱抱 快来抱抱我 我终于找到你"},"F_Others/软件购买信息记录":{"slug":"F_Others/软件购买信息记录","filePath":"F_Others/软件购买信息记录.md","title":"软件购买信息记录","links":[],"tags":["软件"],"content":"\n这个文档记录一下我买过的软件相关信息，方便统一查找\n\n\n\nUPDF 软件永久 + 一年 AI 会员 （账号登录）\n\nrunyang2019.feng@gmail.com\n感觉 AI 会员鸡肋；软件本身还行\n\n\n\nTypora 软件永久\n\n邮箱：1443918219@qq.com\n序列号：A4Q4KH-XBAT79-2ZTL6V-MN7ZRL\n\n\n\nDynamic Wallpaper 壁纸软件永久\n\nApple ID\n基本上没用过\n\n\n\nOffice365 for Mac 软件一年\n\n3319609217@qq.com [微软账户]\n\n\n\nDownie 4 软件永久\n\n用户信息: Runyang Feng / runyang2019.feng@gmail.com\n激活码: F0B3BC29-898D1F60-7E8068E2-B979A595-F450B03A\n\n\n\nPermute 软件永久\n\n用户信息: Runyang Feng / runyang2019.feng@gmail.com\n激活码: 535945BF-4B652B72-23DD57B1-1380C0C4-8AFC8902\n\n\n\n赤友 NTFS 助手 6 软件永久\n\n激活码：63CABE8363CBBE0463CBBE9D14DCA474\n\n\n"},"F_Others/过去，现在，将来？-致自己":{"slug":"F_Others/过去，现在，将来？-致自己","filePath":"F_Others/过去，现在，将来？-致自己.md","title":"小感","links":[],"tags":[],"content":"新冠肺炎使我再读书阶段经历了最长的一段假期，但是假期越久，舒适环境久了就再也不想回去了。研究生压力很大，事情很多，一直在磨着我的性子。\n本科的时候我一直觉得自己经常在学习，但在现在看来，我一直在骗自己。从痴迷于平面设计到视频动效设计，为此学习了各种软件，PhotoShop、Premiere、AfterEffects, Cinema4D, ParticleFlow, 甚至也去接触了Houdini，但是直到最后我却没有像样的作品拿出手，我才真正看清了自己所谓的努力，只是以学完一个东西为目标，这种出发点一开始就是错的。对于设计，软件只是工具，出发点一直应该是惊艳的作品。我自己非常的缺乏独立思考，在动效里面，完全可以自己去模仿别人制作，但是总是找借口，先是一直说电脑太差，然后换了一个(+1)；然后有觉得做一个片子太久了自己没有时间，事实上我所谓没有时间是因为三四个四五个小时打lol就过去了。 理性分析一下，还是因为大学觉悟太低，别人总会说我努力，然后时间久了我自己也慢慢相信这一笑话了。想想真是可笑，自己陶醉于自己的谎言里，如此可悲。\n本科当然并非一无所获，本科直到现在，自己最为自豪的技能就是PhotoShop 了，PS已经内化为我的一个技能了，熟稔于心了，基本日常需求是忘不了了。反思原因，大学的时候大一到大二期间我一直在学PS，PS本身东西是挺多的，关键是实践出真知，加上坚持，那时候很痴迷于图片合成，基本每天都要做一张图，看着别人的教程去做，久而久之我自己都没意识到已经很熟悉的掌握了。同时当时的练习作品有14G左右。\n然而正是有了本科的不足，研究生的经历，才会更加怀念本科时光。本科时候也是我最快乐的时光了，压力很小，和大家一起焦虑、一起迷茫，想学的时候学，想玩就玩，和同学约说出去就出去，说唱歌就唱歌，那段时间自由，散漫，说话也是口无遮拦。和现在非常卑微的自己形成鲜明的对比：没有周末，说话小心翼翼，经常被怼，烦心的事情一件又一件。\n本科有多美好现在就有多苦，出来混，早晚要还的。苍天饶过谁？\n​\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t--------致碌碌无为的自己，间歇性踌躇满志、持续性混吃等死的自己。\n\n固然现在的自己很差，还是应该理性分析一下问题出在哪里，指定符合自己的策略。\n\n不完美的现在\n\n彻底反思一下自己，一直效率不高。关键在于不能集中精力去做一件事，经常会分心，分心原因在于 手机、电脑，如果做什么都可以全力去做，会比现在好很多。\n三分钟热度，总是觉得什么都可以学，但是事实上并不是这样，经过了大学四年的试验，已经深刻体会到，什么都分开一段段去做，真的很容易放弃，很容易半途而废，而且自己还会觉得很爽，或者是自信心又会受到打击，又会责怪自己。除非是自己真心所热爱的，否则对于我自己来说放弃是分分钟的。\n有时候会安慰自己，现实大概率是高斯分布，既然我是这样，那么很多人应该也是。事实上并不是这样，比你努力的人多了去了。\n我经常自嘲智商不够高，自己不够聪明，情商太低。事实上内心深处自己才知道自己对自己的期望，对于专业知识、日常交流等等，和一般人比，我不比一般人差，但是人情世故我可能确实是木讷了点儿，这一点我也并未在意。既然智商正常，为什么还是只到了现在这种程度？还是因为假装努力。每天bilibili刷两三个小时，三国杀两三个小时，白天4-6小时就过去了，吃饭耽误三个小时，9个小时没了，有多少时间提升专业技能呢？\n\n\n目前缺点很多，唯一优点就是至少还有上进心，不是一点上进心都没有。\n\n前面提到很难坚持一件事情，但是如果是以钱为导向呢？研究生毕业后如果能进大厂，那么最白菜的工资也是年薪25W，对于一个农村人真的是诱惑力很大，所以此时对钱的渴望 &gt; 兴趣, 也可以说是坚持下去的很强的一个理由。\n冲出现在\n为了走出现在的困境，我制定以下计划：\n效率不高\n工作的时候，先摆脱手机，把手机放的离自己远一点；电脑尽量不去找壁纸，不去刷网页，不被广告所困扰。尝试全神贯注，集中注意力去做一件事。\n半途而废\n对于专业技能，根据学习路线，一段时间内就攻克一个东西，不要着急去学完，重要的是学精，学会，学完代表不了什么，能学会是最重要的，每一个知识点都如此。\n然后不去并行的很多知识一起学习，这样会很容易放弃。一段时间就搞一个。\n欺骗自己\n这个就不要再假装学习了，到最后受害者是自己啊！直面自己，没什么的。各种技能一定要先确定正确的出发点，\n不要以学完为目的！\n不要以学完为目的！\n不要以学完为目的！\n切记切记。所有的东西以PS的熟练度为终点。\n时间安排\nbilibili已经有了依赖性，卸载不现实，三国杀准备卸载。\nlol已经戒掉。\n一定不要再找壁纸，找软件了!!! 切记切记！\n未来啊\n以金钱为导向，我选出了两个工作方向：\n\nCV算法工程师\n\n机器学习算法熟悉\nA类论文\n英语基础学习\n\n\nJAVA开发工程师\n\n以现在已经定好的学习路线为准，学习JAVA开发\n\n\n\n以后所有的技能都针对于这两个，虽然现在依然喜欢动效，但是同样热爱Java开发。所以以Java为目标！搞起来！\n期待未来的自己能做出改变，哪怕是一点点。\n\n2020/4/30夜"},"G_Personal_Summary/3.17-->-3.23":{"slug":"G_Personal_Summary/3.17-->-3.23","filePath":"G_Personal_Summary/3.17 -> 3.23.md","title":"3.17 -> 3.23","links":[],"tags":["总结"],"content":"本周期望 PAMI 论文有实质性进展"},"G_Personal_Summary/index":{"slug":"G_Personal_Summary/index","filePath":"G_Personal_Summary/index.md","title":"index","links":["G_Personal_Summary/3.17-->-3.23"],"tags":[],"content":"\n个人需要多做总结，每周目标，每月目标，季度目标，等等。\n\n\nWeekly\n3.17 → 3.23\nMonthly\nYearly"},"Resources-Images/index":{"slug":"Resources-Images/index","filePath":"Resources-Images/index.md","title":"index","links":[],"tags":[],"content":"\n该库图库，所有用到的图像资源\n\n"},"index":{"slug":"index","filePath":"index.md","title":"index","links":["A_Navigation/Experimental-Tracking","A_Navigation/Paper-Related","A_Navigation/Research-Knowledge","A_Navigation/Technique-Explores","A_Navigation/Others","A_Navigation/Summary"],"tags":[],"content":"\n这是我的 Obsidian 知识库，构建于 2024-3-26。\n\n构建原则:\n\n放弃海量标签（确实没啥用）\n可以像 book 一样去索引一个东西，目录，这样可以解放复杂分类\n尝试重建联系\n\n\n按这个原则我去尝试一下，至少目前的状态我不喜欢，笔记就是给自己读的，不该这么难受，目录一个个去找，相反和 book 的 index 一样会比较方便。\n\n知识库目录\n一、Experimental Tracking\n二、Paper Related\n三、Research Knowledge\n四、Technique Explores\n五、Others\n六、Summary"}}