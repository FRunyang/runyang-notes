---
title: 从分类到逻辑回归
date: 2020-06-08 22:46:27
cover: https://user-images.githubusercontent.com/60562661/83933911-3a8baf80-a7df-11ea-8e9e-253c0be24056.jpg
tags:
  - Classification
---

关于分类，我博客里面写过很多 相关的：

- [[深度学习入门系列-逻辑回归]]
- [[Softmax函数详解]]
- [[Cross-Entropy-的前世今生]]

但是这些并没有串起来，所以一直是零散的，这里来串一下彻底理解分类相关的东西。

> ​	一句题外话，开始学习的时候我一直说自己是做回归的不做分类，所以不去学。但是现在看来，作为一个研究CV方向的人，学习分类原理等等完全都是基本素养，需要用心学习。

## 二分类

首先在分类中，有概率判别式模型和概率生成式模型，现在用朴素贝叶斯来做二分类(属于概率生成式模型)。

### 朴素贝叶斯分类器

- 模型建立

$$
P(C_1|X) = \frac{P(C_1)*P(X|C_1)}{P(X)} = \frac{P(C_1)*P(X|C_1)}{\sum _{i=1}^2P(C_i)*P(X|C_i)}
$$
其中，X为观测到的样本，求x属于C1的概率

- 假设C1服从高斯分布，那就是求出`均值、方差` 这两个参数使得$P(x|C_1)$最大化，也就是最大似然估计：

$$
P(X|C_1) = \prod_{k=1}^n P(x_k|C_1)
$$

- 选择最好的函数



### **二元高斯分布下的贝叶斯分类器转换为回归问题**

具体推导文章开头第一篇文章有讲，这里只给结论：

通过推导可以看出，分类问题和回归问题是统一的，我们不必再通过计算μ1μ1,μ2μ2,Σ1Σ1,Σ2Σ2,N1N1,N2N2来计算概率，而可以通过DeepLearning的手段直接计算w和b，从而求得概率，而与线性回归不同的是，我们需要将输出结果用σ()函数转化到0~1之间，这就是**逻辑回归**。

## 多分类

**Note:** 引自[blog](https://qqtoyota.github.io/post/从分类到逻辑回归/) ：写的很棒！

`softmax文章开始也有提到，是在深度学习中常用的多分类函数。`

多分类计算有两种方式：

- K个独立的二元分类器——用于**不互斥**的分类
- softmax——用于**互斥**的分类

**softmax VS k个二元分类器**

如果你在开发一个音乐分类的应用，需要对k种类型的音乐进行识别，那么是选择使用 softmax 分类器呢，还是使用 logistic 回归算法建立 k 个独立的二元分类器呢？ 这一选择取决于你的类别之间是否互斥，例如，如果你有四个类别的音乐，分别为：古典音乐、乡村音乐、摇滚乐和爵士乐，那么你可以假设每个训练样本只会被打上一个标签（即：一首歌只能属于这四种音乐类型的其中一种），此时你应该使用类别数 k = 4 的softmax回归。（如果在你的数据集中，有的歌曲不属于以上四类的其中任何一类，那么你可以添加一个“其他类”，并将类别数 k 设为5。） 如果你的四个类别如下：人声音乐、舞曲、影视原声、流行歌曲，那么这些类别之间并不是互斥的。例如：一首歌曲可以来源于影视原声，同时也包含人声 。这种情况下，使用4个二分类的 logistic 回归分类器更为合适。这样，对于每个新的音乐作品 ，我们的算法可以分别判断它是否属于各个类别。 现在我们来看一个计算视觉领域的例子，你的任务是将图像分到三个不同类别中。(i) 假设这三个类别分别是：室内场景、户外城区场景、户外荒野场景。你会使用sofmax回归还是 3个logistic 回归分类器呢？ (ii) 现在假设这三个类别分别是室内场景、黑白图片、包含人物的图片，你又会选择 softmax 回归还是多个 logistic 回归分类器呢？ 在第一个例子中，三个类别是互斥的，因此更适于选择softmax回归分类器 。而在第二个例子中，建立三个独立的 logistic回归分类器更加合适。

## 交叉熵

在交叉熵一文中举的例子就用的逻辑回归，解释了为什么逻辑回归用交叉熵当损失函数，至此，一切都顺理成章了。
[[Cross-Entropy-的前世今生]]
