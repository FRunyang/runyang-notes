---
date: 2024-03-22T20:55:00
tags:
  - Pose-Estimation
aliases:
  - 时序-空间
---

理解 **关键帧（当前帧）** 与**辅助帧（临近帧）**的时序关联性对于视频人体姿态估计来说至关重要。

现有方法通常显式（光流，时间差分）或者隐式（可形变）建模运动信息来捕获关键帧与辅助帧的时间相关性。然而，他们都是基于当前帧的视觉特征进行的。若当前帧发生遮挡等，它们的结果比较差。

我们提出一个新的方法来显式推理**关键帧（当前帧）** 与**辅助帧（临近帧）**的时序关系，且独立于当前帧的视觉特征。

B, L, C. (1,17,C)→Gaussian B,L+17,C

---

目前做法：

- 先把辅助帧过Transformer Encoder建模时空信息；
- 然后把时空信息token与关键点token堆叠，预测关键点位置（建模关键点与所有帧的相关性）
- 得到时空预测后将时空token作为query，关键帧视觉特征作为key和value进行优化
- 最后用query产生增强的预测结果

---

可能存在的问题：

- 直接建模 keypoint token 与每帧的关系难度太大，因为token序列长度较长，可以考虑直接融合多帧特征 （在时间维度上进行 torch.mean），然后建模keypoint token与时空上下文的关系
- keypoint token的位置编码需要加上，用可学习的token
- 整个输入序列的时序编码要加上，这个应该蛮影响性能的
- 修改初始化方法，学习MAE代码的初始化，初始化对模型也有影响

---

后续修改版本：

- 时空特征直接建模融合，建模 keypoint token与其关系，实现无视觉基础的关键点推理；视觉优化先用目前的版本；
- 保持多帧token，建模keyopoint token与其关系，不过视觉refine时直接将纯预测与视觉的整合（取平均），来验证是不是现在的refine策略问题；
- 多帧token加上下文重建，更换重建策略，即用类似的192个token表示关键帧图像信息，（192，embed_dim) → (192, 16_16_3) → (keyframe resize成 192， 16_16_3 进行监督)

---

### 目前v3_2版本得到了不错的结果，可以作为一个不错的baseline，模型架构如下：

1. Encoder部分 **联合时空建模 + 分离的时间-空间建模**
2. 时序预测分支**初始化了关节token；用每帧的特征与平均的关节token相加预测当前帧heatmap；加入了contextual token来重建当前帧作为监督（系数0.0001）；**
3. 视觉特征分支以当前帧特征为query，融合的辅助帧特征（对时间维度取平均）为Key Value 进行特征融合；
4. 监督上进行了密集的监督：
    1. keypoint输出heatmap
    2. 每帧预测的当前帧heatmap进行监督
    3. 最终融合特征进行监督

### 后续需要测试的Baseline：

目前密集预测版本效果较好，试试去掉token直接密集预测

1. 直接用现有的 **v3_2** baseline，去掉重建loss看看结果（**测试重建部分的效果**）
2. 不用每个辅助帧去预测当前帧的heatmap，直接用**Keypoint Token** 出预测结果，进行约束；其他部分保持不变 （保持重建loss为0.0001的比例）**（测试密集预测的效果 ）**
3. 直接用**Keypoint Token** 出预测结果进行约束，并把`visual_prediction`进行约束；最终融合的结果不约束 **（测试中间监督与最终监督，目测结果影响不大）**
4. 在视觉特征分支，不融合多帧特征，直接把多帧作为Key和Value,看看是否有影响（基于v3_2）
5. 其他小的实验，Encoder中测试联合时空建模和分离时空建模有效性（可以试试删掉联合时空建模步骤）

---

与《Global-to-Local Modeling for Video-based 3D Human Pose and Shape Estimation》不同的出发点：

- 任务不同
- 动机完全不同。建模全局-局部运动；无关键帧推理-有关键帧识别

---

## **Contribution**

- 提出一个新颖的DSN框架，并行地学习双重时空表征（关键帧的视觉增强和无关键帧的时序推理），解耦合现有方法（单一视觉增强时空表征分支）对关键帧视觉特征的过度依赖。这允许很早期产生了合理可靠的人体姿态-即使在关键帧视觉退化下。[可以突出现有方法单分支]
- 设计两个模块自适应发觉有用信息实现视觉增强，掌握全局时许演化信息实现姿态推理。
- SOTA.

---

## 投了 CVPR24 没中，但是感觉审稿意见质量挺低的，准备继续投 ECCV

### 一些反思即后续修稿方向

从方法上说我感觉这个方法确实解决了现存的问题，只是包装上还不太够，所以第一眼看上去觉得 contribution 不太大，先入为主就会觉得比较普通，不亮眼

审稿人不懂不是理由，重点是怎么完美的去突出工作的贡献；要突出贡献就得指出难点在哪里

之前只是说现有方法依赖于关键帧，所以我们要预测 然后直接就预测了。

在模型方面，我觉得得去说一下，为什么做预测是难的，与瞎按有方法的区别得点出来。

---

**与之前论文的区别：**

**比如 TDMI，**

- 说现有方法没用过时序差分，所以怎么用是比较重要的；
- 先有方法不能挖掘有用信息，怎么挖掘也是重要的，然后挖掘的方法也新

**进行类比 DiffPose，**

- 怎么在扩散模型中引入时序信息；
- 怎么从噪声中恢复热图

**这次，**

- 现有方法不能预测，预测很重要；先突显预测的重要性，再说现在方法不行，我们的可以
- 但是姿态预测听起来比较简单，相比特征挖掘，扩散恢复等来说，姿态预测确实太简单了～
- 所以就不能直接写我们预测了，而是应该写预测的难点在哪里。

# 第二次投稿更新 ECCV第一轮意见也很差 没必要去 Rebuttal 了，准备重新整一下这个方法，继续去投。这篇我不打算去降低档次。

### 投稿反思
**一些关键的意见：**
- 两个模块过于独立，没啥关系（说明双流网络之间做交互啥的挺重要，或者这条不是很重要）
- Motivation 不太合理，就是图 2 但是我自己来看，这个 motivation 是合理的，就是严重依赖关键帧。但是要论证这点是比较难的。而且大家不太 get 到这一点。图 2 确实不太符合实际情况，这个我自己也考虑到了。所以说图 2 不太好（或者 eecv 投稿的时候去掉图 2 可能会好点？）
>*主要问题就是以上两点* 也就是需要更好的 motivation 去呈现，至少相比 cvpr 是有进步的这次。

其他：
- 参数量；离线方式；位置编码；token 方法依然不能理解

**综上，问题就是 motivation+模型设计，我觉得需要更加 convince 的动机，以及更抓眼的模型设计。**

---

一些初步的动机想法：
- 理解运动信息很重要。现有方法通常用光流、差分等，这个范式更加注重像素级别细粒度的像素级视觉内容对应，而忽略了比较 high-level 的帧间的关系，例如整体活肢体的人体运动。因此，这些方法对于遮挡等视觉退化是很敏感的，容易性能下降。我们提出充分去挖掘帧间的时空关系，学习图像级别的运动信息，即使在某些帧退化下依然有鲁邦的时序推理能力。 --- *本质上来说，光流/差分与像素之间相似度是密不可分的，过度依赖像素相似性，忽略全局的语义关系，会造成很容易失败
- 模型方面，可以搞一个由动态到静态的更新框架，先是一些 global keypoint token 去学习全局动态；然后进一步用关键帧的空间信息去更新，进一步增强之类的。当然主motivation 就是以上的内容。第二个点可以再想想。

---
投稿之后本来纠结像素相似性，语义关系说法的准确性
经过纠结以及看光流论文，这个说法完全没问题。光流/差分很靠像素相似性，过于直觉，且容易失败对于退化，而且本来也无法去理解全局的语义关系，比如理解到这个人在跑步，那他应该是怎么样的姿势。他只能本能地去看这个像素像谁，没有语义的概念，所以实际上也容易出错。所以我们要学习 high-level 的高层的语义动态，对于局部的像素退化是不太敏感的，而不是根据相似度学习像素关系。反观GME ，本来也就是去挖掘帧间语义关系的。
---
实验需要修改，我换一下 Backbone 网络试试，不用 pretrain 的 ViT，直接用原始的 ViT-L 和 ViT-B 去训练，看看结果，也好对比参数量。

---

## 2025.1.25 Accept
## Neurocomputing 中科院二区
## 这个工作也算是有个好的归宿了
